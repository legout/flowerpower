[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "Welcome to the FlowerPower quickstart guide! This guide will walk you through the process of creating a “Hello World” project to demonstrate the core functionalities of the library."
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "Quickstart",
    "section": "Installation",
    "text": "Installation\nFirst, ensure you have FlowerPower installed. We recommend using uv for a fast and reliable installation.\n# Create and activate a virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install FlowerPower with RQ for job queue support\nuv pip install flowerpower[rq]"
  },
  {
    "objectID": "quickstart.html#initialize-your-project",
    "href": "quickstart.html#initialize-your-project",
    "title": "Quickstart",
    "section": "1. Initialize Your Project",
    "text": "1. Initialize Your Project\nYou can create a new project using either the CLI or the Python API.\n\nUsing the CLI\nflowerpower init --name hello-flowerpower --job_queue_type rq\ncd hello-flowerpower\n\n\nUsing the Python API\nfrom flowerpower import FlowerPowerProject\n\n# Initialize a new project with RQ job queue support\nproject = FlowerPowerProject.init(\n    name='hello-flowerpower',\n    job_queue_type='rq'\n)\nThis creates a standard project structure with conf/ and pipelines/ directories."
  },
  {
    "objectID": "quickstart.html#configure-your-project",
    "href": "quickstart.html#configure-your-project",
    "title": "Quickstart",
    "section": "2. Configure Your Project",
    "text": "2. Configure Your Project\nThe conf/project.yml file contains global settings for your project, including the job queue configuration.\n# conf/project.yml\nname: hello-flowerpower\njob_queue:\n  type: rq\n  backend:\n    type: redis\n    host: localhost\n    port: 6379\n    queues:\n      - default\n      - high\n      - low"
  },
  {
    "objectID": "quickstart.html#create-a-pipeline",
    "href": "quickstart.html#create-a-pipeline",
    "title": "Quickstart",
    "section": "3. Create a Pipeline",
    "text": "3. Create a Pipeline\nNext, create a pipeline to define your data processing logic.\n\nUsing the CLI\nflowerpower pipeline new hello_world\n\n\nUsing the Python API\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load('.')\nproject.pipeline_manager.new(name='hello_world')\nThis generates pipelines/hello_world.py for your pipeline logic and conf/pipelines/hello_world.yml for its configuration."
  },
  {
    "objectID": "quickstart.html#implement-the-pipeline",
    "href": "quickstart.html#implement-the-pipeline",
    "title": "Quickstart",
    "section": "4. Implement the Pipeline",
    "text": "4. Implement the Pipeline\nOpen pipelines/hello_world.py and add your Hamilton functions.\n# pipelines/hello_world.py\nfrom pathlib import Path\nfrom hamilton.function_modifiers import parameterize\nfrom flowerpower.cfg import Config\n\n# Load pipeline parameters\nPARAMS = Config.load(\n    Path(__file__).parents[1], pipeline_name=\"hello_world\"\n).pipeline.h_params\n\n@parameterize(**PARAMS.greeting_message)\ndef greeting_message(message: str) -&gt; str:\n    return f\"{message},\"\n\n@parameterize(**PARAMS.target_name)\ndef target_name(name: str) -&gt; str:\n    return f\"{name}!\"\n\ndef full_greeting(greeting_message: str, target_name: str) -&gt; str:\n    \"\"\"Combines the greeting and target.\"\"\"\n    print(f\"Executing pipeline: {greeting_message} {target_name}\")\n    return f\"{greeting_message} {target_name}\""
  },
  {
    "objectID": "quickstart.html#configure-the-pipeline",
    "href": "quickstart.html#configure-the-pipeline",
    "title": "Quickstart",
    "section": "5. Configure the Pipeline",
    "text": "5. Configure the Pipeline\nIn conf/pipelines/hello_world.yml, define the parameters and execution details for your pipeline.\n# conf/pipelines/hello_world.yml\nparams:\n  greeting_message:\n    message: \"Hello\"\n  target_name:\n    name: \"World\"\n\nrun:\n  final_vars:\n    - full_greeting\n\nschedule:\n  cron: \"0 * * * *\" # Run hourly"
  },
  {
    "objectID": "quickstart.html#run-the-pipeline",
    "href": "quickstart.html#run-the-pipeline",
    "title": "Quickstart",
    "section": "6. Run the Pipeline",
    "text": "6. Run the Pipeline\nYou can run your pipeline synchronously for quick tests or asynchronously for scheduled and background jobs.\n\nSynchronous Execution\nThis is useful for debugging and local development.\n\nUsing the CLI\nflowerpower pipeline run hello_world\n\n\nUsing the Python API\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load('.')\nresult = project.run('hello_world')\nprint(result)\n\n\n\nAsynchronous Execution\nFor asynchronous execution, you need a running Redis server.\n\n\n\n\n\n\nNote\n\n\n\nEnsure Redis is running before proceeding with asynchronous execution. You can use the provided Docker setup for a quick start:\ncd docker\ndocker-compose up -d redis\n\n\n\nEnqueue a Job\nAdd your pipeline to the job queue for background processing.\n\nUsing the CLI\nflowerpower pipeline add-job hello_world\n\n\nUsing the Python API\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load('.')\njob_id = project.enqueue('hello_world')\nprint(f\"Job enqueued with ID: {job_id}\")\n\n\n\nStart a Worker\nWorkers are required to process jobs from the queue.\n\nUsing the CLI\nflowerpower job-queue start-worker\n\n\nUsing the Python API\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load('.')\n# Start a worker in the background\nproject.start_worker(background=True)\nFor more details on managing your project, refer to the API documentation for FlowerPowerProject, PipelineManager, and JobQueueManager."
  },
  {
    "objectID": "api/pipelinemanager.html",
    "href": "api/pipelinemanager.html",
    "title": "PipelineManager",
    "section": "",
    "text": "Module: flowerpower.pipeline.PipelineManager\nThe PipelineManager is the central class for managing pipeline operations in FlowerPower. It provides a unified interface for creating, running, and managing pipelines.\n\n\n\n\n__init__(self, base_dir: str | None = None, storage_options: dict | Munch | BaseStorageOptions | None = None, fs: AbstractFileSystem | None = None, cfg_dir: str | None = None, pipelines_dir: str | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, log_level: str | None = None)\nInitializes the PipelineManager.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project. Defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| Munch \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\nNone\n\n\ncfg_dir\nstr \\| None\nThe directory for configuration files.\nsettings.CONFIG_DIR\n\n\npipelines_dir\nstr \\| None\nThe directory for pipeline modules.\nsettings.PIPELINES_DIR\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project.\nsettings.JOB_QUEUE_TYPE\n\n\nlog_level\nstr \\| None\nThe logging level for the manager.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nregistry\nPipelineRegistry\nHandles pipeline registration and discovery.\n\n\nscheduler\nPipelineScheduler\nManages job scheduling and execution.\n\n\nvisualizer\nPipelineVisualizer\nHandles pipeline visualization.\n\n\nio\nPipelineIOManager\nManages pipeline import/export operations.\n\n\nproject_cfg\nProjectConfig\nCurrent project configuration.\n\n\npipeline_cfg\nPipelineConfig\nCurrent pipeline configuration.\n\n\npipelines\nlist[str]\nList of available pipeline names.\n\n\ncurrent_pipeline_name\nstr\nName of the currently loaded pipeline.\n\n\nsummary\ndict[str, dict \\| str]\nSummary of all pipelines.\n\n\n_base_dir\nstr\nThe base directory of the project.\n\n\n_fs\nAbstractFileSystem\nThe filesystem instance used by the manager.\n\n\n_storage_options\ndict \\| Munch \\| BaseStorageOptions\nStorage options for the filesystem.\n\n\n_cfg_dir\nstr\nThe directory for configuration files.\n\n\n_pipelines_dir\nstr\nThe directory for pipeline modules.\n\n\n_project_context\nFlowerPowerProject \\| None\nReference to the FlowerPowerProject instance.\n\n\n\n\n\n\n\n\nrun(self, name: str, inputs: dict | None = None, final_vars: list[str] | None = None, config: dict | None = None, cache: dict | None = None, executor_cfg: str | dict | ExecutorConfig | None = None, with_adapter_cfg: dict | WithAdapterConfig | None = None, pipeline_adapter_cfg: dict | PipelineAdapterConfig | None = None, project_adapter_cfg: dict | ProjectAdapterConfig | None = None, adapter: dict[str, Any] | None = None, reload: bool = False, log_level: str | None = None, max_retries: int | None = None, retry_delay: float | None = None, jitter_factor: float | None = None, retry_exceptions: tuple | list | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None)\nExecute a pipeline synchronously and return its results.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run. Must be a valid identifier.\n\n\n\ninputs\ndict \\| None\nOverride pipeline input values. Example: {\"data_date\": \"2025-04-28\"}\nNone\n\n\nfinal_vars\nlist[str] \\| None\nSpecify which output variables to return. Example: [\"model\", \"metrics\"]\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor. Example: {\"model\": \"LogisticRegression\"}\nNone\n\n\ncache\ndict \\| None\nCache configuration for results. Example: {\"recompute\": [\"node1\", \"final_node\"]}\nNone\n\n\nexecutor_cfg\nstr \\| dict \\| ExecutorConfig \\| None\nExecution configuration, can be: - str: Executor name, e.g. “threadpool”, “local” - dict: Raw config, e.g. {\"type\": \"threadpool\", \"max_workers\": 4} - ExecutorConfig: Structured config object\nNone\n\n\nwith_adapter_cfg\ndict \\| WithAdapterConfig \\| None\nAdapter settings for pipeline execution. Example: {\"opentelemetry\": True, \"tracker\": False}\nNone\n\n\npipeline_adapter_cfg\ndict \\| PipelineAdapterConfig \\| None\nPipeline-specific adapter settings. Example: {\"tracker\": {\"project_id\": \"123\", \"tags\": {\"env\": \"prod\"}}}\nNone\n\n\nproject_adapter_cfg\ndict \\| ProjectAdapterConfig \\| None\nProject-level adapter settings. Example: {\"opentelemetry\": {\"host\": \"http://localhost:4317\"}}\nNone\n\n\nadapter\ndict[str, Any] \\| None\nCustom adapter instance for pipeline Example: {\"ray_graph_adapter\": RayGraphAdapter()}\nNone\n\n\nreload\nbool\nForce reload of pipeline configuration.\nFalse\n\n\nlog_level\nstr \\| None\nLogging level for the execution. Valid values: “DEBUG”, “INFO”, “WARNING”, “ERROR”, “CRITICAL”\nNone\n\n\nmax_retries\nint \\| None\nMaximum number of retries for execution.\nNone\n\n\nretry_delay\nfloat \\| None\nDelay between retries in seconds.\nNone\n\n\njitter_factor\nfloat \\| None\nRandom jitter factor to add to retry delay\nNone\n\n\nretry_exceptions\ntuple \\| list \\| None\nExceptions that trigger a retry.\nNone\n\n\non_success\nCallable \\| tuple[Callable, tuple \\| None, dict \\| None] \\| None\nCallback to run on successful pipeline execution.\nNone\n\n\non_failure\nCallable \\| tuple[Callable, tuple \\| None, dict \\| None] \\| None\nCallback to run on pipeline execution failure.\nNone\n\n\n\nReturns: dict[str, Any] - Pipeline execution results, mapping output variable names to their computed values.\nRaises:\n\nValueError: If pipeline name doesn’t exist or configuration is invalid.\nImportError: If pipeline module cannot be imported.\nRuntimeError: If execution fails due to pipeline or adapter errors.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Simple execution\nresult = manager.run(\"my_pipeline\")\n\n# With custom inputs\nresult = manager.run(\n    \"ml_pipeline\",\n    inputs={\"data_date\": \"2025-01-01\"},\n    final_vars=[\"model\", \"metrics\"]\n)\n\n\n\n\nnew(self, name: str, overwrite: bool = False)\nCreate a new pipeline with the given name.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline. Must be a valid Python identifier.\n\n\n\noverwrite\nbool\nWhether to overwrite existing pipeline with same name.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If name is invalid or pipeline exists and overwrite=False.\nRuntimeError: If file creation fails.\nPermissionError: If lacking write permissions.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\n# Create new pipeline\nmanager = PipelineManager()\nmanager.new(\"data_transformation\")\n\n# Overwrite existing pipeline\nmanager.new(\"data_transformation\", overwrite=True)\n\n\n\n\ndelete(self, name: str)\nDelete an existing pipeline.\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to delete.\n\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nRuntimeError: If deletion fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\nmanager.delete(\"old_pipeline\")\n\n\n\n\nshow_pipelines(self, format: str = \"table\")\nDisplay a summary of all available pipelines.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nformat\nstr\nOutput format for the list (“table”, “json”, “yaml”).\n\"table\"\n\n\n\nReturns: None\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show pipelines in table format (default)\nmanager.show_pipelines()\n\n# Show pipelines in JSON format\nmanager.show_pipelines(format=\"json\")\n\n\n\n\nadd_hook(self, name: str, type: HookType, to: str, function_name: str)\nAdd a hook to a specific pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to add the hook to.\n\n\n\ntype\nHookType\nType of the hook (e.g., HookType.MQTT_BUILD_CONFIG).\n\n\n\nto\nstr\nDestination of the hook (e.g., “mqtt”).\n\n\n\nfunction_name\nstr\nName of the function to be called as the hook.\n\n\n\n\nReturns: None\nRaises:\n\nValueError: If the pipeline does not exist or hook type is invalid.\nFileExistsError: If a hook with the same name and type already exists.\n\n\n\nfrom flowerpower.pipeline import PipelineManager, HookType\n\nmanager = PipelineManager()\nmanager.add_hook(\n    name=\"my_pipeline\",\n    type=HookType.MQTT_BUILD_CONFIG,\n    to=\"mqtt\",\n    function_name=\"build_mqtt_config\"\n)\n\n\n\n\nremove_hook(self, name: str, type: HookType, function_name: str)\nRemove a hook from a specific pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to remove the hook from.\n\n\n\ntype\nHookType\nType of the hook to remove.\n\n\n\nfunction_name\nstr\nName of the function that was used as the hook.\n\n\n\n\nReturns: None\nRaises: FileNotFoundError: If the pipeline or hook does not exist.\n\n\nfrom flowerpower.pipeline import PipelineManager, HookType\n\nmanager = PipelineManager()\nmanager.remove_hook(\n    name=\"my_pipeline\",\n    type=HookType.MQTT_BUILD_CONFIG,\n    function_name=\"build_mqtt_config\"\n)\n\n\n\n\nimport_pipeline(self, name: str, src_base_dir: str, src_fs: AbstractFileSystem | None = None, src_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nImport a pipeline from another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline in the current project.\n\n\n\nsrc_base_dir\nstr\nSource FlowerPower project directory or URI. Examples: - Local: \"/path/to/other/project\" - S3: \"s3://bucket/project\" - GitHub: \"github://org/repo/project\"\n\n\n\nsrc_fs\nAbstractFileSystem \\| None\nPre-configured source filesystem. Example: S3FileSystem(anon=False)\nNone\n\n\nsrc_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for source filesystem access. Example: {\"key\": \"ACCESS_KEY\", \"secret\": \"SECRET_KEY\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipeline if name exists.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If pipeline name exists and overwrite=False.\nFileNotFoundError: If source pipeline not found.\nRuntimeError: If import fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\nfrom s3fs import S3FileSystem\n\nmanager = PipelineManager()\n\n# Import from local filesystem\nmanager.import_pipeline(\n    \"new_pipeline\",\n    \"/path/to/other/project\"\n)\n\n# Import from S3 with custom filesystem\ns3 = S3FileSystem(anon=False)\nmanager.import_pipeline(\n    \"s3_pipeline\",\n    \"s3://bucket/project\",\n    src_fs=s3\n)\n\n\n\n\nimport_many(self, names: list[str], src_base_dir: str, src_fs: AbstractFileSystem | None = None, src_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nImport multiple pipelines from another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnames\nlist[str]\nList of pipeline names to import.\n\n\n\nsrc_base_dir\nstr\nSource FlowerPower project directory or URI. Examples: - Local: \"/path/to/other/project\" - S3: \"s3://bucket/project\" - GitHub: \"github://org/repo/project\"\n\n\n\nsrc_fs\nAbstractFileSystem \\| None\nPre-configured source filesystem. Example: S3FileSystem(anon=False)\nNone\n\n\nsrc_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for source filesystem access. Example: {\"key\": \"ACCESS_KEY\", \"secret\": \"SECRET_KEY\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipelines if names exist.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If any pipeline name exists and overwrite=False.\nFileNotFoundError: If any source pipeline not found.\nRuntimeError: If import fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Import multiple pipelines\nmanager.import_many(\n    names=[\"pipeline1\", \"pipeline2\"],\n    src_base_dir=\"/path/to/other/project\"\n)\n\n# Import multiple pipelines from S3\nmanager.import_many(\n    names=[\"s3_pipeline_a\", \"s3_pipeline_b\"],\n    src_base_dir=\"s3://bucket/source\",\n    src_storage_options={\n        \"key\": \"ACCESS_KEY\",\n        \"secret\": \"SECRET_KEY\"\n    }\n)\n\n\n\n\nexport_pipeline(self, name: str, dest_base_dir: str, dest_fs: AbstractFileSystem | None = None, dest_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nExport a pipeline to another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to export.\n\n\n\ndest_base_dir\nstr\nDestination FlowerPower project directory or URI. Examples: - Local: \"/path/to/backup\" - S3: \"s3://bucket/backups\" - GCS: \"gs://bucket/backups\"\n\n\n\ndest_fs\nAbstractFileSystem \\| None\nPre-configured destination filesystem. Example: GCSFileSystem(project='my-project')\nNone\n\n\ndest_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for destination filesystem access. Example: {\"token\": \"my_token\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipeline in destination if name exists.\nFalse\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If the pipeline does not exist in the current project.\nFileExistsError: If destination pipeline exists and overwrite=False.\nRuntimeError: If export fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\nfrom gcsfs import GCSFileSystem\n\nmanager = PipelineManager()\n\n# Export to local backup\nmanager.export_pipeline(\n    \"my_pipeline\",\n    \"/path/to/backup\"\n)\n\n# Export to Google Cloud Storage\ngcs = GCSFileSystem(project='my-project')\nmanager.export_pipeline(\n    \"prod_pipeline\",\n    \"gs://my-bucket/backups\",\n    dest_fs=gcs\n)\n\n\n\n\nexport_many(self, names: list[str], dest_base_dir: str, dest_fs: AbstractFileSystem | None = None, dest_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nExport multiple pipelines to another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnames\nlist[str]\nList of pipeline names to export.\n\n\n\ndest_base_dir\nstr\nDestination FlowerPower project directory or URI. Examples: - Local: \"/path/to/backup\" - S3: \"s3://bucket/backups\" - GCS: \"gs://bucket/backups\"\n\n\n\ndest_fs\nAbstractFileSystem \\| None\nPre-configured destination filesystem. Example: GCSFileSystem(project='my-project')\nNone\n\n\ndest_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for destination filesystem access. Example: {\"token\": \"my_token\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipelines in destination if names exist.\nFalse\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If any pipeline does not exist in the current project.\nFileExistsError: If any destination pipeline exists and overwrite=False.\nRuntimeError: If export fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Export multiple pipelines\nmanager.export_many(\n    names=[\"pipeline1\", \"pipeline2\"],\n    dest_base_dir=\"/path/to/backup\"\n)\n\n# Export multiple pipelines from S3\nmanager.export_many(\n    names=[\"s3_pipeline_a\", \"s3_pipeline_b\"],\n    dest_base_dir=\"s3://bucket/backups\",\n    dest_storage_options={\n        \"key\": \"ACCESS_KEY\",\n        \"secret\": \"SECRET_KEY\"\n    }\n)\n\n\n\n\nshow_dag(self, name: str, format: str = \"png\", show_outputs: bool = False, display_html: bool = False)\nGenerate and display the Directed Acyclic Graph (DAG) of a pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize.\n\n\n\nformat\nstr\nOutput format for the DAG (“png”, “svg”, “html”, “dot”).\n\"png\"\n\n\nshow_outputs\nbool\nWhether to include output nodes in the DAG.\nFalse\n\n\ndisplay_html\nbool\nWhether to display the HTML directly in the notebook (only for “html” format).\nFalse\n\n\n\nReturns: None (displays the DAG directly or saves it to a file).\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nValueError: If format is invalid or visualization fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show DAG as PNG\nmanager.show_dag(\"my_pipeline\")\n\n# Show DAG as SVG with outputs\nmanager.show_dag(\"ml_pipeline\", format=\"svg\", show_outputs=True)\n\n\n\n\nshow_execution_graph(self, name: str, format: str = \"png\", show_outputs: bool = False, display_html: bool = False, inputs: dict | None = None, config: dict | None = None)\nGenerate and display the execution graph of a pipeline, considering inputs and configuration.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize.\n\n\n\nformat\nstr\nOutput format for the graph (“png”, “svg”, “html”, “dot”).\n\"png\"\n\n\nshow_outputs\nbool\nWhether to include output nodes in the graph.\nFalse\n\n\ndisplay_html\nbool\nWhether to display the HTML directly in the notebook (only for “html” format).\nFalse\n\n\ninputs\ndict \\| None\nInput values to consider for graph generation.\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor.\nNone\n\n\n\nReturns: None (displays the graph directly or saves it to a file).\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nValueError: If format is invalid or visualization fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show execution graph\nmanager.show_execution_graph(\"my_pipeline\", inputs={\"data_date\": \"2025-01-01\"})"
  },
  {
    "objectID": "api/pipelinemanager.html#initialization",
    "href": "api/pipelinemanager.html#initialization",
    "title": "PipelineManager",
    "section": "",
    "text": "__init__(self, base_dir: str | None = None, storage_options: dict | Munch | BaseStorageOptions | None = None, fs: AbstractFileSystem | None = None, cfg_dir: str | None = None, pipelines_dir: str | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, log_level: str | None = None)\nInitializes the PipelineManager.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project. Defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| Munch \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\nNone\n\n\ncfg_dir\nstr \\| None\nThe directory for configuration files.\nsettings.CONFIG_DIR\n\n\npipelines_dir\nstr \\| None\nThe directory for pipeline modules.\nsettings.PIPELINES_DIR\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project.\nsettings.JOB_QUEUE_TYPE\n\n\nlog_level\nstr \\| None\nThe logging level for the manager.\nNone"
  },
  {
    "objectID": "api/pipelinemanager.html#attributes",
    "href": "api/pipelinemanager.html#attributes",
    "title": "PipelineManager",
    "section": "",
    "text": "Attribute\nType\nDescription\n\n\n\n\nregistry\nPipelineRegistry\nHandles pipeline registration and discovery.\n\n\nscheduler\nPipelineScheduler\nManages job scheduling and execution.\n\n\nvisualizer\nPipelineVisualizer\nHandles pipeline visualization.\n\n\nio\nPipelineIOManager\nManages pipeline import/export operations.\n\n\nproject_cfg\nProjectConfig\nCurrent project configuration.\n\n\npipeline_cfg\nPipelineConfig\nCurrent pipeline configuration.\n\n\npipelines\nlist[str]\nList of available pipeline names.\n\n\ncurrent_pipeline_name\nstr\nName of the currently loaded pipeline.\n\n\nsummary\ndict[str, dict \\| str]\nSummary of all pipelines.\n\n\n_base_dir\nstr\nThe base directory of the project.\n\n\n_fs\nAbstractFileSystem\nThe filesystem instance used by the manager.\n\n\n_storage_options\ndict \\| Munch \\| BaseStorageOptions\nStorage options for the filesystem.\n\n\n_cfg_dir\nstr\nThe directory for configuration files.\n\n\n_pipelines_dir\nstr\nThe directory for pipeline modules.\n\n\n_project_context\nFlowerPowerProject \\| None\nReference to the FlowerPowerProject instance."
  },
  {
    "objectID": "api/pipelinemanager.html#methods",
    "href": "api/pipelinemanager.html#methods",
    "title": "PipelineManager",
    "section": "",
    "text": "run(self, name: str, inputs: dict | None = None, final_vars: list[str] | None = None, config: dict | None = None, cache: dict | None = None, executor_cfg: str | dict | ExecutorConfig | None = None, with_adapter_cfg: dict | WithAdapterConfig | None = None, pipeline_adapter_cfg: dict | PipelineAdapterConfig | None = None, project_adapter_cfg: dict | ProjectAdapterConfig | None = None, adapter: dict[str, Any] | None = None, reload: bool = False, log_level: str | None = None, max_retries: int | None = None, retry_delay: float | None = None, jitter_factor: float | None = None, retry_exceptions: tuple | list | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None)\nExecute a pipeline synchronously and return its results.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run. Must be a valid identifier.\n\n\n\ninputs\ndict \\| None\nOverride pipeline input values. Example: {\"data_date\": \"2025-04-28\"}\nNone\n\n\nfinal_vars\nlist[str] \\| None\nSpecify which output variables to return. Example: [\"model\", \"metrics\"]\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor. Example: {\"model\": \"LogisticRegression\"}\nNone\n\n\ncache\ndict \\| None\nCache configuration for results. Example: {\"recompute\": [\"node1\", \"final_node\"]}\nNone\n\n\nexecutor_cfg\nstr \\| dict \\| ExecutorConfig \\| None\nExecution configuration, can be: - str: Executor name, e.g. “threadpool”, “local” - dict: Raw config, e.g. {\"type\": \"threadpool\", \"max_workers\": 4} - ExecutorConfig: Structured config object\nNone\n\n\nwith_adapter_cfg\ndict \\| WithAdapterConfig \\| None\nAdapter settings for pipeline execution. Example: {\"opentelemetry\": True, \"tracker\": False}\nNone\n\n\npipeline_adapter_cfg\ndict \\| PipelineAdapterConfig \\| None\nPipeline-specific adapter settings. Example: {\"tracker\": {\"project_id\": \"123\", \"tags\": {\"env\": \"prod\"}}}\nNone\n\n\nproject_adapter_cfg\ndict \\| ProjectAdapterConfig \\| None\nProject-level adapter settings. Example: {\"opentelemetry\": {\"host\": \"http://localhost:4317\"}}\nNone\n\n\nadapter\ndict[str, Any] \\| None\nCustom adapter instance for pipeline Example: {\"ray_graph_adapter\": RayGraphAdapter()}\nNone\n\n\nreload\nbool\nForce reload of pipeline configuration.\nFalse\n\n\nlog_level\nstr \\| None\nLogging level for the execution. Valid values: “DEBUG”, “INFO”, “WARNING”, “ERROR”, “CRITICAL”\nNone\n\n\nmax_retries\nint \\| None\nMaximum number of retries for execution.\nNone\n\n\nretry_delay\nfloat \\| None\nDelay between retries in seconds.\nNone\n\n\njitter_factor\nfloat \\| None\nRandom jitter factor to add to retry delay\nNone\n\n\nretry_exceptions\ntuple \\| list \\| None\nExceptions that trigger a retry.\nNone\n\n\non_success\nCallable \\| tuple[Callable, tuple \\| None, dict \\| None] \\| None\nCallback to run on successful pipeline execution.\nNone\n\n\non_failure\nCallable \\| tuple[Callable, tuple \\| None, dict \\| None] \\| None\nCallback to run on pipeline execution failure.\nNone\n\n\n\nReturns: dict[str, Any] - Pipeline execution results, mapping output variable names to their computed values.\nRaises:\n\nValueError: If pipeline name doesn’t exist or configuration is invalid.\nImportError: If pipeline module cannot be imported.\nRuntimeError: If execution fails due to pipeline or adapter errors.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Simple execution\nresult = manager.run(\"my_pipeline\")\n\n# With custom inputs\nresult = manager.run(\n    \"ml_pipeline\",\n    inputs={\"data_date\": \"2025-01-01\"},\n    final_vars=[\"model\", \"metrics\"]\n)\n\n\n\n\nnew(self, name: str, overwrite: bool = False)\nCreate a new pipeline with the given name.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline. Must be a valid Python identifier.\n\n\n\noverwrite\nbool\nWhether to overwrite existing pipeline with same name.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If name is invalid or pipeline exists and overwrite=False.\nRuntimeError: If file creation fails.\nPermissionError: If lacking write permissions.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\n# Create new pipeline\nmanager = PipelineManager()\nmanager.new(\"data_transformation\")\n\n# Overwrite existing pipeline\nmanager.new(\"data_transformation\", overwrite=True)\n\n\n\n\ndelete(self, name: str)\nDelete an existing pipeline.\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to delete.\n\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nRuntimeError: If deletion fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\nmanager.delete(\"old_pipeline\")\n\n\n\n\nshow_pipelines(self, format: str = \"table\")\nDisplay a summary of all available pipelines.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nformat\nstr\nOutput format for the list (“table”, “json”, “yaml”).\n\"table\"\n\n\n\nReturns: None\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show pipelines in table format (default)\nmanager.show_pipelines()\n\n# Show pipelines in JSON format\nmanager.show_pipelines(format=\"json\")\n\n\n\n\nadd_hook(self, name: str, type: HookType, to: str, function_name: str)\nAdd a hook to a specific pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to add the hook to.\n\n\n\ntype\nHookType\nType of the hook (e.g., HookType.MQTT_BUILD_CONFIG).\n\n\n\nto\nstr\nDestination of the hook (e.g., “mqtt”).\n\n\n\nfunction_name\nstr\nName of the function to be called as the hook.\n\n\n\n\nReturns: None\nRaises:\n\nValueError: If the pipeline does not exist or hook type is invalid.\nFileExistsError: If a hook with the same name and type already exists.\n\n\n\nfrom flowerpower.pipeline import PipelineManager, HookType\n\nmanager = PipelineManager()\nmanager.add_hook(\n    name=\"my_pipeline\",\n    type=HookType.MQTT_BUILD_CONFIG,\n    to=\"mqtt\",\n    function_name=\"build_mqtt_config\"\n)\n\n\n\n\nremove_hook(self, name: str, type: HookType, function_name: str)\nRemove a hook from a specific pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to remove the hook from.\n\n\n\ntype\nHookType\nType of the hook to remove.\n\n\n\nfunction_name\nstr\nName of the function that was used as the hook.\n\n\n\n\nReturns: None\nRaises: FileNotFoundError: If the pipeline or hook does not exist.\n\n\nfrom flowerpower.pipeline import PipelineManager, HookType\n\nmanager = PipelineManager()\nmanager.remove_hook(\n    name=\"my_pipeline\",\n    type=HookType.MQTT_BUILD_CONFIG,\n    function_name=\"build_mqtt_config\"\n)\n\n\n\n\nimport_pipeline(self, name: str, src_base_dir: str, src_fs: AbstractFileSystem | None = None, src_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nImport a pipeline from another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline in the current project.\n\n\n\nsrc_base_dir\nstr\nSource FlowerPower project directory or URI. Examples: - Local: \"/path/to/other/project\" - S3: \"s3://bucket/project\" - GitHub: \"github://org/repo/project\"\n\n\n\nsrc_fs\nAbstractFileSystem \\| None\nPre-configured source filesystem. Example: S3FileSystem(anon=False)\nNone\n\n\nsrc_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for source filesystem access. Example: {\"key\": \"ACCESS_KEY\", \"secret\": \"SECRET_KEY\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipeline if name exists.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If pipeline name exists and overwrite=False.\nFileNotFoundError: If source pipeline not found.\nRuntimeError: If import fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\nfrom s3fs import S3FileSystem\n\nmanager = PipelineManager()\n\n# Import from local filesystem\nmanager.import_pipeline(\n    \"new_pipeline\",\n    \"/path/to/other/project\"\n)\n\n# Import from S3 with custom filesystem\ns3 = S3FileSystem(anon=False)\nmanager.import_pipeline(\n    \"s3_pipeline\",\n    \"s3://bucket/project\",\n    src_fs=s3\n)\n\n\n\n\nimport_many(self, names: list[str], src_base_dir: str, src_fs: AbstractFileSystem | None = None, src_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nImport multiple pipelines from another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnames\nlist[str]\nList of pipeline names to import.\n\n\n\nsrc_base_dir\nstr\nSource FlowerPower project directory or URI. Examples: - Local: \"/path/to/other/project\" - S3: \"s3://bucket/project\" - GitHub: \"github://org/repo/project\"\n\n\n\nsrc_fs\nAbstractFileSystem \\| None\nPre-configured source filesystem. Example: S3FileSystem(anon=False)\nNone\n\n\nsrc_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for source filesystem access. Example: {\"key\": \"ACCESS_KEY\", \"secret\": \"SECRET_KEY\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipelines if names exist.\nFalse\n\n\n\nReturns: None\nRaises:\n\nValueError: If any pipeline name exists and overwrite=False.\nFileNotFoundError: If any source pipeline not found.\nRuntimeError: If import fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Import multiple pipelines\nmanager.import_many(\n    names=[\"pipeline1\", \"pipeline2\"],\n    src_base_dir=\"/path/to/other/project\"\n)\n\n# Import multiple pipelines from S3\nmanager.import_many(\n    names=[\"s3_pipeline_a\", \"s3_pipeline_b\"],\n    src_base_dir=\"s3://bucket/source\",\n    src_storage_options={\n        \"key\": \"ACCESS_KEY\",\n        \"secret\": \"SECRET_KEY\"\n    }\n)\n\n\n\n\nexport_pipeline(self, name: str, dest_base_dir: str, dest_fs: AbstractFileSystem | None = None, dest_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nExport a pipeline to another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to export.\n\n\n\ndest_base_dir\nstr\nDestination FlowerPower project directory or URI. Examples: - Local: \"/path/to/backup\" - S3: \"s3://bucket/backups\" - GCS: \"gs://bucket/backups\"\n\n\n\ndest_fs\nAbstractFileSystem \\| None\nPre-configured destination filesystem. Example: GCSFileSystem(project='my-project')\nNone\n\n\ndest_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for destination filesystem access. Example: {\"token\": \"my_token\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipeline in destination if name exists.\nFalse\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If the pipeline does not exist in the current project.\nFileExistsError: If destination pipeline exists and overwrite=False.\nRuntimeError: If export fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\nfrom gcsfs import GCSFileSystem\n\nmanager = PipelineManager()\n\n# Export to local backup\nmanager.export_pipeline(\n    \"my_pipeline\",\n    \"/path/to/backup\"\n)\n\n# Export to Google Cloud Storage\ngcs = GCSFileSystem(project='my-project')\nmanager.export_pipeline(\n    \"prod_pipeline\",\n    \"gs://my-bucket/backups\",\n    dest_fs=gcs\n)\n\n\n\n\nexport_many(self, names: list[str], dest_base_dir: str, dest_fs: AbstractFileSystem | None = None, dest_storage_options: dict | BaseStorageOptions | None = None, overwrite: bool = False)\nExport multiple pipelines to another FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnames\nlist[str]\nList of pipeline names to export.\n\n\n\ndest_base_dir\nstr\nDestination FlowerPower project directory or URI. Examples: - Local: \"/path/to/backup\" - S3: \"s3://bucket/backups\" - GCS: \"gs://bucket/backups\"\n\n\n\ndest_fs\nAbstractFileSystem \\| None\nPre-configured destination filesystem. Example: GCSFileSystem(project='my-project')\nNone\n\n\ndest_storage_options\ndict \\| BaseStorageOptions \\| None\nOptions for destination filesystem access. Example: {\"token\": \"my_token\"}\nNone\n\n\noverwrite\nbool\nWhether to replace existing pipelines in destination if names exist.\nFalse\n\n\n\nReturns: None\nRaises:\n\nFileNotFoundError: If any pipeline does not exist in the current project.\nFileExistsError: If any destination pipeline exists and overwrite=False.\nRuntimeError: If export fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Export multiple pipelines\nmanager.export_many(\n    names=[\"pipeline1\", \"pipeline2\"],\n    dest_base_dir=\"/path/to/backup\"\n)\n\n# Export multiple pipelines from S3\nmanager.export_many(\n    names=[\"s3_pipeline_a\", \"s3_pipeline_b\"],\n    dest_base_dir=\"s3://bucket/backups\",\n    dest_storage_options={\n        \"key\": \"ACCESS_KEY\",\n        \"secret\": \"SECRET_KEY\"\n    }\n)\n\n\n\n\nshow_dag(self, name: str, format: str = \"png\", show_outputs: bool = False, display_html: bool = False)\nGenerate and display the Directed Acyclic Graph (DAG) of a pipeline.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize.\n\n\n\nformat\nstr\nOutput format for the DAG (“png”, “svg”, “html”, “dot”).\n\"png\"\n\n\nshow_outputs\nbool\nWhether to include output nodes in the DAG.\nFalse\n\n\ndisplay_html\nbool\nWhether to display the HTML directly in the notebook (only for “html” format).\nFalse\n\n\n\nReturns: None (displays the DAG directly or saves it to a file).\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nValueError: If format is invalid or visualization fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show DAG as PNG\nmanager.show_dag(\"my_pipeline\")\n\n# Show DAG as SVG with outputs\nmanager.show_dag(\"ml_pipeline\", format=\"svg\", show_outputs=True)\n\n\n\n\nshow_execution_graph(self, name: str, format: str = \"png\", show_outputs: bool = False, display_html: bool = False, inputs: dict | None = None, config: dict | None = None)\nGenerate and display the execution graph of a pipeline, considering inputs and configuration.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize.\n\n\n\nformat\nstr\nOutput format for the graph (“png”, “svg”, “html”, “dot”).\n\"png\"\n\n\nshow_outputs\nbool\nWhether to include output nodes in the graph.\nFalse\n\n\ndisplay_html\nbool\nWhether to display the HTML directly in the notebook (only for “html” format).\nFalse\n\n\ninputs\ndict \\| None\nInput values to consider for graph generation.\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor.\nNone\n\n\n\nReturns: None (displays the graph directly or saves it to a file).\nRaises:\n\nFileNotFoundError: If the pipeline does not exist.\nValueError: If format is invalid or visualization fails.\n\n\n\nfrom flowerpower.pipeline import PipelineManager\n\nmanager = PipelineManager()\n\n# Show execution graph\nmanager.show_execution_graph(\"my_pipeline\", inputs={\"data_date\": \"2025-01-01\"})"
  },
  {
    "objectID": "api/jobqueuemanager.html",
    "href": "api/jobqueuemanager.html",
    "title": "JobQueueManager",
    "section": "",
    "text": "Module: flowerpower.job_queue.JobQueueManager\nThe JobQueueManager is an abstract base class that defines the interface for job queue operations in FlowerPower. It is responsible for enqueuing, scheduling, and managing jobs.\n\n\n\n\n__init__(self, type: str | None = None, name: str | None = None, base_dir: str | None = None, backend: BaseBackend | None = None, storage_options: dict | None = None, fs: AbstractFileSystem | None = None, **kwargs)\nInitializes the JobQueueManager.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nParameter\nType\nDescription\n\n\n:———-\n:—–\n:————\n\n\ntype\nstr \\| None\nThe type of job queue backend (e.g., “rq”).\n\n\nname\nstr \\| None\nThe name of the scheduler.\n\n\nbase_dir\nstr \\| None\nThe base directory of the project.\n\n\nbackend\nBaseBackend \\| None\nA backend instance.\n\n\nstorage_options\ndict \\| None\nStorage options for the filesystem.\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nis_worker_running\nbool\nIndicates if a worker is currently running.\n\n\nis_scheduler_running\nbool\nIndicates if the scheduler is currently running.\n\n\n\n\n\n\n\n\nenqueue_pipeline(self, name: str, *args, **kwargs)\nEnqueues a pipeline for immediate execution.\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\n*args\nAny\nPositional arguments for the job.\n\n\n**kwargs\nAny\nKeyword arguments for the job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If the pipeline name is invalid.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Assuming manager is an instance of a concrete JobQueueManager subclass\njob = manager.enqueue_pipeline(\"my_data_pipeline\", data_path=\"/data/new.csv\")\nprint(f\"Enqueued job: {job.id}\")\n\n\n\n\nschedule_pipeline(self, name: str, *args, **kwargs)\nSchedules a pipeline for future or recurring execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\n*args\nAny\nPositional arguments for the job.\n\n\n**kwargs\nAny\nKeyword arguments for the job (e.g., cron_string, interval).\n\n\n\nReturns: ScheduledJob - The scheduled job object.\nRaises: ValueError: If the pipeline name is invalid or scheduling parameters are insufficient.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Schedule a pipeline to run every day at midnight\nscheduled_job = manager.schedule_pipeline(\n    \"daily_report_pipeline\",\n    cron_string=\"0 0 * * *\"\n)\nprint(f\"Scheduled job: {scheduled_job.id}\")\n\n\n\n\nstart_worker(self, queue_name: str | list[str] | None = None, **kwargs)\nStarts a worker process to process jobs from the queue.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueue_name\nstr \\| list[str] \\| None\nThe name(s) of the queue(s) to listen to. Defaults to all queues.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the worker.\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker fails to start.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Start a worker for a specific queue\nmanager.start_worker(\"high_priority_queue\")\n\n# Start a worker for multiple queues\nmanager.start_worker([\"default\", \"low_priority\"])\n\n\n\n\nstop_worker(self)\nStops the currently running worker process.\nReturns: None\nRaises: RuntimeError: If stopping the worker fails.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\nmanager.stop_worker()\n\n\n\n\nstart_worker_pool(self, num_workers: int = 1, queue_name: str | list[str] | None = None, **kwargs)\nStarts a pool of worker processes.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnum_workers\nint\nThe number of worker processes to start.\n\n\nqueue_name\nstr \\| list[str] \\| None\nThe name(s) of the queue(s) for the workers to listen to. Defaults to all queues.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the worker processes.\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker pool fails to start.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Start a pool of 4 workers\nmanager.start_worker_pool(num_workers=4)\n\n\n\n\nstop_worker_pool(self)\nStops all worker processes in the pool.\nReturns: None\nRaises: RuntimeError: If stopping the worker pool fails.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\nmanager.stop_worker_pool()\n\n\n\n\nenqueue(self, func: Callable, *args, **kwargs)\nEnqueues a job for immediate, delayed, or scheduled execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job (e.g., job_id, timeout).\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\ndef my_task(x, y):\n    return x + y\n\njob = manager.enqueue(my_task, 1, 2, job_id=\"my_sum_job\")\nprint(f\"Enqueued job: {job.id}\")\n\n\n\n\nenqueue_in(self, delay: timedelta | int | str, func: Callable, *args, **kwargs)\nEnqueues a job to run after a specified delay.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndelay\ntimedelta | int | str\nThe delay before execution. Can be a timedelta object, an integer (seconds), or a string (e.g., “1m” for 1 minute).\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If delay is invalid or func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\nfrom datetime import timedelta\n\ndef send_notification(message):\n    print(f\"Notification: {message}\")\n\n# Enqueue a job to run in 5 minutes\njob = manager.enqueue_in(timedelta(minutes=5), send_notification, \"Your report is ready!\")\n\n# Enqueue a job to run in 30 seconds (integer delay)\njob = manager.enqueue_in(30, send_notification, \"Quick update!\")\n\n# Enqueue a job to run in 1 hour (string delay)\njob = manager.enqueue_in(\"1h\", send_notification, \"Hourly reminder!\")\n\n\n\n\nenqueue_at(self, datetime_obj: datetime, func: Callable, *args, **kwargs)\nEnqueues a job to run at a specific datetime.\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndatetime_obj\ndatetime\nThe datetime to execute the job.\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If datetime_obj is in the past or func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\nfrom datetime import datetime\n\ndef generate_monthly_report(month, year):\n    print(f\"Generating report for {month}/{year}\")\n\n# Enqueue a job to run at a specific future date and time\ntarget_time = datetime(2025, 1, 1, 9, 0, 0)\njob = manager.enqueue_at(target_time, generate_monthly_report, 1, 2025)\n\n\n\n\nadd_schedule(self, id: str, func: Callable, cron_string: str | None = None, interval: int | None = None, repeat: int | None = None, enabled: bool = True, **kwargs)\nSchedules a job for repeated or one-time execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nid\nstr\nA unique identifier for the scheduled job.\n\n\nfunc\nCallable\nThe function to execute.\n\n\ncron_string\nstr | None\nA cron string for recurring schedules (e.g., “0 0 * * *” for daily at midnight).\n\n\ninterval\nint | None\nInterval in seconds for recurring schedules.\n\n\nrepeat\nint | None\nNumber of times to repeat the job. None for infinite.\n\n\nenabled\nbool\nWhether the schedule is active.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the function and job.\n\n\n\nReturns: ScheduledJob - The scheduled job object.\nRaises: ValueError: If scheduling parameters are invalid or insufficient.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\ndef clean_temp_files():\n    print(\"Cleaning temporary files...\")\n\n# Schedule a job to clean temp files every hour\nscheduled_job = manager.add_schedule(\n    id=\"hourly_cleanup\",\n    func=clean_temp_files,\n    interval=3600 # Every hour\n)\n\n# Schedule a job using a cron string (every Monday at 9 AM)\nscheduled_job = manager.add_schedule(\n    id=\"weekly_summary\",\n    func=lambda: print(\"Generating weekly summary...\"),\n    cron_string=\"0 9 * * MON\"\n)\n\n\n\n\nget_job_result(self, job: str | Job, delete_result: bool = False)\nGets the result of a completed job.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\njob\nstr | Job\nThe job ID or Job object.\n\n\ndelete_result\nbool\nIf True, deletes the result after retrieval.\n\n\n\nReturns: Any - The result of the job execution.\nRaises:\n\nJobNotFinishedError: If the job has not completed yet.\nJobDoesNotExistError: If the job ID is not found.\n\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Assuming 'my_job_id' is the ID of a completed job\nresult = manager.get_job_result(\"my_job_id\")\nprint(f\"Job result: {result}\")\n\n\n\n\nget_jobs(self, queue_name: str | list[str] | None = None)\nGets all jobs from specified queues.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueue_name\nstr | list[str] | None\nThe name of the queue(s). Defaults to all queues.\n\n\n\nReturns: list[Job] - A list of job objects.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Get all jobs from the default queue\nall_jobs = manager.get_jobs(\"default\")\n\n# Get jobs from multiple queues\npriority_jobs = manager.get_jobs([\"high_priority\", \"medium_priority\"])\n\n\n\n\nget_schedules(self, id: str | list[str] | None = None)\nGets all schedules from the scheduler.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nid\nstr | list[str] | None\nThe ID(s) of the schedule(s). Defaults to all schedules.\n\n\n\nReturns: list[ScheduledJob] - A list of scheduled job objects.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Get all active schedules\nall_schedules = manager.get_schedules()\n\n# Get a specific schedule\nmy_schedule = manager.get_schedules(id=\"hourly_cleanup\")"
  },
  {
    "objectID": "api/jobqueuemanager.html#initialization",
    "href": "api/jobqueuemanager.html#initialization",
    "title": "JobQueueManager",
    "section": "",
    "text": "__init__(self, type: str | None = None, name: str | None = None, base_dir: str | None = None, backend: BaseBackend | None = None, storage_options: dict | None = None, fs: AbstractFileSystem | None = None, **kwargs)\nInitializes the JobQueueManager.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nParameter\nType\nDescription\n\n\n:———-\n:—–\n:————\n\n\ntype\nstr \\| None\nThe type of job queue backend (e.g., “rq”).\n\n\nname\nstr \\| None\nThe name of the scheduler.\n\n\nbase_dir\nstr \\| None\nThe base directory of the project.\n\n\nbackend\nBaseBackend \\| None\nA backend instance.\n\n\nstorage_options\ndict \\| None\nStorage options for the filesystem.\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance."
  },
  {
    "objectID": "api/jobqueuemanager.html#attributes",
    "href": "api/jobqueuemanager.html#attributes",
    "title": "JobQueueManager",
    "section": "",
    "text": "Attribute\nType\nDescription\n\n\n\n\nis_worker_running\nbool\nIndicates if a worker is currently running.\n\n\nis_scheduler_running\nbool\nIndicates if the scheduler is currently running."
  },
  {
    "objectID": "api/jobqueuemanager.html#methods-1",
    "href": "api/jobqueuemanager.html#methods-1",
    "title": "JobQueueManager",
    "section": "",
    "text": "enqueue_pipeline(self, name: str, *args, **kwargs)\nEnqueues a pipeline for immediate execution.\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\n*args\nAny\nPositional arguments for the job.\n\n\n**kwargs\nAny\nKeyword arguments for the job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If the pipeline name is invalid.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Assuming manager is an instance of a concrete JobQueueManager subclass\njob = manager.enqueue_pipeline(\"my_data_pipeline\", data_path=\"/data/new.csv\")\nprint(f\"Enqueued job: {job.id}\")\n\n\n\n\nschedule_pipeline(self, name: str, *args, **kwargs)\nSchedules a pipeline for future or recurring execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\n*args\nAny\nPositional arguments for the job.\n\n\n**kwargs\nAny\nKeyword arguments for the job (e.g., cron_string, interval).\n\n\n\nReturns: ScheduledJob - The scheduled job object.\nRaises: ValueError: If the pipeline name is invalid or scheduling parameters are insufficient.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Schedule a pipeline to run every day at midnight\nscheduled_job = manager.schedule_pipeline(\n    \"daily_report_pipeline\",\n    cron_string=\"0 0 * * *\"\n)\nprint(f\"Scheduled job: {scheduled_job.id}\")\n\n\n\n\nstart_worker(self, queue_name: str | list[str] | None = None, **kwargs)\nStarts a worker process to process jobs from the queue.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueue_name\nstr \\| list[str] \\| None\nThe name(s) of the queue(s) to listen to. Defaults to all queues.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the worker.\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker fails to start.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Start a worker for a specific queue\nmanager.start_worker(\"high_priority_queue\")\n\n# Start a worker for multiple queues\nmanager.start_worker([\"default\", \"low_priority\"])\n\n\n\n\nstop_worker(self)\nStops the currently running worker process.\nReturns: None\nRaises: RuntimeError: If stopping the worker fails.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\nmanager.stop_worker()\n\n\n\n\nstart_worker_pool(self, num_workers: int = 1, queue_name: str | list[str] | None = None, **kwargs)\nStarts a pool of worker processes.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnum_workers\nint\nThe number of worker processes to start.\n\n\nqueue_name\nstr \\| list[str] \\| None\nThe name(s) of the queue(s) for the workers to listen to. Defaults to all queues.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the worker processes.\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker pool fails to start.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Start a pool of 4 workers\nmanager.start_worker_pool(num_workers=4)\n\n\n\n\nstop_worker_pool(self)\nStops all worker processes in the pool.\nReturns: None\nRaises: RuntimeError: If stopping the worker pool fails.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\nmanager.stop_worker_pool()\n\n\n\n\nenqueue(self, func: Callable, *args, **kwargs)\nEnqueues a job for immediate, delayed, or scheduled execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job (e.g., job_id, timeout).\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\ndef my_task(x, y):\n    return x + y\n\njob = manager.enqueue(my_task, 1, 2, job_id=\"my_sum_job\")\nprint(f\"Enqueued job: {job.id}\")\n\n\n\n\nenqueue_in(self, delay: timedelta | int | str, func: Callable, *args, **kwargs)\nEnqueues a job to run after a specified delay.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndelay\ntimedelta | int | str\nThe delay before execution. Can be a timedelta object, an integer (seconds), or a string (e.g., “1m” for 1 minute).\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If delay is invalid or func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\nfrom datetime import timedelta\n\ndef send_notification(message):\n    print(f\"Notification: {message}\")\n\n# Enqueue a job to run in 5 minutes\njob = manager.enqueue_in(timedelta(minutes=5), send_notification, \"Your report is ready!\")\n\n# Enqueue a job to run in 30 seconds (integer delay)\njob = manager.enqueue_in(30, send_notification, \"Quick update!\")\n\n# Enqueue a job to run in 1 hour (string delay)\njob = manager.enqueue_in(\"1h\", send_notification, \"Hourly reminder!\")\n\n\n\n\nenqueue_at(self, datetime_obj: datetime, func: Callable, *args, **kwargs)\nEnqueues a job to run at a specific datetime.\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndatetime_obj\ndatetime\nThe datetime to execute the job.\n\n\nfunc\nCallable\nThe function to execute.\n\n\n*args\nAny\nPositional arguments for the function.\n\n\n**kwargs\nAny\nKeyword arguments for the function and job.\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If datetime_obj is in the past or func is not callable.\n\n\nfrom flowerpower.job_queue import JobQueueManager\nfrom datetime import datetime\n\ndef generate_monthly_report(month, year):\n    print(f\"Generating report for {month}/{year}\")\n\n# Enqueue a job to run at a specific future date and time\ntarget_time = datetime(2025, 1, 1, 9, 0, 0)\njob = manager.enqueue_at(target_time, generate_monthly_report, 1, 2025)\n\n\n\n\nadd_schedule(self, id: str, func: Callable, cron_string: str | None = None, interval: int | None = None, repeat: int | None = None, enabled: bool = True, **kwargs)\nSchedules a job for repeated or one-time execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nid\nstr\nA unique identifier for the scheduled job.\n\n\nfunc\nCallable\nThe function to execute.\n\n\ncron_string\nstr | None\nA cron string for recurring schedules (e.g., “0 0 * * *” for daily at midnight).\n\n\ninterval\nint | None\nInterval in seconds for recurring schedules.\n\n\nrepeat\nint | None\nNumber of times to repeat the job. None for infinite.\n\n\nenabled\nbool\nWhether the schedule is active.\n\n\n**kwargs\nAny\nAdditional keyword arguments for the function and job.\n\n\n\nReturns: ScheduledJob - The scheduled job object.\nRaises: ValueError: If scheduling parameters are invalid or insufficient.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\ndef clean_temp_files():\n    print(\"Cleaning temporary files...\")\n\n# Schedule a job to clean temp files every hour\nscheduled_job = manager.add_schedule(\n    id=\"hourly_cleanup\",\n    func=clean_temp_files,\n    interval=3600 # Every hour\n)\n\n# Schedule a job using a cron string (every Monday at 9 AM)\nscheduled_job = manager.add_schedule(\n    id=\"weekly_summary\",\n    func=lambda: print(\"Generating weekly summary...\"),\n    cron_string=\"0 9 * * MON\"\n)\n\n\n\n\nget_job_result(self, job: str | Job, delete_result: bool = False)\nGets the result of a completed job.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\njob\nstr | Job\nThe job ID or Job object.\n\n\ndelete_result\nbool\nIf True, deletes the result after retrieval.\n\n\n\nReturns: Any - The result of the job execution.\nRaises:\n\nJobNotFinishedError: If the job has not completed yet.\nJobDoesNotExistError: If the job ID is not found.\n\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Assuming 'my_job_id' is the ID of a completed job\nresult = manager.get_job_result(\"my_job_id\")\nprint(f\"Job result: {result}\")\n\n\n\n\nget_jobs(self, queue_name: str | list[str] | None = None)\nGets all jobs from specified queues.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueue_name\nstr | list[str] | None\nThe name of the queue(s). Defaults to all queues.\n\n\n\nReturns: list[Job] - A list of job objects.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Get all jobs from the default queue\nall_jobs = manager.get_jobs(\"default\")\n\n# Get jobs from multiple queues\npriority_jobs = manager.get_jobs([\"high_priority\", \"medium_priority\"])\n\n\n\n\nget_schedules(self, id: str | list[str] | None = None)\nGets all schedules from the scheduler.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nid\nstr | list[str] | None\nThe ID(s) of the schedule(s). Defaults to all schedules.\n\n\n\nReturns: list[ScheduledJob] - A list of scheduled job objects.\n\n\nfrom flowerpower.job_queue import JobQueueManager\n\n# Get all active schedules\nall_schedules = manager.get_schedules()\n\n# Get a specific schedule\nmy_schedule = manager.get_schedules(id=\"hourly_cleanup\")"
  },
  {
    "objectID": "api/cli_job_queue.html",
    "href": "api/cli_job_queue.html",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "This section details the commands available under flowerpower job-queue.\n\n\nStart a worker or worker pool to process jobs.\nThis command starts a worker process (or a pool of worker processes) that will execute jobs from the queue. The worker will continue running until stopped or can be run in the background.\n\n\nflowerpower job-queue start_worker [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nbackground\nstr\nRun the worker in the background\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nnum_workers\nstr\nNumber of worker processes to start (pool mode)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue start-worker\n\n# Start a worker for a specific backend type\n$ flowerpower job-queue start-worker --type rq\n\n# Start a worker pool with 4 processes\n$ flowerpower job-queue start-worker --num-workers 4\n\n# Run a worker in the background\n$ flowerpower job-queue start-worker --background\n\n# Set a specific logging level\n$ flowerpower job-queue start-worker --log-level debug\n\n\n\n\n\nCancel a job or multiple jobs in the queue.\nThis command stops a job from executing (if it hasn’t started yet) or signals it to stop (if already running). Canceling is different from deleting as it maintains the job history but prevents execution.\n\n\nflowerpower job-queue cancel_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to cancel (ignored if –all is used)\nRequired\n\n\nall\nstr\nCancel all jobs instead of a specific one\nRequired\n\n\nqueue_name\nstr\nFor RQ only, specifies the queue to cancel jobs from\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue cancel-job job-123456\n\n# Cancel all jobs in the default queue\n$ flowerpower job-queue cancel-job --all dummy-id\n\n# Cancel all jobs in a specific queue (RQ only)\n$ flowerpower job-queue cancel-job --all dummy-id --queue-name high-priority\n\n# Specify the backend type explicitly\n$ flowerpower job-queue cancel-job job-123456 --type rq\n\n\n\n\n\nCancel a specific schedule.\nNote: This is different from deleting a schedule as it only stops it from running but keeps its configuration.\n\n\nflowerpower job-queue cancel_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to cancel\nRequired\n\n\nall\nstr\nIf True, cancel all schedules\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired\n\n\n\n\n\n\n\n\nDelete a specific job.\n\n\nflowerpower job-queue delete_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to delete\nRequired\n\n\nall\nstr\nIf True, delete all jobs\nRequired\n\n\nqueue_name\nstr\nName of the queue (RQ only). If provided and all is True, delete all jobs in the queue\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired\n\n\n\n\n\n\n\n\nDelete a specific schedule.\n\n\nflowerpower job-queue delete_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to delete\nRequired\n\n\nall\nstr\nIf True, delete all schedules\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired\n\n\n\n\n\n\n\n\nShow all job IDs in the job queue.\nThis command displays all job IDs currently in the system, helping you identify jobs for other operations like getting results, canceling, or deleting jobs.\n\n\nflowerpower job-queue show_job_ids [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-job-ids\n\n# Show job IDs for a specific queue type\n$ flowerpower job-queue show-job-ids --type rq\n\n# Show job IDs with a custom scheduler configuration\n$ flowerpower job-queue show-job-ids --name my-scheduler\n\n# Show job IDs with debug logging\n$ flowerpower job-queue show-job-ids --log-level debug\n\n\n\n\n\nShow all schedule IDs in the job queue.\nThis command displays all schedule IDs currently in the system, helping you identify schedules for other operations like pausing, resuming, or deleting schedules.\n\n\nflowerpower job-queue show_schedule_ids [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-schedule-ids\n\n# Show schedule IDs for RQ\n$ flowerpower job-queue show-schedule-ids --type rq\n\n# Show schedule IDs with a custom scheduler configuration\n$ flowerpower job-queue show-schedule-ids --name my-scheduler\n\n# Show schedule IDs with debug logging\n$ flowerpower job-queue show-schedule-ids --log-level debug\n\n\n\n\n\nPause a schedule or multiple schedules.\nThis command temporarily stops a scheduled job from running while maintaining its configuration. Paused schedules can be resumed later. Note that this functionality is only available for APScheduler workers.\n\n\nflowerpower job-queue pause_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to pause (ignored if –all is used)\nRequired\n\n\nall\nstr\nPause all schedules instead of a specific one\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue pause-schedule schedule-123456\n\n# Pause all schedules\n$ flowerpower job-queue pause-schedule --all dummy-id\n\n# Note: Schedule pausing is not supported for RQ workers\n\n\n\n\n\nResume a paused schedule or multiple schedules.\nThis command restarts previously paused schedules, allowing them to run again according to their original configuration. Note that this functionality is only available for APScheduler workers.\n\n\nflowerpower job-queue resume_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to resume (ignored if –all is used)\nRequired\n\n\nall\nstr\nResume all schedules instead of a specific one\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue resume-schedule schedule-123456\n\n# Resume all schedules\n$ flowerpower job-queue resume-schedule --all dummy-id\n\n# Note: Schedule resuming is not supported for RQ workers\n\n# Set a specific logging level\n$ flowerpower job-queue resume-schedule schedule-123456 --log-level debug\n\n\n\n\n\nDisplay detailed information about all jobs in the queue.\nThis command shows comprehensive information about jobs including their status, creation time, execution time, and other details in a user-friendly format.\n\n\nflowerpower job-queue show_jobs [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nqueue_name\nstr\nName of the queue to show jobs from (RQ only)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the job information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-jobs\n\n# Show jobs for a specific queue type\n$ flowerpower job-queue show-jobs --type rq\n\n# Show jobs in a specific RQ queue\n$ flowerpower job-queue show-jobs --queue-name high-priority\n\n# Display jobs in JSON format\n$ flowerpower job-queue show-jobs --format json\n\n\n\n\n\nDisplay detailed information about all schedules.\nThis command shows comprehensive information about scheduled jobs including their timing configuration, status, and other details in a user-friendly format.\n\n\nflowerpower job-queue show_schedules [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the schedule information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-schedules\n\n# Show schedules for RQ\n$ flowerpower job-queue show-schedules --type rq\n\n# Display schedules in JSON format\n$ flowerpower job-queue show-schedules --format json\n\n\n\n\n\nEnqueue a pipeline for execution via the job queue.\nThis command queues a pipeline for asynchronous execution using the configured job queue backend (RQ). The job can be executed immediately, after a delay, or at a specific time.\n\n\nflowerpower job-queue enqueue_pipeline [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to enqueue\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nrun_in\nstr\nDelay before execution (duration format like ‘5m’, ‘1h’, ‘30s’)\nRequired\n\n\nrun_at\nstr\nSpecific datetime for execution (ISO format)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue enqueue-pipeline my_pipeline\n\n# Enqueue with custom inputs\n$ flowerpower job-queue enqueue-pipeline my_pipeline --inputs '{\"data_path\": \"data/file.csv\"}'\n\n# Enqueue with delay\n$ flowerpower job-queue enqueue-pipeline my_pipeline --run-in \"30m\"\n\n# Enqueue for specific time\n$ flowerpower job-queue enqueue-pipeline my_pipeline --run-at \"2025-01-01T09:00:00\"\n\n\n\n\n\nSchedule a pipeline for recurring or future execution.\nThis command sets up recurring or future execution of a pipeline using cron expressions or interval-based scheduling via the configured job queue backend.\n\n\nflowerpower job-queue schedule_pipeline [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to schedule\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ncron\nstr\nCron expression for scheduling (e.g., ’0 9 * * *’ for 9 AM daily)\nRequired\n\n\ninterval\nstr\nInterval for recurring execution (duration format)\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nschedule_id\nstr\nCustom identifier for the schedule\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue schedule-pipeline my_pipeline --cron \"0 9 * * *\"\n\n# Schedule every 30 minutes\n$ flowerpower job-queue schedule-pipeline my_pipeline --interval \"30m\"\n\n# Schedule with custom inputs and ID\n$ flowerpower job-queue schedule-pipeline my_pipeline --cron \"0 0 * * *\" \\\\\n--inputs '{\"env\": \"prod\"}' --schedule-id \"nightly-prod\"\n\n\n\n\n\nExecute a specific job by its ID.\nThis command runs a job that has been previously enqueued in the job queue. The job will be executed immediately regardless of its original schedule.\n\n\nflowerpower job-queue run_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to run\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue run-job job-123456\n\n# Run a job with a specific backend type\n$ flowerpower job-queue run-job job-123456 --type rq\n\n# Run a job with debug logging\n$ flowerpower job-queue run-job job-123456 --log-level debug\n\n\n\n\n\nList all schedules with detailed status information.\nThis command provides enhanced schedule listing showing trigger configuration, status, next run time, and execution history. This is an enhanced version of show-schedules with more detailed information.\n\n\nflowerpower job-queue list_schedules [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the schedule information\nRequired\n\n\nshow_status\nstr\nInclude schedule status information\nRequired\n\n\nshow_next_run\nstr\nInclude next execution time information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue list-schedules\n\n# List schedules in JSON format\n$ flowerpower job-queue list-schedules --format json\n\n# List schedules without status information\n$ flowerpower job-queue list-schedules --no-show-status\n\n# List schedules for a specific backend\n$ flowerpower job-queue list-schedules --type rq"
  },
  {
    "objectID": "api/cli_job_queue.html#start_worker",
    "href": "api/cli_job_queue.html#start_worker",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Start a worker or worker pool to process jobs.\nThis command starts a worker process (or a pool of worker processes) that will execute jobs from the queue. The worker will continue running until stopped or can be run in the background.\n\n\nflowerpower job-queue start_worker [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nbackground\nstr\nRun the worker in the background\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nnum_workers\nstr\nNumber of worker processes to start (pool mode)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue start-worker\n\n# Start a worker for a specific backend type\n$ flowerpower job-queue start-worker --type rq\n\n# Start a worker pool with 4 processes\n$ flowerpower job-queue start-worker --num-workers 4\n\n# Run a worker in the background\n$ flowerpower job-queue start-worker --background\n\n# Set a specific logging level\n$ flowerpower job-queue start-worker --log-level debug"
  },
  {
    "objectID": "api/cli_job_queue.html#cancel_job",
    "href": "api/cli_job_queue.html#cancel_job",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Cancel a job or multiple jobs in the queue.\nThis command stops a job from executing (if it hasn’t started yet) or signals it to stop (if already running). Canceling is different from deleting as it maintains the job history but prevents execution.\n\n\nflowerpower job-queue cancel_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to cancel (ignored if –all is used)\nRequired\n\n\nall\nstr\nCancel all jobs instead of a specific one\nRequired\n\n\nqueue_name\nstr\nFor RQ only, specifies the queue to cancel jobs from\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue cancel-job job-123456\n\n# Cancel all jobs in the default queue\n$ flowerpower job-queue cancel-job --all dummy-id\n\n# Cancel all jobs in a specific queue (RQ only)\n$ flowerpower job-queue cancel-job --all dummy-id --queue-name high-priority\n\n# Specify the backend type explicitly\n$ flowerpower job-queue cancel-job job-123456 --type rq"
  },
  {
    "objectID": "api/cli_job_queue.html#cancel_schedule",
    "href": "api/cli_job_queue.html#cancel_schedule",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Cancel a specific schedule.\nNote: This is different from deleting a schedule as it only stops it from running but keeps its configuration.\n\n\nflowerpower job-queue cancel_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to cancel\nRequired\n\n\nall\nstr\nIf True, cancel all schedules\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired"
  },
  {
    "objectID": "api/cli_job_queue.html#delete_job",
    "href": "api/cli_job_queue.html#delete_job",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Delete a specific job.\n\n\nflowerpower job-queue delete_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to delete\nRequired\n\n\nall\nstr\nIf True, delete all jobs\nRequired\n\n\nqueue_name\nstr\nName of the queue (RQ only). If provided and all is True, delete all jobs in the queue\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired"
  },
  {
    "objectID": "api/cli_job_queue.html#delete_schedule",
    "href": "api/cli_job_queue.html#delete_schedule",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Delete a specific schedule.\n\n\nflowerpower job-queue delete_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to delete\nRequired\n\n\nall\nstr\nIf True, delete all schedules\nRequired\n\n\ntype\nstr\nType of the job queue (rq)\nRequired\n\n\nname\nstr\nName of the scheduler\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level\nRequired"
  },
  {
    "objectID": "api/cli_job_queue.html#show_job_ids",
    "href": "api/cli_job_queue.html#show_job_ids",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Show all job IDs in the job queue.\nThis command displays all job IDs currently in the system, helping you identify jobs for other operations like getting results, canceling, or deleting jobs.\n\n\nflowerpower job-queue show_job_ids [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-job-ids\n\n# Show job IDs for a specific queue type\n$ flowerpower job-queue show-job-ids --type rq\n\n# Show job IDs with a custom scheduler configuration\n$ flowerpower job-queue show-job-ids --name my-scheduler\n\n# Show job IDs with debug logging\n$ flowerpower job-queue show-job-ids --log-level debug"
  },
  {
    "objectID": "api/cli_job_queue.html#show_schedule_ids",
    "href": "api/cli_job_queue.html#show_schedule_ids",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Show all schedule IDs in the job queue.\nThis command displays all schedule IDs currently in the system, helping you identify schedules for other operations like pausing, resuming, or deleting schedules.\n\n\nflowerpower job-queue show_schedule_ids [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-schedule-ids\n\n# Show schedule IDs for RQ\n$ flowerpower job-queue show-schedule-ids --type rq\n\n# Show schedule IDs with a custom scheduler configuration\n$ flowerpower job-queue show-schedule-ids --name my-scheduler\n\n# Show schedule IDs with debug logging\n$ flowerpower job-queue show-schedule-ids --log-level debug"
  },
  {
    "objectID": "api/cli_job_queue.html#pause_schedule",
    "href": "api/cli_job_queue.html#pause_schedule",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Pause a schedule or multiple schedules.\nThis command temporarily stops a scheduled job from running while maintaining its configuration. Paused schedules can be resumed later. Note that this functionality is only available for APScheduler workers.\n\n\nflowerpower job-queue pause_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to pause (ignored if –all is used)\nRequired\n\n\nall\nstr\nPause all schedules instead of a specific one\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue pause-schedule schedule-123456\n\n# Pause all schedules\n$ flowerpower job-queue pause-schedule --all dummy-id\n\n# Note: Schedule pausing is not supported for RQ workers"
  },
  {
    "objectID": "api/cli_job_queue.html#resume_schedule",
    "href": "api/cli_job_queue.html#resume_schedule",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Resume a paused schedule or multiple schedules.\nThis command restarts previously paused schedules, allowing them to run again according to their original configuration. Note that this functionality is only available for APScheduler workers.\n\n\nflowerpower job-queue resume_schedule [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschedule_id\nstr\nID of the schedule to resume (ignored if –all is used)\nRequired\n\n\nall\nstr\nResume all schedules instead of a specific one\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue resume-schedule schedule-123456\n\n# Resume all schedules\n$ flowerpower job-queue resume-schedule --all dummy-id\n\n# Note: Schedule resuming is not supported for RQ workers\n\n# Set a specific logging level\n$ flowerpower job-queue resume-schedule schedule-123456 --log-level debug"
  },
  {
    "objectID": "api/cli_job_queue.html#show_jobs",
    "href": "api/cli_job_queue.html#show_jobs",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Display detailed information about all jobs in the queue.\nThis command shows comprehensive information about jobs including their status, creation time, execution time, and other details in a user-friendly format.\n\n\nflowerpower job-queue show_jobs [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nqueue_name\nstr\nName of the queue to show jobs from (RQ only)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the job information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-jobs\n\n# Show jobs for a specific queue type\n$ flowerpower job-queue show-jobs --type rq\n\n# Show jobs in a specific RQ queue\n$ flowerpower job-queue show-jobs --queue-name high-priority\n\n# Display jobs in JSON format\n$ flowerpower job-queue show-jobs --format json"
  },
  {
    "objectID": "api/cli_job_queue.html#show_schedules",
    "href": "api/cli_job_queue.html#show_schedules",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Display detailed information about all schedules.\nThis command shows comprehensive information about scheduled jobs including their timing configuration, status, and other details in a user-friendly format.\n\n\nflowerpower job-queue show_schedules [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the schedule information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue show-schedules\n\n# Show schedules for RQ\n$ flowerpower job-queue show-schedules --type rq\n\n# Display schedules in JSON format\n$ flowerpower job-queue show-schedules --format json"
  },
  {
    "objectID": "api/cli_job_queue.html#enqueue_pipeline",
    "href": "api/cli_job_queue.html#enqueue_pipeline",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Enqueue a pipeline for execution via the job queue.\nThis command queues a pipeline for asynchronous execution using the configured job queue backend (RQ). The job can be executed immediately, after a delay, or at a specific time.\n\n\nflowerpower job-queue enqueue_pipeline [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to enqueue\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nrun_in\nstr\nDelay before execution (duration format like ‘5m’, ‘1h’, ‘30s’)\nRequired\n\n\nrun_at\nstr\nSpecific datetime for execution (ISO format)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue enqueue-pipeline my_pipeline\n\n# Enqueue with custom inputs\n$ flowerpower job-queue enqueue-pipeline my_pipeline --inputs '{\"data_path\": \"data/file.csv\"}'\n\n# Enqueue with delay\n$ flowerpower job-queue enqueue-pipeline my_pipeline --run-in \"30m\"\n\n# Enqueue for specific time\n$ flowerpower job-queue enqueue-pipeline my_pipeline --run-at \"2025-01-01T09:00:00\""
  },
  {
    "objectID": "api/cli_job_queue.html#schedule_pipeline",
    "href": "api/cli_job_queue.html#schedule_pipeline",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Schedule a pipeline for recurring or future execution.\nThis command sets up recurring or future execution of a pipeline using cron expressions or interval-based scheduling via the configured job queue backend.\n\n\nflowerpower job-queue schedule_pipeline [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to schedule\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ncron\nstr\nCron expression for scheduling (e.g., ’0 9 * * *’ for 9 AM daily)\nRequired\n\n\ninterval\nstr\nInterval for recurring execution (duration format)\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nschedule_id\nstr\nCustom identifier for the schedule\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue schedule-pipeline my_pipeline --cron \"0 9 * * *\"\n\n# Schedule every 30 minutes\n$ flowerpower job-queue schedule-pipeline my_pipeline --interval \"30m\"\n\n# Schedule with custom inputs and ID\n$ flowerpower job-queue schedule-pipeline my_pipeline --cron \"0 0 * * *\" \\\\\n--inputs '{\"env\": \"prod\"}' --schedule-id \"nightly-prod\""
  },
  {
    "objectID": "api/cli_job_queue.html#run_job",
    "href": "api/cli_job_queue.html#run_job",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "Execute a specific job by its ID.\nThis command runs a job that has been previously enqueued in the job queue. The job will be executed immediately regardless of its original schedule.\n\n\nflowerpower job-queue run_job [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njob_id\nstr\nID of the job to run\nRequired\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue run-job job-123456\n\n# Run a job with a specific backend type\n$ flowerpower job-queue run-job job-123456 --type rq\n\n# Run a job with debug logging\n$ flowerpower job-queue run-job job-123456 --log-level debug"
  },
  {
    "objectID": "api/cli_job_queue.html#list_schedules",
    "href": "api/cli_job_queue.html#list_schedules",
    "title": "flowerpower job-queue Commands",
    "section": "",
    "text": "List all schedules with detailed status information.\nThis command provides enhanced schedule listing showing trigger configuration, status, next run time, and execution history. This is an enhanced version of show-schedules with more detailed information.\n\n\nflowerpower job-queue list_schedules [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nType of job queue backend (rq)\nRequired\n\n\nname\nstr\nName of the scheduler configuration to use\nRequired\n\n\nbase_dir\nstr\nBase directory for the scheduler configuration\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON or key=value pairs\nRequired\n\n\nlog_level\nstr\nLogging level (debug, info, warning, error, critical)\nRequired\n\n\nformat\nstr\nOutput format for the schedule information\nRequired\n\n\nshow_status\nstr\nInclude schedule status information\nRequired\n\n\nshow_next_run\nstr\nInclude next execution time information\nRequired\n\n\n\n\n\n\n$ flowerpower job-queue list-schedules\n\n# List schedules in JSON format\n$ flowerpower job-queue list-schedules --format json\n\n# List schedules without status information\n$ flowerpower job-queue list-schedules --no-show-status\n\n# List schedules for a specific backend\n$ flowerpower job-queue list-schedules --type rq"
  },
  {
    "objectID": "api/init.html",
    "href": "api/init.html",
    "title": "init",
    "section": "",
    "text": "init\nModule: flowerpower.init\nThe init function is a top-level function that initializes a new FlowerPower project. It is a convenient alias for FlowerPowerProject.init().\ninit(name: str | None = None, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = None, fs: AbstractFileSystem | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, hooks_dir: str = settings.HOOKS_DIR)\nInitializes a new FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr \\| None\nThe name of the project. Defaults to the current directory name.\nNone\n\n\nbase_dir\nstr \\| None\nThe base directory for the project. Defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\nNone\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\nNone\n\n\njob_queue_type\nstr\nThe type of job queue to use (e.g., “rq”).\nsettings.JOB_QUEUE_TYPE\n\n\nhooks_dir\nstr\nThe directory for project hooks.\nsettings.HOOKS_DIR\n\n\n\nReturns: FlowerPowerProject - A FlowerPowerProject instance.\nRaises: FileExistsError: If the project already exists.\n\nExample\nfrom flowerpower import init\n\n# Initialize a new project\nproject = init(name=\"my-new-project\", job_queue_type=\"rq\")"
  },
  {
    "objectID": "api/configuration.html",
    "href": "api/configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "FlowerPower uses a hierarchical configuration system to manage project and pipeline settings. The main configuration classes are:\n\nConfig\nProjectConfig\nPipelineConfig\n\nThese classes are designed to be flexible and extensible, allowing you to manage your project’s configuration in a clean and organized way.\n\n\n\n\nModule: flowerpower.cfg.Config\nThe Config class is the main configuration class that combines project and pipeline settings. It serves as the central configuration manager.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\npipeline\nPipelineConfig\nA PipelineConfig object containing pipeline-specific settings.\n\n\nproject\nProjectConfig\nA ProjectConfig object containing project-level settings.\n\n\n\n\n\nfrom flowerpower.cfg import Config\n\n# Load default configuration\nconfig = Config()\n\n# Access project and pipeline settings\nprint(config.project.name)\nprint(config.pipeline.name)\n\n\n\n\nModule: flowerpower.cfg.ProjectConfig\nThe ProjectConfig class manages project-level settings, including job queue and adapter configurations.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the project.\n\n\njob_queue\nJobQueueConfig\nA JobQueueConfig object for the job queue settings.\n\n\nadapter\nAdapterConfig\nAn AdapterConfig object for the project-level adapter settings.\n\n\n\n\n\nfrom flowerpower.cfg import ProjectConfig\n\n# Load project configuration\nproject_config = ProjectConfig()\n\n# Access project settings\nprint(project_config.name)\nprint(project_config.job_queue.type)\n\n\n\n\nModule: flowerpower.cfg.PipelineConfig\nThe PipelineConfig class manages pipeline-specific settings, including run settings, scheduling, parameters, and adapter configurations.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\nrun\nRunConfig\nA RunConfig object for pipeline execution settings.\n\n\nschedule\nScheduleConfig\nA ScheduleConfig object for pipeline scheduling.\n\n\nparams\ndict\nA dictionary of pipeline parameters.\n\n\nadapter\nAdapterConfig\nAn AdapterConfig object for pipeline-specific adapter settings.\n\n\n\n\n\nfrom flowerpower.cfg import PipelineConfig\n\n# Load pipeline configuration\npipeline_config = PipelineConfig()\n\n# Access pipeline settings\nprint(pipeline_config.name)\nprint(pipeline_config.run.executor)\n\n\n\n\nModule: flowerpower.cfg.ExecutorConfig\nDefines the configuration for the pipeline executor (e.g., “local”, “threadpool”).\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ntype\nstr\nThe type of executor (e.g., “local”, “threadpool”).\n\n\nconfig\ndict\nA dictionary of executor-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import ExecutorConfig\n\n# Create an ExecutorConfig\nexecutor_config = ExecutorConfig(type=\"threadpool\", config={\"max_workers\": 4})\nprint(executor_config.type)\n\n\n\n\nModule: flowerpower.cfg.WithAdapterConfig\nDefines settings for using adapters during pipeline execution.\nAttributes:\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nadapter_name\nstr\nThe name of the adapter.\n\n\nenabled\nbool\nWhether the adapter is enabled.\n\n\nconfig\ndict\nAdapter-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import WithAdapterConfig\n\n# Create a WithAdapterConfig\nadapter_config = WithAdapterConfig(adapter_name=\"opentelemetry\", enabled=True)\nprint(adapter_config.enabled)\n\n\n\n\nModule: flowerpower.cfg.AdapterConfig\nA base class for adapter configurations, used for both project and pipeline-level settings.\nAttributes:\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ntype\nstr\nThe type of adapter.\n\n\nconfig\ndict\nA dictionary of adapter-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import AdapterConfig\n\n# Create an AdapterConfig\nadapter_config = AdapterConfig(type=\"tracker\", config={\"project_id\": \"abc\"})\nprint(adapter_config.type)"
  },
  {
    "objectID": "api/configuration.html#classes",
    "href": "api/configuration.html#classes",
    "title": "Configuration",
    "section": "",
    "text": "Module: flowerpower.cfg.Config\nThe Config class is the main configuration class that combines project and pipeline settings. It serves as the central configuration manager.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\npipeline\nPipelineConfig\nA PipelineConfig object containing pipeline-specific settings.\n\n\nproject\nProjectConfig\nA ProjectConfig object containing project-level settings.\n\n\n\n\n\nfrom flowerpower.cfg import Config\n\n# Load default configuration\nconfig = Config()\n\n# Access project and pipeline settings\nprint(config.project.name)\nprint(config.pipeline.name)\n\n\n\n\nModule: flowerpower.cfg.ProjectConfig\nThe ProjectConfig class manages project-level settings, including job queue and adapter configurations.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the project.\n\n\njob_queue\nJobQueueConfig\nA JobQueueConfig object for the job queue settings.\n\n\nadapter\nAdapterConfig\nAn AdapterConfig object for the project-level adapter settings.\n\n\n\n\n\nfrom flowerpower.cfg import ProjectConfig\n\n# Load project configuration\nproject_config = ProjectConfig()\n\n# Access project settings\nprint(project_config.name)\nprint(project_config.job_queue.type)\n\n\n\n\nModule: flowerpower.cfg.PipelineConfig\nThe PipelineConfig class manages pipeline-specific settings, including run settings, scheduling, parameters, and adapter configurations.\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the pipeline.\n\n\nrun\nRunConfig\nA RunConfig object for pipeline execution settings.\n\n\nschedule\nScheduleConfig\nA ScheduleConfig object for pipeline scheduling.\n\n\nparams\ndict\nA dictionary of pipeline parameters.\n\n\nadapter\nAdapterConfig\nAn AdapterConfig object for pipeline-specific adapter settings.\n\n\n\n\n\nfrom flowerpower.cfg import PipelineConfig\n\n# Load pipeline configuration\npipeline_config = PipelineConfig()\n\n# Access pipeline settings\nprint(pipeline_config.name)\nprint(pipeline_config.run.executor)\n\n\n\n\nModule: flowerpower.cfg.ExecutorConfig\nDefines the configuration for the pipeline executor (e.g., “local”, “threadpool”).\nAttributes:\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ntype\nstr\nThe type of executor (e.g., “local”, “threadpool”).\n\n\nconfig\ndict\nA dictionary of executor-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import ExecutorConfig\n\n# Create an ExecutorConfig\nexecutor_config = ExecutorConfig(type=\"threadpool\", config={\"max_workers\": 4})\nprint(executor_config.type)\n\n\n\n\nModule: flowerpower.cfg.WithAdapterConfig\nDefines settings for using adapters during pipeline execution.\nAttributes:\n\n\n\nAttribute\nType\nDescription\n\n\n\n\nadapter_name\nstr\nThe name of the adapter.\n\n\nenabled\nbool\nWhether the adapter is enabled.\n\n\nconfig\ndict\nAdapter-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import WithAdapterConfig\n\n# Create a WithAdapterConfig\nadapter_config = WithAdapterConfig(adapter_name=\"opentelemetry\", enabled=True)\nprint(adapter_config.enabled)\n\n\n\n\nModule: flowerpower.cfg.AdapterConfig\nA base class for adapter configurations, used for both project and pipeline-level settings.\nAttributes:\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ntype\nstr\nThe type of adapter.\n\n\nconfig\ndict\nA dictionary of adapter-specific configurations.\n\n\n\n\n\nfrom flowerpower.cfg import AdapterConfig\n\n# Create an AdapterConfig\nadapter_config = AdapterConfig(type=\"tracker\", config={\"project_id\": \"abc\"})\nprint(adapter_config.type)"
  },
  {
    "objectID": "api/rqmanager.html",
    "href": "api/rqmanager.html",
    "title": "RQManager",
    "section": "",
    "text": "Module: flowerpower.job_queue.rq.RQManager\nThe RQManager is the implementation of JobQueueManager for Redis Queue (RQ). It handles the specifics of interacting with an RQ backend.\n\n\n\n\n__init__(self, name: str, base_dir: str | None = None, backend: RQBackend | None = None, storage_options: dict | None = None, fs: AbstractFileSystem | None = None, log_level: str | None = None)\nInitializes the RQManager.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the scheduler instance.\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project.\nNone\n\n\nbackend\nRQBackend \\| None\nAn RQBackend instance for Redis connection configuration.\nNone\n\n\nstorage_options\ndict \\| None\nStorage options for the filesystem.\nNone\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\nNone\n\n\nlog_level\nstr \\| None\nThe logging level.\nNone\n\n\n\n\n\n\n\n\n\nadd_job(self, func: Callable, func_args: list | None = None, func_kwargs: dict | None = None, job_id: str | None = None, result_ttl: int | None = None, ttl: int | None = None, timeout: int | None = None, queue_name: str | None = None, run_at: datetime | None = None, run_in: timedelta | int | str | None = None, retry: Retry | None = None, repeat: int | None = None, meta: dict | None = None, failure_ttl: int | None = None, group_id: str | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_stopped: Callable | tuple[Callable, tuple | None, dict | None] | None = None, **job_kwargs)\nAdds a job to the queue for immediate or scheduled execution.\n\n\n\n\n\n\nNote\n\n\n\nThis method is deprecated. Use enqueue, enqueue_in, or enqueue_at instead.\n\n\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nThe function to execute.\n\n\n\nfunc_args\nlist | None\nPositional arguments for the function.\nNone\n\n\nfunc_kwargs\ndict | None\nKeyword arguments for the function.\nNone\n\n\njob_id\nstr | None\nUnique identifier for the job.\nNone\n\n\nresult_ttl\nint | None\nTime to live for job result (seconds).\nNone\n\n\nttl\nint | None\nTotal time to live for the job (seconds).\nNone\n\n\ntimeout\nint | None\nJob execution timeout (seconds).\nNone\n\n\nqueue_name\nstr | None\nThe name of the RQ queue to use.\nNone\n\n\nrun_at\ndatetime | None\nSpecific datetime to run the job.\nNone\n\n\nrun_in\ntimedelta | int | str | None\nDelay before running the job.\nNone\n\n\nretry\nRetry | None\nRetry policy for the job.\nNone\n\n\nrepeat\nint | None\nNumber of times to repeat the job.\nNone\n\n\nmeta\ndict | None\nArbitrary metadata for the job.\nNone\n\n\nfailure_ttl\nint | None\nTime to live for failed job result (seconds).\nNone\n\n\ngroup_id\nstr | None\nGroup ID for the job.\nNone\n\n\non_success\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job success.\nNone\n\n\non_failure\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job failure.\nNone\n\n\non_stopped\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job stopped.\nNone\n\n\n**job_kwargs\nAny\nAdditional keyword arguments for RQ’s Job class.\n\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If required parameters are missing or invalid.\n\n\nfrom flowerpower.job_queue.rq import RQManager\nfrom datetime import datetime, timedelta\n\nmanager = RQManager(name=\"my_rq_manager\")\n\n# Enqueue a simple job\ndef my_task(x, y):\n    return x + y\n\njob = manager.add_job(my_task, func_args=[1, 2], queue_name=\"default\")\nprint(f\"Enqueued job {job.id}\")\n\n# Schedule a job to run in 5 minutes\njob = manager.add_job(my_task, func_args=[3, 4], run_in=timedelta(minutes=5), queue_name=\"default\")\n\n# Schedule a job to run at a specific time\ntarget_time = datetime(2025, 1, 1, 10, 0, 0)\njob = manager.add_job(my_task, func_args=[5, 6], run_at=target_time, queue_name=\"default\")\n\n\n\n\nstart_worker(self, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = False, **kwargs)\nStarts a worker process for the job queue.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbackground\nbool\nIf True, runs the worker in the background.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nA list of RQ queues to listen to. Defaults to all queues.\nNone\n\n\nwith_scheduler\nbool\nIf True, the worker also processes scheduled jobs.\nFalse\n\n\n**kwargs\nAny\nAdditional arguments for RQ’s Worker class.\n\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker fails to start.\n\n\nfrom flowerpower.job_queue.rq import RQManager\n\nmanager = RQManager(name=\"my_rq_manager\")\n\n# Start a worker in the foreground, listening to the 'default' queue\nmanager.start_worker(queue_names=[\"default\"])\n\n# Start a worker in the background with scheduler enabled\nmanager.start_worker(background=True, with_scheduler=True)"
  },
  {
    "objectID": "api/rqmanager.html#initialization",
    "href": "api/rqmanager.html#initialization",
    "title": "RQManager",
    "section": "",
    "text": "__init__(self, name: str, base_dir: str | None = None, backend: RQBackend | None = None, storage_options: dict | None = None, fs: AbstractFileSystem | None = None, log_level: str | None = None)\nInitializes the RQManager.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the scheduler instance.\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project.\nNone\n\n\nbackend\nRQBackend \\| None\nAn RQBackend instance for Redis connection configuration.\nNone\n\n\nstorage_options\ndict \\| None\nStorage options for the filesystem.\nNone\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance.\nNone\n\n\nlog_level\nstr \\| None\nThe logging level.\nNone"
  },
  {
    "objectID": "api/rqmanager.html#methods",
    "href": "api/rqmanager.html#methods",
    "title": "RQManager",
    "section": "",
    "text": "add_job(self, func: Callable, func_args: list | None = None, func_kwargs: dict | None = None, job_id: str | None = None, result_ttl: int | None = None, ttl: int | None = None, timeout: int | None = None, queue_name: str | None = None, run_at: datetime | None = None, run_in: timedelta | int | str | None = None, retry: Retry | None = None, repeat: int | None = None, meta: dict | None = None, failure_ttl: int | None = None, group_id: str | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_stopped: Callable | tuple[Callable, tuple | None, dict | None] | None = None, **job_kwargs)\nAdds a job to the queue for immediate or scheduled execution.\n\n\n\n\n\n\nNote\n\n\n\nThis method is deprecated. Use enqueue, enqueue_in, or enqueue_at instead.\n\n\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nThe function to execute.\n\n\n\nfunc_args\nlist | None\nPositional arguments for the function.\nNone\n\n\nfunc_kwargs\ndict | None\nKeyword arguments for the function.\nNone\n\n\njob_id\nstr | None\nUnique identifier for the job.\nNone\n\n\nresult_ttl\nint | None\nTime to live for job result (seconds).\nNone\n\n\nttl\nint | None\nTotal time to live for the job (seconds).\nNone\n\n\ntimeout\nint | None\nJob execution timeout (seconds).\nNone\n\n\nqueue_name\nstr | None\nThe name of the RQ queue to use.\nNone\n\n\nrun_at\ndatetime | None\nSpecific datetime to run the job.\nNone\n\n\nrun_in\ntimedelta | int | str | None\nDelay before running the job.\nNone\n\n\nretry\nRetry | None\nRetry policy for the job.\nNone\n\n\nrepeat\nint | None\nNumber of times to repeat the job.\nNone\n\n\nmeta\ndict | None\nArbitrary metadata for the job.\nNone\n\n\nfailure_ttl\nint | None\nTime to live for failed job result (seconds).\nNone\n\n\ngroup_id\nstr | None\nGroup ID for the job.\nNone\n\n\non_success\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job success.\nNone\n\n\non_failure\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job failure.\nNone\n\n\non_stopped\nCallable | tuple[Callable, tuple | None, dict | None] | None\nCallback on job stopped.\nNone\n\n\n**job_kwargs\nAny\nAdditional keyword arguments for RQ’s Job class.\n\n\n\n\nReturns: Job - The enqueued job object.\nRaises: ValueError: If required parameters are missing or invalid.\n\n\nfrom flowerpower.job_queue.rq import RQManager\nfrom datetime import datetime, timedelta\n\nmanager = RQManager(name=\"my_rq_manager\")\n\n# Enqueue a simple job\ndef my_task(x, y):\n    return x + y\n\njob = manager.add_job(my_task, func_args=[1, 2], queue_name=\"default\")\nprint(f\"Enqueued job {job.id}\")\n\n# Schedule a job to run in 5 minutes\njob = manager.add_job(my_task, func_args=[3, 4], run_in=timedelta(minutes=5), queue_name=\"default\")\n\n# Schedule a job to run at a specific time\ntarget_time = datetime(2025, 1, 1, 10, 0, 0)\njob = manager.add_job(my_task, func_args=[5, 6], run_at=target_time, queue_name=\"default\")\n\n\n\n\nstart_worker(self, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = False, **kwargs)\nStarts a worker process for the job queue.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbackground\nbool\nIf True, runs the worker in the background.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nA list of RQ queues to listen to. Defaults to all queues.\nNone\n\n\nwith_scheduler\nbool\nIf True, the worker also processes scheduled jobs.\nFalse\n\n\n**kwargs\nAny\nAdditional arguments for RQ’s Worker class.\n\n\n\n\nReturns: None\nRaises: RuntimeError: If the worker fails to start.\n\n\nfrom flowerpower.job_queue.rq import RQManager\n\nmanager = RQManager(name=\"my_rq_manager\")\n\n# Start a worker in the foreground, listening to the 'default' queue\nmanager.start_worker(queue_names=[\"default\"])\n\n# Start a worker in the background with scheduler enabled\nmanager.start_worker(background=True, with_scheduler=True)"
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Usage",
    "section": "",
    "text": "Welcome to the advanced usage guide for FlowerPower. This document covers more complex configurations and use cases to help you get the most out of the library."
  },
  {
    "objectID": "advanced.html#configuration-flexibility",
    "href": "advanced.html#configuration-flexibility",
    "title": "Advanced Usage",
    "section": "Configuration Flexibility",
    "text": "Configuration Flexibility\nFlowerPower offers multiple ways to configure your project, ensuring flexibility for different environments and workflows. The configuration is loaded in the following order of precedence:\n\nProgrammatic Overrides: Highest priority.\nEnvironment Variables: Set in your shell or .env file.\nsettings.py: A dedicated settings module.\nYAML files: anypath.yaml for your project.\n\n\nProgrammatic Configuration\nYou can override configuration settings directly in your Python code. This is useful for dynamic adjustments or for settings that are determined at runtime.\nfrom flowerpower.core.config import settings\n\n# Override the default Redis host\nsettings.set('redis.host', 'localhost')\n\n# You can also update nested settings\nsettings.set('pipelines.my_pipeline.retries', 3)"
  },
  {
    "objectID": "advanced.html#direct-module-usage",
    "href": "advanced.html#direct-module-usage",
    "title": "Advanced Usage",
    "section": "Direct Module Usage",
    "text": "Direct Module Usage\nFor fine-grained control, you can work directly with PipelineManager and JobQueueManager.\n\nPipelineManager\nThe PipelineManager is responsible for loading, validating, and executing data pipelines.\nfrom flowerpower.core.pipeline import PipelineManager\n\n# Initialize the manager\npipeline_manager = PipelineManager()\n\n# Load a specific pipeline\npipeline = pipeline_manager.get_pipeline(\"sales_etl\")\n\n# Execute the pipeline\nresult = pipeline.run(input_data=\"path/to/data.csv\")\nprint(result)\n\n\nJobQueueManager\nThe JobQueueManager handles job queuing, scheduling, and worker management.\nfrom flowerpower.core.job_queue import JobQueueManager\n\n# Initialize the manager\njob_queue_manager = JobQueueManager()\n\n# Enqueue a job\njob = job_queue_manager.enqueue(\"my_task\", arg1=\"value1\", arg2=\"value2\")\nprint(f\"Job {job.id} enqueued.\")\n\n# Schedule a job to run at a specific time\njob_queue_manager.schedule(\"my_task\", cron=\"0 0 * * *\") # Daily at midnight"
  },
  {
    "objectID": "advanced.html#adapters",
    "href": "advanced.html#adapters",
    "title": "Advanced Usage",
    "section": "Adapters",
    "text": "Adapters\nIntegrate with popular MLOps and observability tools using adapters.\n\nHamilton Tracker: For dataflow and lineage tracking.\nMLflow: For experiment tracking.\nOpenTelemetry: For distributed tracing and metrics."
  },
  {
    "objectID": "advanced.html#filesystem-abstraction",
    "href": "advanced.html#filesystem-abstraction",
    "title": "Advanced Usage",
    "section": "Filesystem Abstraction",
    "text": "Filesystem Abstraction\nFlowerPower uses the library fsspec-utils to provide a unified interface for interacting with different filesystems, including local storage, S3, and GCS. This allows you to switch between storage backends without changing your code."
  },
  {
    "objectID": "advanced.html#worker-management",
    "href": "advanced.html#worker-management",
    "title": "Advanced Usage",
    "section": "Worker Management",
    "text": "Worker Management\nYou can manage workers to process your queued jobs.\n\nSingle Worker\nStart a single worker in the foreground:\nflowerpower job-queue start-worker\n\n\nWorker Pool\nStart a pool of workers in the background:\nflowerpower job-queue start-worker --pool-size 5 --background\nTo stop background workers:\nflowerpower job-queue start-worker stop"
  },
  {
    "objectID": "advanced.html#scheduling-options",
    "href": "advanced.html#scheduling-options",
    "title": "Advanced Usage",
    "section": "Scheduling Options",
    "text": "Scheduling Options\nFlowerPower supports several scheduling strategies for your jobs:\n\nCron: For recurring jobs at specific times (e.g., 0 2 * * *).\nInterval: For jobs that run at regular intervals (e.g., every 30 minutes).\nDate: For jobs that run once at a specific date and time."
  },
  {
    "objectID": "advanced.html#extensible-io-plugins",
    "href": "advanced.html#extensible-io-plugins",
    "title": "Advanced Usage",
    "section": "Extensible I/O Plugins",
    "text": "Extensible I/O Plugins\nThe FlowerPower plugin flowerpower-io enhances FlowerPower’s I/O capabilities, allowing you to connect to various data sources and sinks using a simple plugin architecture.\nSupported Types Include:\n\nCSV, JSON, Parquet\nDeltaTable\nDuckDB, PostgreSQL, MySQL, MSSQL, Oracle, SQLite\nMQTT\n\nTo use a plugin, simply specify its type in your pipeline configuration."
  },
  {
    "objectID": "advanced.html#troubleshooting",
    "href": "advanced.html#troubleshooting",
    "title": "Advanced Usage",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nHere are some common issues and how to resolve them:\n\nRedis Connection Error: Ensure your Redis server is running and accessible. Check the redis.host and redis.port settings in your configuration.\nConfiguration Errors: Use the flowerpower config show command to inspect the loaded configuration and identify any misconfigurations.\nModule Not Found: Make sure your pipeline and task modules are in Python’s path. You can add directories to the path using the PYTHONPATH environment variable.\n\n\n\n\n\n\n\nNote\n\n\n\nFor more detailed information, refer to the API documentation."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing to FlowerPower",
    "section": "",
    "text": "First off, thank you for considering contributing to FlowerPower! It’s people like you that make open source such a great community.\nWe welcome contributions in various forms, from reporting bugs and suggesting enhancements to submitting pull requests with new features or bug fixes.\n\n\nIf you encounter a bug or have a suggestion for a new feature, please open an issue on our GitHub Issue Tracker.\nWhen reporting a bug, please include the following to help us resolve it quickly: - A clear and descriptive title. - A detailed description of the problem, including steps to reproduce it. - Your operating system, Python version, and FlowerPower version. - Any relevant logs or tracebacks.\n\n\n\nWe love pull requests! To ensure a smooth process, please follow these guidelines:\n\nFork the repository and create a new branch for your feature or bug fix.\nSet up your development environment (see “Development Setup” below).\nMake your changes and ensure the code is well-tested.\nUpdate the documentation if your changes affect it.\nEnsure your code passes all tests before submitting.\nSubmit a pull request with a clear description of your changes.\n\n\n\n\nWe use uv for managing dependencies and running our development environment.\n\nInstall uv: Follow the official instructions to install uv.\nCreate a virtual environment: bash     uv venv\nActivate the environment: bash     source .venv/bin/activate\nInstall dependencies: To install the base dependencies along with the development and test dependencies, run: bash     uv pip install -e \".[dev,test]\"\n\n\n\n\n\n\nNote\n\n\n\nIf you need to install optional dependencies for specific features (e.g., mqtt, redis), you can add them to the install command: uv pip install -e \".[dev,test,mqtt,redis]\".\n\n\nRun tests: To ensure everything is working correctly, run the test suite: bash     uv run pytest\n\n\n\n\nWe are committed to providing a welcoming and inclusive environment for everyone. Please read and follow our Code of Conduct (assuming one exists or will be created).\nThank you for your contribution!"
  },
  {
    "objectID": "contributing.html#reporting-issues",
    "href": "contributing.html#reporting-issues",
    "title": "Contributing to FlowerPower",
    "section": "",
    "text": "If you encounter a bug or have a suggestion for a new feature, please open an issue on our GitHub Issue Tracker.\nWhen reporting a bug, please include the following to help us resolve it quickly: - A clear and descriptive title. - A detailed description of the problem, including steps to reproduce it. - Your operating system, Python version, and FlowerPower version. - Any relevant logs or tracebacks."
  },
  {
    "objectID": "contributing.html#submitting-pull-requests",
    "href": "contributing.html#submitting-pull-requests",
    "title": "Contributing to FlowerPower",
    "section": "",
    "text": "We love pull requests! To ensure a smooth process, please follow these guidelines:\n\nFork the repository and create a new branch for your feature or bug fix.\nSet up your development environment (see “Development Setup” below).\nMake your changes and ensure the code is well-tested.\nUpdate the documentation if your changes affect it.\nEnsure your code passes all tests before submitting.\nSubmit a pull request with a clear description of your changes."
  },
  {
    "objectID": "contributing.html#development-setup",
    "href": "contributing.html#development-setup",
    "title": "Contributing to FlowerPower",
    "section": "",
    "text": "We use uv for managing dependencies and running our development environment.\n\nInstall uv: Follow the official instructions to install uv.\nCreate a virtual environment: bash     uv venv\nActivate the environment: bash     source .venv/bin/activate\nInstall dependencies: To install the base dependencies along with the development and test dependencies, run: bash     uv pip install -e \".[dev,test]\"\n\n\n\n\n\n\nNote\n\n\n\nIf you need to install optional dependencies for specific features (e.g., mqtt, redis), you can add them to the install command: uv pip install -e \".[dev,test,mqtt,redis]\".\n\n\nRun tests: To ensure everything is working correctly, run the test suite: bash     uv run pytest"
  },
  {
    "objectID": "contributing.html#code-of-conduct",
    "href": "contributing.html#code-of-conduct",
    "title": "Contributing to FlowerPower",
    "section": "",
    "text": "We are committed to providing a welcoming and inclusive environment for everyone. Please read and follow our Code of Conduct (assuming one exists or will be created).\nThank you for your contribution!"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "This section provides an overview of various example projects built with FlowerPower. These examples demonstrate different use cases and functionalities of the framework, from simple “Hello World” setups to complex machine learning pipelines and data processing workflows. You can find the source code for these examples in the FlowerPower GitHub repository.\n\n\n\n\nThis example demonstrates a typical Extract, Transform, Load (ETL) pipeline, showcasing how FlowerPower can be used to process and move data between different systems.\n\n\n\nA basic “Hello World” example illustrating the fundamental structure of a FlowerPower project and how to define and run a simple pipeline or job.\n\n\n\nThis example focuses specifically on the job queuing capabilities of FlowerPower, demonstrating how to use the built-in job queue to manage and execute background tasks independently of pipelines.\n\n\n\nShowcases how to build a machine learning training pipeline with FlowerPower, including data preparation, model training, and evaluation steps.\n\n\n\nThis example highlights the core pipeline functionality of FlowerPower, demonstrating how to define and execute a sequence of interconnected tasks without relying on a job queue.\n\n\n\nDemonstrates how to use FlowerPower to automate the generation and distribution of scheduled reports, leveraging its scheduling and task management features.\n\n\n\nAn example illustrating a web scraping workflow implemented as a FlowerPower pipeline, showing how to extract data from websites and process it."
  },
  {
    "objectID": "examples.html#available-examples",
    "href": "examples.html#available-examples",
    "title": "Examples",
    "section": "",
    "text": "This example demonstrates a typical Extract, Transform, Load (ETL) pipeline, showcasing how FlowerPower can be used to process and move data between different systems.\n\n\n\nA basic “Hello World” example illustrating the fundamental structure of a FlowerPower project and how to define and run a simple pipeline or job.\n\n\n\nThis example focuses specifically on the job queuing capabilities of FlowerPower, demonstrating how to use the built-in job queue to manage and execute background tasks independently of pipelines.\n\n\n\nShowcases how to build a machine learning training pipeline with FlowerPower, including data preparation, model training, and evaluation steps.\n\n\n\nThis example highlights the core pipeline functionality of FlowerPower, demonstrating how to define and execute a sequence of interconnected tasks without relying on a job queue.\n\n\n\nDemonstrates how to use FlowerPower to automate the generation and distribution of scheduled reports, leveraging its scheduling and task management features.\n\n\n\nAn example illustrating a web scraping workflow implemented as a FlowerPower pipeline, showing how to extract data from websites and process it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FlowerPower: Data Pipeline Orchestration",
    "section": "",
    "text": "Welcome to the official documentation for FlowerPower, a powerful Python library designed to help you build, configure, schedule, and execute data processing pipelines with ease.\nFlowerPower streamlines complex data workflows by integrating the modularity of Hamilton for pipeline logic and the robustness of Redis Queue (RQ) for asynchronous job processing."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "FlowerPower: Data Pipeline Orchestration",
    "section": "Get Started",
    "text": "Get Started\nReady to dive in? Our Quickstart Guide will walk you through installing FlowerPower and running your first pipeline in just a few minutes."
  },
  {
    "objectID": "index.html#core-concepts",
    "href": "index.html#core-concepts",
    "title": "FlowerPower: Data Pipeline Orchestration",
    "section": "Core Concepts",
    "text": "Core Concepts\nFlowerPower is built around a few key concepts that make it both powerful and flexible:\n\nModular Pipeline Design: Define your data transformations as a collection of simple Python functions. FlowerPower, using Hamilton, automatically understands their dependencies and assembles them into a Directed Acyclic Graph (DAG).\nConfiguration-Driven: Separate your pipeline logic from its execution parameters. Environments, data sources, and pipeline settings are all managed through clear and simple YAML files.\nJob Queue Integration: Scale your data processing by offloading tasks to a distributed job queue. FlowerPower provides a seamless interface for sending, managing, and monitoring asynchronous jobs with RQ.\nUnified Project Interface: Interact with your pipelines through the method that suits you best—a Python API (FlowerPowerProject), a command-line interface (CLI), or a web-based UI for visualization and monitoring.\nExtensible I/O: Easily read from and write to various data sources with built-in and custom I/O plugins, ensuring your pipelines can connect to any data, anywhere.\n\n\n\n\n\n\n\nNote\n\n\n\nA Note on Hamilton and RQ\nFlowerPower acts as an orchestrator, not a replacement. You will still write your pipeline logic using Hamilton’s function-based syntax and interact with job queue concepts from RQ. FlowerPower’s role is to connect these two ecosystems, providing a structured project environment and simplifying their combined use."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "Welcome to the architectural overview of FlowerPower. This document provides a high-level look at the library’s design, its core components, and the principles that guide its development. Our goal is to create a powerful, flexible, and easy-to-use platform for building data pipelines and managing asynchronous jobs."
  },
  {
    "objectID": "architecture.html#introduction",
    "href": "architecture.html#introduction",
    "title": "Architecture Overview",
    "section": "",
    "text": "Welcome to the architectural overview of FlowerPower. This document provides a high-level look at the library’s design, its core components, and the principles that guide its development. Our goal is to create a powerful, flexible, and easy-to-use platform for building data pipelines and managing asynchronous jobs."
  },
  {
    "objectID": "architecture.html#core-design-principles",
    "href": "architecture.html#core-design-principles",
    "title": "Architecture Overview",
    "section": "Core Design Principles",
    "text": "Core Design Principles\nFlowerPower is built on a foundation of modularity and clear separation of concerns. Key design principles include:\n\nModular and Configuration-Driven: Components are designed to be self-contained and configurable, allowing you to easily swap implementations and adapt the library to your needs.\nUnified Interface: A single, clean entry point (FlowerPowerProject) simplifies interaction with the library’s powerful features.\nSeparation of Concerns: Pipeline execution (the “what”) is decoupled from job queue management (the “how” and “when”).\nExtensibility: The library is designed to be extended with custom plugins and adapters for I/O, messaging, and more."
  },
  {
    "objectID": "architecture.html#key-components",
    "href": "architecture.html#key-components",
    "title": "Architecture Overview",
    "section": "Key Components",
    "text": "Key Components\nThe library’s architecture is centered around a few key components that work together to provide a seamless experience.\n\n\n\n\n\ngraph TD\n    A[FlowerPowerProject] --&gt;|Manages| B(PipelineManager)\n    A --&gt;|Manages| C(JobQueueManager)\n    B --&gt;|Uses| D[Hamilton]\n    C --&gt;|Uses| E[RQManager]\n    E --&gt;|Uses| F[Redis]\n\n    subgraph \"Core Components\"\n        B\n        C\n        E\n    end\n\n    subgraph \"External Dependencies\"\n        D\n        F\n    end\n\n\n\n\n\n\n\nFlowerPowerProject\nThe FlowerPowerProject class is the main entry point and public-facing API of the library. It acts as a facade, providing a unified interface to the underlying PipelineManager and JobQueueManager. This simplifies the user experience by abstracting away the complexities of the individual components.\n\n\nPipelineManager\nThe PipelineManager is responsible for everything related to data pipelines:\n\nConfiguration: It loads and manages pipeline definitions from YAML files.\nExecution: It uses the Hamilton library to execute dataflows defined as a Directed Acyclic Graph (DAG) of Python functions.\nVisualization: It provides tools for visualizing pipeline graphs.\nI/O: It handles data loading and saving through an extensible system of I/O adapters.\n\n\nHamilton Integration\nFlowerPower leverages Hamilton to define the logic of its data pipelines. Hamilton’s declarative, function-based approach allows you to define complex dataflows in a clear and maintainable way. Each function in a Hamilton module represents a node in the DAG, and Hamilton automatically resolves the dependencies and executes the functions in the correct order.\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about Hamilton, visit the official documentation.\n\n\n\n\n\nJobQueueManager and RQManager\nThe JobQueueManager is a factory responsible for creating and managing job queue backends. Currently, the primary implementation is the RQManager, which uses the powerful Redis Queue (RQ) library.\nThe RQManager handles:\n\nAsynchronous Processing: It allows you to offload long-running tasks to background workers, keeping your application responsive.\nJob Scheduling: You can enqueue jobs to run at a specific time or on a recurring schedule.\nDistributed Workers: RQ’s worker-based architecture enables you to distribute tasks across multiple machines for parallel processing.\n\n\nRQ and Redis\nRQ uses Redis as its message broker and storage backend. This provides a robust and performant foundation for the job queueing system.\n\n\n\n\n\n\nTip\n\n\n\nYou can monitor and manage your RQ queues using tools like rq-dashboard."
  },
  {
    "objectID": "architecture.html#filesystem-abstraction",
    "href": "architecture.html#filesystem-abstraction",
    "title": "Architecture Overview",
    "section": "Filesystem Abstraction",
    "text": "Filesystem Abstraction\nFlowerPower includes a filesystem abstraction layer that allows you to work with local and remote filesystems (e.g., S3, GCS) using a consistent API. This makes it easy to build pipelines that can read from and write to various storage backends without changing your core logic."
  },
  {
    "objectID": "architecture.html#conclusion",
    "href": "architecture.html#conclusion",
    "title": "Architecture Overview",
    "section": "Conclusion",
    "text": "Conclusion\nFlowerPower’s architecture is designed to be both powerful and flexible. By combining the strengths of Hamilton for dataflow definition and RQ for asynchronous processing, it provides a comprehensive solution for a wide range of data-intensive applications. The modular design and unified interface make it easy to get started, while the extensible nature of the library allows it to grow with your needs."
  },
  {
    "objectID": "api/cli_mqtt.html",
    "href": "api/cli_mqtt.html",
    "title": "flowerpower mqtt Commands",
    "section": "",
    "text": "This section details the commands available under flowerpower mqtt.\n\n\nStart an MQTT client to listen to messages on a topic\nThe connection to the MQTT broker is established using the provided configuration o a MQTT event broker defined in the project configuration file conf/project.yml. If not configuration is found, you have to provide the connection parameters, such as host, port, username, and password.\nThe on_message module should contain a function on_message that will be called with the message payload as argument.\n\n\nflowerpower mqtt start_listener [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\non_message\nstr\nName of the module containing the on_message function\nRequired\n\n\ntopic\nstr\nMQTT topic to listen to\nRequired\n\n\nbase_dir\nstr\nBase directory for the module\nRequired\n\n\nhost\nstr\nMQTT broker host\nRequired\n\n\nport\nstr\nMQTT broker port\nRequired\n\n\nusername\nstr\nMQTT broker username\nRequired\n\n\npassword\nstr\nMQTT broker password\nRequired\n\n\n\n\n\n\n$ flowerpower mqtt start_listener --on-message my_module --topic my_topic --base-dir /path/to/module\n\n\n\n\n\nRun a pipeline on a message\nThis command sets up an MQTT listener that executes a pipeline whenever a message is received on the specified topic. The pipeline can be configured to retry on failure using exponential backoff with jitter for better resilience.\n\n\nflowerpower mqtt run_pipeline_on_message [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline\nRequired\n\n\ntopic\nstr\nMQTT topic to listen to\nRequired\n\n\nexecutor\nstr\nName of the executor\nRequired\n\n\nbase_dir\nstr\nBase directory for the pipeline\nRequired\n\n\ninputs\nstr\nInputs as JSON or key=value pairs or dict string\nRequired\n\n\nfinal_vars\nstr\nFinal variables as JSON or list\nRequired\n\n\nconfig\nstr\nConfig for the hamilton pipeline executor\nRequired\n\n\nwith_tracker\nstr\nEnable tracking with hamilton ui\nRequired\n\n\nwith_opentelemetry\nstr\nEnable OpenTelemetry tracing\nRequired\n\n\nwith_progressbar\nstr\nEnable progress bar\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON, dict string or key=value pairs\nRequired\n\n\nas_job\nstr\nRun as a job in the scheduler\nRequired\n\n\nhost\nstr\nMQTT broker host\nRequired\n\n\nport\nstr\nMQTT broker port\nRequired\n\n\nusername\nstr\nMQTT broker username\nRequired\n\n\npassword\nstr\nMQTT broker password\nRequired\n\n\nclean_session\nstr\nWhether to start a clean session with the broker\nRequired\n\n\nqos\nstr\nMQTT Quality of Service level (0, 1, or 2)\nRequired\n\n\nclient_id\nstr\nCustom MQTT client identifier\nRequired\n\n\nclient_id_suffix\nstr\nOptional suffix to append to client_id\nRequired\n\n\nconfig_hook\nstr\nFunction to process incoming messages into pipeline config\nRequired\n\n\nmax_retries\nstr\nMaximum number of retry attempts if pipeline execution fails\nRequired\n\n\nretry_delay\nstr\nBase delay between retries in seconds\nRequired\n\n\njitter_factor\nstr\nRandom factor (0-1) applied to delay for jitter\nRequired\n\n\n\n\n\n\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic sensors/data\n\n# Configure retries for resilience\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic sensors/data --max-retries 5 --retry-delay 2.0\n\n# Run as a job with custom MQTT settings\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic events/process --as-job --qos 2 --host mqtt.example.com\n\n# Use a config hook to process messages\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic data/incoming --config-hook process_message"
  },
  {
    "objectID": "api/cli_mqtt.html#start_listener",
    "href": "api/cli_mqtt.html#start_listener",
    "title": "flowerpower mqtt Commands",
    "section": "",
    "text": "Start an MQTT client to listen to messages on a topic\nThe connection to the MQTT broker is established using the provided configuration o a MQTT event broker defined in the project configuration file conf/project.yml. If not configuration is found, you have to provide the connection parameters, such as host, port, username, and password.\nThe on_message module should contain a function on_message that will be called with the message payload as argument.\n\n\nflowerpower mqtt start_listener [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\non_message\nstr\nName of the module containing the on_message function\nRequired\n\n\ntopic\nstr\nMQTT topic to listen to\nRequired\n\n\nbase_dir\nstr\nBase directory for the module\nRequired\n\n\nhost\nstr\nMQTT broker host\nRequired\n\n\nport\nstr\nMQTT broker port\nRequired\n\n\nusername\nstr\nMQTT broker username\nRequired\n\n\npassword\nstr\nMQTT broker password\nRequired\n\n\n\n\n\n\n$ flowerpower mqtt start_listener --on-message my_module --topic my_topic --base-dir /path/to/module"
  },
  {
    "objectID": "api/cli_mqtt.html#run_pipeline_on_message",
    "href": "api/cli_mqtt.html#run_pipeline_on_message",
    "title": "flowerpower mqtt Commands",
    "section": "",
    "text": "Run a pipeline on a message\nThis command sets up an MQTT listener that executes a pipeline whenever a message is received on the specified topic. The pipeline can be configured to retry on failure using exponential backoff with jitter for better resilience.\n\n\nflowerpower mqtt run_pipeline_on_message [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline\nRequired\n\n\ntopic\nstr\nMQTT topic to listen to\nRequired\n\n\nexecutor\nstr\nName of the executor\nRequired\n\n\nbase_dir\nstr\nBase directory for the pipeline\nRequired\n\n\ninputs\nstr\nInputs as JSON or key=value pairs or dict string\nRequired\n\n\nfinal_vars\nstr\nFinal variables as JSON or list\nRequired\n\n\nconfig\nstr\nConfig for the hamilton pipeline executor\nRequired\n\n\nwith_tracker\nstr\nEnable tracking with hamilton ui\nRequired\n\n\nwith_opentelemetry\nstr\nEnable OpenTelemetry tracing\nRequired\n\n\nwith_progressbar\nstr\nEnable progress bar\nRequired\n\n\nstorage_options\nstr\nStorage options as JSON, dict string or key=value pairs\nRequired\n\n\nas_job\nstr\nRun as a job in the scheduler\nRequired\n\n\nhost\nstr\nMQTT broker host\nRequired\n\n\nport\nstr\nMQTT broker port\nRequired\n\n\nusername\nstr\nMQTT broker username\nRequired\n\n\npassword\nstr\nMQTT broker password\nRequired\n\n\nclean_session\nstr\nWhether to start a clean session with the broker\nRequired\n\n\nqos\nstr\nMQTT Quality of Service level (0, 1, or 2)\nRequired\n\n\nclient_id\nstr\nCustom MQTT client identifier\nRequired\n\n\nclient_id_suffix\nstr\nOptional suffix to append to client_id\nRequired\n\n\nconfig_hook\nstr\nFunction to process incoming messages into pipeline config\nRequired\n\n\nmax_retries\nstr\nMaximum number of retry attempts if pipeline execution fails\nRequired\n\n\nretry_delay\nstr\nBase delay between retries in seconds\nRequired\n\n\njitter_factor\nstr\nRandom factor (0-1) applied to delay for jitter\nRequired\n\n\n\n\n\n\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic sensors/data\n\n# Configure retries for resilience\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic sensors/data --max-retries 5 --retry-delay 2.0\n\n# Run as a job with custom MQTT settings\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic events/process --as-job --qos 2 --host mqtt.example.com\n\n# Use a config hook to process messages\n$ flowerpower mqtt run-pipeline-on-message my_pipeline --topic data/incoming --config-hook process_message"
  },
  {
    "objectID": "api/cli.html",
    "href": "api/cli.html",
    "title": "CLI Reference",
    "section": "",
    "text": "This section provides a comprehensive reference for the FlowerPower Command Line Interface (CLI).\n\n\n\n\n\nInitialize a new FlowerPower project.\nThis command creates a new FlowerPower project with the necessary directory structure and configuration files. If no project name is provided, the current directory name will be used as the project name.\n\n\nflowerpower init [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nproject_name\nstr\nName of the FlowerPower project to create. If not provided,\nRequired\n\n\nbase_dir\nstr\nBase directory where the project will be created. If not provided,\nRequired\n\n\nstorage_options\nstr\nStorage options for filesystem access, as a JSON or dict string\nRequired\n\n\njob_queue_type\nstr\nType of job queue backend to use (rq)\nRequired\n\n\n\n\n\n\n$ flowerpower init\n\n# Create a project with a specific name\n$ flowerpower init --name my-awesome-project\n\n# Create a project in a specific location\n$ flowerpower init --name my-project --base-dir /path/to/projects\n\n# Create a project with RQ as the job queue backend (default)\n$ flowerpower init --job-queue-type rq\n\n\n\n\n\nStart the Hamilton UI web application.\nThis command launches the Hamilton UI, which provides a web interface for visualizing and interacting with your FlowerPower pipelines. The UI allows you to explore pipeline execution graphs, view results, and manage jobs.\n\n\nflowerpower ui [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nport\nstr\nPort to run the UI server on\nRequired\n\n\nbase_dir\nstr\nBase directory where the UI will store its data\nRequired\n\n\nno_migration\nstr\nSkip running database migrations on startup\nRequired\n\n\nno_open\nstr\nPrevent automatically opening the browser\nRequired\n\n\nsettings_file\nstr\nSettings profile to use (mini, dev, prod)\nRequired\n\n\nconfig_file\nstr\nOptional custom configuration file path\nRequired\n\n\n\n\n\n\n$ flowerpower ui\n\n# Run the UI on a specific port\n$ flowerpower ui --port 9000\n\n# Use a custom data directory\n$ flowerpower ui --base-dir ~/my-project/.hamilton-data\n\n# Start without opening a browser\n$ flowerpower ui --no-open\n\n# Use production settings\n$ flowerpower ui --settings prod"
  },
  {
    "objectID": "api/cli.html#flowerpower-init",
    "href": "api/cli.html#flowerpower-init",
    "title": "CLI Reference",
    "section": "",
    "text": "Initialize a new FlowerPower project.\nThis command creates a new FlowerPower project with the necessary directory structure and configuration files. If no project name is provided, the current directory name will be used as the project name.\n\n\nflowerpower init [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nproject_name\nstr\nName of the FlowerPower project to create. If not provided,\nRequired\n\n\nbase_dir\nstr\nBase directory where the project will be created. If not provided,\nRequired\n\n\nstorage_options\nstr\nStorage options for filesystem access, as a JSON or dict string\nRequired\n\n\njob_queue_type\nstr\nType of job queue backend to use (rq)\nRequired\n\n\n\n\n\n\n$ flowerpower init\n\n# Create a project with a specific name\n$ flowerpower init --name my-awesome-project\n\n# Create a project in a specific location\n$ flowerpower init --name my-project --base-dir /path/to/projects\n\n# Create a project with RQ as the job queue backend (default)\n$ flowerpower init --job-queue-type rq"
  },
  {
    "objectID": "api/cli.html#flowerpower-ui",
    "href": "api/cli.html#flowerpower-ui",
    "title": "CLI Reference",
    "section": "",
    "text": "Start the Hamilton UI web application.\nThis command launches the Hamilton UI, which provides a web interface for visualizing and interacting with your FlowerPower pipelines. The UI allows you to explore pipeline execution graphs, view results, and manage jobs.\n\n\nflowerpower ui [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nport\nstr\nPort to run the UI server on\nRequired\n\n\nbase_dir\nstr\nBase directory where the UI will store its data\nRequired\n\n\nno_migration\nstr\nSkip running database migrations on startup\nRequired\n\n\nno_open\nstr\nPrevent automatically opening the browser\nRequired\n\n\nsettings_file\nstr\nSettings profile to use (mini, dev, prod)\nRequired\n\n\nconfig_file\nstr\nOptional custom configuration file path\nRequired\n\n\n\n\n\n\n$ flowerpower ui\n\n# Run the UI on a specific port\n$ flowerpower ui --port 9000\n\n# Use a custom data directory\n$ flowerpower ui --base-dir ~/my-project/.hamilton-data\n\n# Start without opening a browser\n$ flowerpower ui --no-open\n\n# Use production settings\n$ flowerpower ui --settings prod"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "This section provides a detailed reference for the FlowerPower API.\n\n\n\nFlowerPowerProject\nPipelineManager\nJobQueueManager\nRQManager\n\n\n\n\n\nConfiguration\n\n\n\n\n\ninit\n\n\n\n\n\nCLI Overview\nCLI Pipeline Commands\nCLI Job Queue Commands\nCLI MQTT Commands"
  },
  {
    "objectID": "api/index.html#core-components",
    "href": "api/index.html#core-components",
    "title": "API Reference",
    "section": "",
    "text": "FlowerPowerProject\nPipelineManager\nJobQueueManager\nRQManager"
  },
  {
    "objectID": "api/index.html#configuration",
    "href": "api/index.html#configuration",
    "title": "API Reference",
    "section": "",
    "text": "Configuration"
  },
  {
    "objectID": "api/index.html#top-level-functions",
    "href": "api/index.html#top-level-functions",
    "title": "API Reference",
    "section": "",
    "text": "init"
  },
  {
    "objectID": "api/index.html#cli-reference",
    "href": "api/index.html#cli-reference",
    "title": "API Reference",
    "section": "",
    "text": "CLI Overview\nCLI Pipeline Commands\nCLI Job Queue Commands\nCLI MQTT Commands"
  },
  {
    "objectID": "api/flowerpower.html",
    "href": "api/flowerpower.html",
    "title": "FlowerPower",
    "section": "",
    "text": "Module: flowerpower.flowerpower\nThe FlowerPower class is the main entry point for initializing and interacting with FlowerPower projects. It acts as a factory for FlowerPowerProject instances, allowing users to load existing projects or create new ones.\n\n\n\n\n__new__(cls, name: str | None = None, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, hooks_dir: str = settings.HOOKS_DIR) -&gt; FlowerPowerProject\nThis method is called when you instantiate FlowerPower(). It checks if a project already exists at the specified base_dir and either loads it or initializes a new one.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr \\| None\nThe name of the project. If None, it defaults to the current directory name.\nNone\n\n\nbase_dir\nstr \\| None\nThe base directory where the project will be created or loaded. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance to use for file operations.\nNone\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project (e.g., “rq”).\nsettings.JOB_QUEUE_TYPE\n\n\nhooks_dir\nstr\nThe directory where the project hooks will be stored.\nsettings.HOOKS_DIR\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject initialized with the new or loaded project.\n\n\nfrom flowerpower import FlowerPower\n\n# Initialize or load a project in the current directory\nproject = FlowerPower()\n\n# Initialize or load a project with a specific name and job queue type\nproject = FlowerPower(name=\"my-data-project\", job_queue_type=\"rq\")\n\n\n\n\n\nModule: flowerpower.flowerpower\nThe FlowerPowerProject class represents an initialized FlowerPower project, providing an interface to manage pipelines, job queues, and project-level settings.\n\n\n\n\n\n__init__(self, pipeline_manager: PipelineManager, job_queue_manager: JobQueueManager | None = None)\nInitializes a FlowerPowerProject instance. This constructor is typically called internally by FlowerPowerProject.load() or FlowerPowerProject.init().\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npipeline_manager\nPipelineManager\nAn instance of PipelineManager to manage pipelines within this project.\n\n\njob_queue_manager\nJobQueueManager \\| None\nAn optional instance of JobQueueManager to handle job queue operations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\npipeline_manager\nPipelineManager\nManages pipelines within the project.\n\n\njob_queue_manager\nJobQueueManager \\| None\nManages job queue operations, if configured.\n\n\nname\nstr\nThe name of the current project.\n\n\n_base_dir\nstr\nThe base directory of the project.\n\n\n_fs\nAbstractFileSystem\nThe fsspec-compatible filesystem instance used by the project.\n\n\n_storage_options\ndict \\| Munch \\| BaseStorageOptions\nStorage options for the filesystem.\n\n\njob_queue_type\nstr \\| None\nThe type of job queue configured for the project (e.g., “rq”).\n\n\njob_queue_backend\nAny \\| None\nThe backend instance for the job queue, if configured.\n\n\n\n\n\n\n\n\nrun(self, name: str, inputs: dict | None = None, final_vars: list[str] | None = None, config: dict | None = None, cache: dict | None = None, executor_cfg: str | dict | ExecutorConfig | None = None, with_adapter_cfg: dict | WithAdapterConfig | None = None, pipeline_adapter_cfg: dict | PipelineAdapterConfig | None = None, project_adapter_cfg: dict | ProjectAdapterConfig | None = None, adapter: dict[str, Any] | None = None, reload: bool = False, log_level: str | None = None, max_retries: int | None = None, retry_delay: float | None = None, jitter_factor: float | None = None, retry_exceptions: tuple | list | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None) -&gt; dict[str, Any]\nExecute a pipeline synchronously and return its results.\nThis is a convenience method that delegates to the pipeline manager. It provides the same functionality as self.pipeline_manager.run().\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run. Must be a valid identifier.\n\n\n\ninputs\ndict \\| None\nOverride pipeline input values. Example: {\"data_date\": \"2025-04-28\"}\nNone\n\n\nfinal_vars\nlist[str] \\| None\nSpecify which output variables to return. Example: [\"model\", \"metrics\"]\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor. Example: {\"model\": \"LogisticRegression\"}\nNone\n\n\ncache\ndict \\| None\nCache configuration for results. Example: {\"recompute\": [\"node1\", \"final_node\"]}\nNone\n\n\nexecutor_cfg\nstr \\| dict \\| ExecutorConfig \\| None\nExecution configuration, can be: - str: Executor name, e.g. “threadpool”, “local” - dict: Raw config, e.g. {\"type\": \"threadpool\", \"max_workers\": 4} - ExecutorConfig: Structured config object\nNone\n\n\nwith_adapter_cfg\ndict \\| WithAdapterConfig \\| None\nAdapter settings for pipeline execution. Example: {\"opentelemetry\": True, \"tracker\": False}\nNone\n\n\npipeline_adapter_cfg\ndict \\| PipelineAdapterConfig \\| None\nPipeline-specific adapter settings. Example: {\"tracker\": {\"project_id\": \"123\", \"tags\": {\"env\": \"prod\"}}}\nNone\n\n\nproject_adapter_cfg\ndict \\| ProjectAdapterConfig \\| None\nProject-level adapter settings. Example: {\"opentelemetry\": {\"host\": \"http://localhost:4317\"}}\nNone\n\n\nadapter\ndict[str, Any] \\| None\nCustom adapter instance for pipeline Example: {\"ray_graph_adapter\": RayGraphAdapter()}\nNone\n\n\nreload\nbool\nForce reload of pipeline configuration.\nFalse\n\n\nlog_level\nstr \\| None\nLogging level for the execution. Valid values: “DEBUG”, “INFO”, “WARNING”, “ERROR”, “CRITICAL”\nNone\n\n\nmax_retries\nint \\| None\nMaximum number of retries for execution.\nNone\n\n\nretry_delay\nfloat \\| None\nDelay between retries in seconds.\nNone\n\n\njitter_factor\nfloat \\| None\nRandom jitter factor to add to retry delay\nNone\n\n\nretry_exceptions\ntuple \\| list \\| None\nExceptions that trigger a retry.\nNone\n\n\non_success\nCallable \\| tuple[Callable, tuple | None, dict | None] \\| None\nCallback to run on successful pipeline execution.\nNone\n\n\non_failure\nCallable \\| tuple[Callable, tuple | None, dict | None] \\| None\nCallback to run on pipeline execution failure.\nNone\n\n\n\nReturns: dict[str, Any] - Pipeline execution results, mapping output variable names to their computed values.\nRaises:\n\nValueError: If pipeline name doesn’t exist or configuration is invalid.\nImportError: If pipeline module cannot be imported.\nRuntimeError: If execution fails due to pipeline or adapter errors.\n\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Simple execution\nresult = project.run(\"my_pipeline\")\n\n# With custom inputs\nresult = project.run(\n    \"ml_pipeline\",\n    inputs={\"data_date\": \"2025-01-01\"},\n    final_vars=[\"model\", \"metrics\"]\n)\n\n\n\n\nenqueue(self, name: str, *args, **kwargs)\nEnqueue a pipeline for execution via the job queue.\nThis is a convenience method that delegates to the job queue manager’s enqueue_pipeline method. It provides asynchronous pipeline execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nName of the pipeline to enqueue.\n\n\n*args\nAny\nAdditional positional arguments for job execution.\n\n\n**kwargs\nAny\nKeyword arguments for pipeline execution and job queue options. Supports all parameters from pipeline_manager.run() plus job queue specific options: - run_in: Schedule the job to run after a delay - run_at: Schedule the job to run at a specific datetime - queue_name: Queue to use (for RQ) - timeout: Job execution timeout - retry: Number of retries - result_ttl: Result time to live - ttl: Job time to live\n\n\n\nReturns: Job - Job ID or result depending on implementation, or None if job queue not configured.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\nfrom datetime import datetime\n\nproject = FlowerPowerProject.load(\".\")\n\n# Immediate execution via job queue\njob_id = project.enqueue(\"my_pipeline\", inputs={\"date\": \"today\"})\n\n# Delayed execution\njob_id = project.enqueue(\"my_pipeline\", inputs={\"date\": \"today\"}, run_in=300)\n\n# Scheduled execution\njob_id = project.enqueue(\n    \"my_pipeline\",\n    inputs={\"date\": \"today\"},\n    run_at=datetime(2025, 1, 1, 9, 0)\n)\n\n\n\n\nschedule(self, name: str, *args, **kwargs)\nSchedule a pipeline for recurring or future execution.\nThis is a convenience method that delegates to the job queue manager’s schedule_pipeline method. It provides scheduled pipeline execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nName of the pipeline to schedule.\n\n\n*args\nAny\nAdditional positional arguments for scheduling.\n\n\n**kwargs\nAny\nKeyword arguments for pipeline execution and scheduling options. Supports all parameters from pipeline_manager.run() plus scheduling options: - cron: Cron expression for recurring execution (e.g., “0 9 * * *“) - interval: Time interval for recurring execution (int seconds or dict) - date: Future date for one-time execution (datetime or ISO string) - schedule_id: Unique identifier for the schedule - overwrite: Whether to overwrite existing schedule with same ID\n\n\n\nReturns: ScheduledJob - Schedule ID or job ID depending on implementation, or None if job queue not configured.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\nfrom datetime import datetime, timedelta\n\nproject = FlowerPowerProject.load(\".\")\n\n# Daily schedule with cron\nschedule_id = project.schedule(\n    \"daily_metrics\",\n    cron=\"0 9 * * *\",  # 9 AM daily\n    inputs={\"date\": \"{{ execution_date }}\"}\n)\n\n# Interval-based schedule\nschedule_id = project.schedule(\n    \"monitoring\",\n    interval={\"minutes\": 15},\n    inputs={\"check_type\": \"health\"}\n)\n\n# Future one-time execution\nfuture_date = datetime.now() + timedelta(days=1)\nschedule_id = project.schedule(\n    \"batch_process\",\n    date=future_date,\n    inputs={\"process_date\": \"tomorrow\"}\n)\n\n\n\n\nstart_worker(self, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = True, **kwargs: Any) -&gt; None\nStart a worker process for processing jobs from the queues.\nThis is a convenience method that delegates to the job queue manager’s start_worker method.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbackground\nbool\nIf True, runs the worker in a non-blocking background mode. If False, runs in the current process and blocks until stopped.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nList of queue names to process. If None, processes all queues defined in the backend configuration.\nNone\n\n\nwith_scheduler\nbool\nWhether to include the scheduler queue for processing scheduled jobs (if supported by the backend).\nTrue\n\n\n**kwargs\nAny\nAdditional worker configuration options specific to the job queue backend.\n\n\n\n\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Start worker in foreground (blocks)\nproject.start_worker()\n\n# Start worker in background\nproject.start_worker(background=True)\n\n# Start worker for specific queues\nproject.start_worker(queue_names=[\"high_priority\", \"default\"])\n\n\n\n\nstop_worker(self) -&gt; None\nStop the worker process.\nThis is a convenience method that delegates to the job queue manager’s stop_worker method.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\nproject.stop_worker()\n\n\n\n\nstart_worker_pool(self, num_workers: int | None = None, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = True, **kwargs: Any) -&gt; None\nStart a pool of worker processes to handle jobs in parallel.\nThis is a convenience method that delegates to the job queue manager’s start_worker_pool method.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnum_workers\nint \\| None\nNumber of worker processes to start. If None, uses CPU count or backend-specific default.\nNone\n\n\nbackground\nbool\nIf True, runs the worker pool in a non-blocking background mode. If False, runs in the current process and blocks until stopped.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nList of queue names to process. If None, processes all queues defined in the backend configuration.\nNone\n\n\nwith_scheduler\nbool\nWhether to include the scheduler queue for processing scheduled jobs (if supported by the backend).\nTrue\n\n\n**kwargs\nAny\nAdditional worker pool configuration options specific to the job queue backend.\n\n\n\n\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Start worker pool with default number of workers\nproject.start_worker_pool()\n\n# Start 4 workers in background\nproject.start_worker_pool(num_workers=4, background=True)\n\n# Start worker pool for specific queues\nproject.start_worker_pool(\n    num_workers=2,\n    queue_names=[\"high_priority\", \"default\"]\n)\n\n\n\n\nstop_worker_pool(self) -&gt; None\nStop all worker processes in the worker pool.\nThis is a convenience method that delegates to the job queue manager’s stop_worker_pool method.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\nproject.stop_worker_pool()\n\n\n\n\nload(cls, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, log_level: str | None = None) -&gt; \"FlowerPowerProject\"\nLoad an existing FlowerPower project.\nIf the project does not exist, it will raise an error.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn instance of AbstractFileSystem to use for file operations.\nNone\n\n\nlog_level\nstr \\| None\nThe logging level to set for the project. If None, it uses the default log level.\nNone\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject if the project exists, otherwise None.\nRaises: FileNotFoundError: If the project does not exist at the specified base directory.\n\n\nfrom flowerpower import FlowerPowerProject\n\n# Load a project from the current directory\nproject = FlowerPowerProject.load(\".\")\n\n# Load a project from a specific path\nproject = FlowerPowerProject.load(\"/path/to/my/project\")\n\n\n\n\ninit(cls, name: str | None = None, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, hooks_dir: str = settings.HOOKS_DIR, log_level: str | None = None) -&gt; \"FlowerPowerProject\"\nInitialize a new FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr \\| None\nThe name of the project. If None, it defaults to the current directory name.\nNone\n\n\nbase_dir\nstr \\| None\nThe base directory where the project will be created. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn instance of AbstractFileSystem to use for file operations.\nNone\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project.\nsettings.JOB_QUEUE_TYPE\n\n\nhooks_dir\nstr\nThe directory where the project hooks will be stored.\nsettings.HOOKS_DIR\n\n\nlog_level\nstr \\| None\nThe logging level to set for the project. If None, it uses the default log level.\nNone\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject initialized with the new project.\nRaises: FileExistsError: If the project already exists at the specified base directory.\n\n\nfrom flowerpower import FlowerPowerProject\n\n# Initialize a new project in the current directory\nproject = FlowerPowerProject.init()\n\n# Initialize a new project with a specific name and job queue type\nproject = FlowerPowerProject.init(name=\"my-new-project\", job_queue_type=\"rq\")"
  },
  {
    "objectID": "api/flowerpower.html#initialization",
    "href": "api/flowerpower.html#initialization",
    "title": "FlowerPower",
    "section": "",
    "text": "__new__(cls, name: str | None = None, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, hooks_dir: str = settings.HOOKS_DIR) -&gt; FlowerPowerProject\nThis method is called when you instantiate FlowerPower(). It checks if a project already exists at the specified base_dir and either loads it or initializes a new one.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr \\| None\nThe name of the project. If None, it defaults to the current directory name.\nNone\n\n\nbase_dir\nstr \\| None\nThe base directory where the project will be created or loaded. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn fsspec-compatible filesystem instance to use for file operations.\nNone\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project (e.g., “rq”).\nsettings.JOB_QUEUE_TYPE\n\n\nhooks_dir\nstr\nThe directory where the project hooks will be stored.\nsettings.HOOKS_DIR\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject initialized with the new or loaded project.\n\n\nfrom flowerpower import FlowerPower\n\n# Initialize or load a project in the current directory\nproject = FlowerPower()\n\n# Initialize or load a project with a specific name and job queue type\nproject = FlowerPower(name=\"my-data-project\", job_queue_type=\"rq\")"
  },
  {
    "objectID": "api/flowerpower.html#flowerpowerproject",
    "href": "api/flowerpower.html#flowerpowerproject",
    "title": "FlowerPower",
    "section": "",
    "text": "Module: flowerpower.flowerpower\nThe FlowerPowerProject class represents an initialized FlowerPower project, providing an interface to manage pipelines, job queues, and project-level settings."
  },
  {
    "objectID": "api/flowerpower.html#initialization-1",
    "href": "api/flowerpower.html#initialization-1",
    "title": "FlowerPower",
    "section": "",
    "text": "__init__(self, pipeline_manager: PipelineManager, job_queue_manager: JobQueueManager | None = None)\nInitializes a FlowerPowerProject instance. This constructor is typically called internally by FlowerPowerProject.load() or FlowerPowerProject.init().\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npipeline_manager\nPipelineManager\nAn instance of PipelineManager to manage pipelines within this project.\n\n\njob_queue_manager\nJobQueueManager \\| None\nAn optional instance of JobQueueManager to handle job queue operations."
  },
  {
    "objectID": "api/flowerpower.html#attributes",
    "href": "api/flowerpower.html#attributes",
    "title": "FlowerPower",
    "section": "",
    "text": "Attribute\nType\nDescription\n\n\n\n\npipeline_manager\nPipelineManager\nManages pipelines within the project.\n\n\njob_queue_manager\nJobQueueManager \\| None\nManages job queue operations, if configured.\n\n\nname\nstr\nThe name of the current project.\n\n\n_base_dir\nstr\nThe base directory of the project.\n\n\n_fs\nAbstractFileSystem\nThe fsspec-compatible filesystem instance used by the project.\n\n\n_storage_options\ndict \\| Munch \\| BaseStorageOptions\nStorage options for the filesystem.\n\n\njob_queue_type\nstr \\| None\nThe type of job queue configured for the project (e.g., “rq”).\n\n\njob_queue_backend\nAny \\| None\nThe backend instance for the job queue, if configured."
  },
  {
    "objectID": "api/flowerpower.html#methods",
    "href": "api/flowerpower.html#methods",
    "title": "FlowerPower",
    "section": "",
    "text": "run(self, name: str, inputs: dict | None = None, final_vars: list[str] | None = None, config: dict | None = None, cache: dict | None = None, executor_cfg: str | dict | ExecutorConfig | None = None, with_adapter_cfg: dict | WithAdapterConfig | None = None, pipeline_adapter_cfg: dict | PipelineAdapterConfig | None = None, project_adapter_cfg: dict | ProjectAdapterConfig | None = None, adapter: dict[str, Any] | None = None, reload: bool = False, log_level: str | None = None, max_retries: int | None = None, retry_delay: float | None = None, jitter_factor: float | None = None, retry_exceptions: tuple | list | None = None, on_success: Callable | tuple[Callable, tuple | None, dict | None] | None = None, on_failure: Callable | tuple[Callable, tuple | None, dict | None] | None = None) -&gt; dict[str, Any]\nExecute a pipeline synchronously and return its results.\nThis is a convenience method that delegates to the pipeline manager. It provides the same functionality as self.pipeline_manager.run().\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run. Must be a valid identifier.\n\n\n\ninputs\ndict \\| None\nOverride pipeline input values. Example: {\"data_date\": \"2025-04-28\"}\nNone\n\n\nfinal_vars\nlist[str] \\| None\nSpecify which output variables to return. Example: [\"model\", \"metrics\"]\nNone\n\n\nconfig\ndict \\| None\nConfiguration for Hamilton pipeline executor. Example: {\"model\": \"LogisticRegression\"}\nNone\n\n\ncache\ndict \\| None\nCache configuration for results. Example: {\"recompute\": [\"node1\", \"final_node\"]}\nNone\n\n\nexecutor_cfg\nstr \\| dict \\| ExecutorConfig \\| None\nExecution configuration, can be: - str: Executor name, e.g. “threadpool”, “local” - dict: Raw config, e.g. {\"type\": \"threadpool\", \"max_workers\": 4} - ExecutorConfig: Structured config object\nNone\n\n\nwith_adapter_cfg\ndict \\| WithAdapterConfig \\| None\nAdapter settings for pipeline execution. Example: {\"opentelemetry\": True, \"tracker\": False}\nNone\n\n\npipeline_adapter_cfg\ndict \\| PipelineAdapterConfig \\| None\nPipeline-specific adapter settings. Example: {\"tracker\": {\"project_id\": \"123\", \"tags\": {\"env\": \"prod\"}}}\nNone\n\n\nproject_adapter_cfg\ndict \\| ProjectAdapterConfig \\| None\nProject-level adapter settings. Example: {\"opentelemetry\": {\"host\": \"http://localhost:4317\"}}\nNone\n\n\nadapter\ndict[str, Any] \\| None\nCustom adapter instance for pipeline Example: {\"ray_graph_adapter\": RayGraphAdapter()}\nNone\n\n\nreload\nbool\nForce reload of pipeline configuration.\nFalse\n\n\nlog_level\nstr \\| None\nLogging level for the execution. Valid values: “DEBUG”, “INFO”, “WARNING”, “ERROR”, “CRITICAL”\nNone\n\n\nmax_retries\nint \\| None\nMaximum number of retries for execution.\nNone\n\n\nretry_delay\nfloat \\| None\nDelay between retries in seconds.\nNone\n\n\njitter_factor\nfloat \\| None\nRandom jitter factor to add to retry delay\nNone\n\n\nretry_exceptions\ntuple \\| list \\| None\nExceptions that trigger a retry.\nNone\n\n\non_success\nCallable \\| tuple[Callable, tuple | None, dict | None] \\| None\nCallback to run on successful pipeline execution.\nNone\n\n\non_failure\nCallable \\| tuple[Callable, tuple | None, dict | None] \\| None\nCallback to run on pipeline execution failure.\nNone\n\n\n\nReturns: dict[str, Any] - Pipeline execution results, mapping output variable names to their computed values.\nRaises:\n\nValueError: If pipeline name doesn’t exist or configuration is invalid.\nImportError: If pipeline module cannot be imported.\nRuntimeError: If execution fails due to pipeline or adapter errors.\n\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Simple execution\nresult = project.run(\"my_pipeline\")\n\n# With custom inputs\nresult = project.run(\n    \"ml_pipeline\",\n    inputs={\"data_date\": \"2025-01-01\"},\n    final_vars=[\"model\", \"metrics\"]\n)\n\n\n\n\nenqueue(self, name: str, *args, **kwargs)\nEnqueue a pipeline for execution via the job queue.\nThis is a convenience method that delegates to the job queue manager’s enqueue_pipeline method. It provides asynchronous pipeline execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nName of the pipeline to enqueue.\n\n\n*args\nAny\nAdditional positional arguments for job execution.\n\n\n**kwargs\nAny\nKeyword arguments for pipeline execution and job queue options. Supports all parameters from pipeline_manager.run() plus job queue specific options: - run_in: Schedule the job to run after a delay - run_at: Schedule the job to run at a specific datetime - queue_name: Queue to use (for RQ) - timeout: Job execution timeout - retry: Number of retries - result_ttl: Result time to live - ttl: Job time to live\n\n\n\nReturns: Job - Job ID or result depending on implementation, or None if job queue not configured.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\nfrom datetime import datetime\n\nproject = FlowerPowerProject.load(\".\")\n\n# Immediate execution via job queue\njob_id = project.enqueue(\"my_pipeline\", inputs={\"date\": \"today\"})\n\n# Delayed execution\njob_id = project.enqueue(\"my_pipeline\", inputs={\"date\": \"today\"}, run_in=300)\n\n# Scheduled execution\njob_id = project.enqueue(\n    \"my_pipeline\",\n    inputs={\"date\": \"today\"},\n    run_at=datetime(2025, 1, 1, 9, 0)\n)\n\n\n\n\nschedule(self, name: str, *args, **kwargs)\nSchedule a pipeline for recurring or future execution.\nThis is a convenience method that delegates to the job queue manager’s schedule_pipeline method. It provides scheduled pipeline execution.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nName of the pipeline to schedule.\n\n\n*args\nAny\nAdditional positional arguments for scheduling.\n\n\n**kwargs\nAny\nKeyword arguments for pipeline execution and scheduling options. Supports all parameters from pipeline_manager.run() plus scheduling options: - cron: Cron expression for recurring execution (e.g., “0 9 * * *“) - interval: Time interval for recurring execution (int seconds or dict) - date: Future date for one-time execution (datetime or ISO string) - schedule_id: Unique identifier for the schedule - overwrite: Whether to overwrite existing schedule with same ID\n\n\n\nReturns: ScheduledJob - Schedule ID or job ID depending on implementation, or None if job queue not configured.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\nfrom datetime import datetime, timedelta\n\nproject = FlowerPowerProject.load(\".\")\n\n# Daily schedule with cron\nschedule_id = project.schedule(\n    \"daily_metrics\",\n    cron=\"0 9 * * *\",  # 9 AM daily\n    inputs={\"date\": \"{{ execution_date }}\"}\n)\n\n# Interval-based schedule\nschedule_id = project.schedule(\n    \"monitoring\",\n    interval={\"minutes\": 15},\n    inputs={\"check_type\": \"health\"}\n)\n\n# Future one-time execution\nfuture_date = datetime.now() + timedelta(days=1)\nschedule_id = project.schedule(\n    \"batch_process\",\n    date=future_date,\n    inputs={\"process_date\": \"tomorrow\"}\n)\n\n\n\n\nstart_worker(self, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = True, **kwargs: Any) -&gt; None\nStart a worker process for processing jobs from the queues.\nThis is a convenience method that delegates to the job queue manager’s start_worker method.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbackground\nbool\nIf True, runs the worker in a non-blocking background mode. If False, runs in the current process and blocks until stopped.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nList of queue names to process. If None, processes all queues defined in the backend configuration.\nNone\n\n\nwith_scheduler\nbool\nWhether to include the scheduler queue for processing scheduled jobs (if supported by the backend).\nTrue\n\n\n**kwargs\nAny\nAdditional worker configuration options specific to the job queue backend.\n\n\n\n\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Start worker in foreground (blocks)\nproject.start_worker()\n\n# Start worker in background\nproject.start_worker(background=True)\n\n# Start worker for specific queues\nproject.start_worker(queue_names=[\"high_priority\", \"default\"])\n\n\n\n\nstop_worker(self) -&gt; None\nStop the worker process.\nThis is a convenience method that delegates to the job queue manager’s stop_worker method.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\nproject.stop_worker()\n\n\n\n\nstart_worker_pool(self, num_workers: int | None = None, background: bool = False, queue_names: list[str] | None = None, with_scheduler: bool = True, **kwargs: Any) -&gt; None\nStart a pool of worker processes to handle jobs in parallel.\nThis is a convenience method that delegates to the job queue manager’s start_worker_pool method.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nnum_workers\nint \\| None\nNumber of worker processes to start. If None, uses CPU count or backend-specific default.\nNone\n\n\nbackground\nbool\nIf True, runs the worker pool in a non-blocking background mode. If False, runs in the current process and blocks until stopped.\nFalse\n\n\nqueue_names\nlist[str] \\| None\nList of queue names to process. If None, processes all queues defined in the backend configuration.\nNone\n\n\nwith_scheduler\nbool\nWhether to include the scheduler queue for processing scheduled jobs (if supported by the backend).\nTrue\n\n\n**kwargs\nAny\nAdditional worker pool configuration options specific to the job queue backend.\n\n\n\n\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\n\n# Start worker pool with default number of workers\nproject.start_worker_pool()\n\n# Start 4 workers in background\nproject.start_worker_pool(num_workers=4, background=True)\n\n# Start worker pool for specific queues\nproject.start_worker_pool(\n    num_workers=2,\n    queue_names=[\"high_priority\", \"default\"]\n)\n\n\n\n\nstop_worker_pool(self) -&gt; None\nStop all worker processes in the worker pool.\nThis is a convenience method that delegates to the job queue manager’s stop_worker_pool method.\nRaises: RuntimeError: If job queue manager is not configured.\n\n\nfrom flowerpower import FlowerPowerProject\n\nproject = FlowerPowerProject.load(\".\")\nproject.stop_worker_pool()\n\n\n\n\nload(cls, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, log_level: str | None = None) -&gt; \"FlowerPowerProject\"\nLoad an existing FlowerPower project.\nIf the project does not exist, it will raise an error.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr \\| None\nThe base directory of the project. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn instance of AbstractFileSystem to use for file operations.\nNone\n\n\nlog_level\nstr \\| None\nThe logging level to set for the project. If None, it uses the default log level.\nNone\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject if the project exists, otherwise None.\nRaises: FileNotFoundError: If the project does not exist at the specified base directory.\n\n\nfrom flowerpower import FlowerPowerProject\n\n# Load a project from the current directory\nproject = FlowerPowerProject.load(\".\")\n\n# Load a project from a specific path\nproject = FlowerPowerProject.load(\"/path/to/my/project\")\n\n\n\n\ninit(cls, name: str | None = None, base_dir: str | None = None, storage_options: dict | BaseStorageOptions | None = {}, fs: AbstractFileSystem | None = None, job_queue_type: str = settings.JOB_QUEUE_TYPE, hooks_dir: str = settings.HOOKS_DIR, log_level: str | None = None) -&gt; \"FlowerPowerProject\"\nInitialize a new FlowerPower project.\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\nDefault\n\n\n\n\nname\nstr \\| None\nThe name of the project. If None, it defaults to the current directory name.\nNone\n\n\nbase_dir\nstr \\| None\nThe base directory where the project will be created. If None, it defaults to the current working directory.\nNone\n\n\nstorage_options\ndict \\| BaseStorageOptions \\| None\nStorage options for the filesystem.\n{}\n\n\nfs\nAbstractFileSystem \\| None\nAn instance of AbstractFileSystem to use for file operations.\nNone\n\n\njob_queue_type\nstr\nThe type of job queue to use for the project.\nsettings.JOB_QUEUE_TYPE\n\n\nhooks_dir\nstr\nThe directory where the project hooks will be stored.\nsettings.HOOKS_DIR\n\n\nlog_level\nstr \\| None\nThe logging level to set for the project. If None, it uses the default log level.\nNone\n\n\n\nReturns: FlowerPowerProject - An instance of FlowerPowerProject initialized with the new project.\nRaises: FileExistsError: If the project already exists at the specified base directory.\n\n\nfrom flowerpower import FlowerPowerProject\n\n# Initialize a new project in the current directory\nproject = FlowerPowerProject.init()\n\n# Initialize a new project with a specific name and job queue type\nproject = FlowerPowerProject.init(name=\"my-new-project\", job_queue_type=\"rq\")"
  },
  {
    "objectID": "api/cli_pipeline.html",
    "href": "api/cli_pipeline.html",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "This section details the commands available under flowerpower pipeline.\n\n\nRun a pipeline immediately.\nThis command executes a pipeline with the specified configuration and inputs. The pipeline will run synchronously, and the command will wait for completion.\n\n\nflowerpower pipeline run [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run\nRequired\n\n\nexecutor\nstr\nType of executor to use\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nconfig\nstr\nConfiguration for the Hamilton executor\nRequired\n\n\ncache\nstr\nCache configuration for improved performance\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nwith_adapter\nstr\nConfiguration for adapters like trackers or monitors\nRequired\n\n\nmax_retries\nstr\nMaximum number of retry attempts on failure\nRequired\n\n\nretry_delay\nstr\nBase delay between retries in seconds\nRequired\n\n\njitter_factor\nstr\nRandom factor applied to delay for jitter (0-1)\nRequired\n\n\n\n\n\n\n$ pipeline run my_pipeline\n\n# Run with custom inputs\n$ pipeline run my_pipeline --inputs '{\"data_path\": \"data/myfile.csv\", \"limit\": 100}'\n\n# Specify which final variables to calculate\n$ pipeline run my_pipeline --final-vars '[\"output_table\", \"summary_metrics\"]'\n\n# Configure caching\n$ pipeline run my_pipeline --cache '{\"type\": \"memory\", \"ttl\": 3600}'\n\n# Use a different executor\n$ pipeline run my_pipeline --executor distributed\n\n# Enable adapters for monitoring/tracking\n$ pipeline run my_pipeline --with-adapter '{\"tracker\": true, \"opentelemetry\": true}'\n\n# Set a specific logging level\n$ pipeline run my_pipeline --log-level debug\n\n# Configure automatic retries on failure\n$ pipeline run my_pipeline --max-retries 3 --retry-delay 2.0 --jitter-factor 0.2\n\n\n\n\n\nCreate a new pipeline structure.\nThis command creates a new pipeline with the necessary directory structure, configuration file, and skeleton module file. It prepares all the required components for you to start implementing your pipeline logic.\n\n\nflowerpower pipeline new [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline\nRequired\n\n\nbase_dir\nstr\nBase directory to create the pipeline in\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\noverwrite\nstr\nWhether to overwrite existing pipeline with the same name\nRequired\n\n\n\n\n\n\n$ pipeline new my_new_pipeline\n\n# Create a pipeline, overwriting if it exists\n$ pipeline new my_new_pipeline --overwrite\n\n# Create a pipeline in a specific directory\n$ pipeline new my_new_pipeline --base-dir /path/to/project\n\n\n\n\n\nDelete a pipeline’s configuration and/or module files.\nThis command removes a pipeline’s configuration file and/or module file from the project. If neither –cfg nor –module is specified, both will be deleted.\n\n\nflowerpower pipeline delete [options]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to delete\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\ncfg\nstr\nDelete only the configuration file\nRequired\n\n\nmodule\nstr\nDelete only the pipeline module\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\n\n\n\n\n$ pipeline delete my_pipeline\n\n# Delete only the configuration file\n$ pipeline delete my_pipeline --cfg\n\n# Delete only the module file\n$ pipeline delete my_pipeline --module\n\n\n\n\n\nShow the DAG (Directed Acyclic Graph) of a pipeline.\nThis command generates and displays a visual representation of the pipeline’s execution graph, showing how nodes are connected and dependencies between them.\n\n\nflowerpower pipeline show_dag [options]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the visualization\nRequired\n\n\n\n\n\n\n$ pipeline show-dag my_pipeline\n\n# Generate SVG format visualization\n$ pipeline show-dag my_pipeline --format svg\n\n# Get raw graphviz object\n$ pipeline show-dag my_pipeline --format raw\n\n\n\n\n\nSave the DAG (Directed Acyclic Graph) of a pipeline to a file.\nThis command generates a visual representation of the pipeline’s execution graph and saves it to a file in the specified format.\n\n\nflowerpower pipeline save_dag [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the visualization\nRequired\n\n\noutput_path\nstr\nCustom file path to save the output (defaults to pipeline name)\nRequired\n\n\n\n\n\n\n$ pipeline save-dag my_pipeline\n\n# Save in SVG format\n$ pipeline save-dag my_pipeline --format svg\n\n# Save to a custom location\n$ pipeline save-dag my_pipeline --output-path ./visualizations/my_graph.png\n\n\n\n\n\nList all available pipelines in the project.\nThis command displays a list of all pipelines defined in the project, providing an overview of what pipelines are available to run or schedule.\n\n\nflowerpower pipeline show_pipelines [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr\nBase directory containing pipelines\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the list (table, json, yaml)\nRequired\n\n\n\n\n\n\n$ pipeline show-pipelines\n\n# Output in JSON format\n$ pipeline show-pipelines --format json\n\n# List pipelines from a specific directory\n$ pipeline show-pipelines --base-dir /path/to/project\n\n\n\n\n\nShow summary information for one or all pipelines.\nThis command displays detailed information about pipelines including their configuration, code structure, and project context. You can view information for a specific pipeline or get an overview of all pipelines.\n\n\nflowerpower pipeline show_summary [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of specific pipeline to summarize (all if not specified)\nRequired\n\n\ncfg\nstr\nInclude configuration details\nRequired\n\n\ncode\nstr\nInclude code/module details\nRequired\n\n\nproject\nstr\nInclude project context information\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nto_html\nstr\nGenerate HTML output instead of text\nRequired\n\n\nto_svg\nstr\nGenerate SVG output (where applicable)\nRequired\n\n\noutput_file\nstr\nFile path to save the output instead of printing to console\nRequired\n\n\n\n\n\n\n$ pipeline show-summary\n\n# Show summary for a specific pipeline\n$ pipeline show-summary --name my_pipeline\n\n# Show only configuration information\n$ pipeline show-summary --name my_pipeline --cfg --no-code --no-project\n\n# Generate HTML report\n$ pipeline show-summary --to-html --output-file pipeline_report.html\n\n\n\n\n\nAdd a hook to a pipeline configuration.\nThis command adds a hook function to a pipeline’s configuration. Hooks are functions that are called at specific points during pipeline execution to perform additional tasks like logging, monitoring, or data validation.\n\n\nflowerpower pipeline add_hook [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to add the hook to\nRequired\n\n\nfunction_name\nstr\nName of the hook function (must be defined in the pipeline module)\nRequired\n\n\ntype\nstr\nType of hook (determines when the hook is called during execution)\nRequired\n\n\nto\nstr\nTarget node or tag (required for node-specific hooks)\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\n\n\n\n\n$ pipeline add-hook my_pipeline --function log_results\n\n# Add a pre-run hook\n$ pipeline add-hook my_pipeline --function validate_inputs --type PRE_RUN\n\n# Add a node-specific hook (executed before a specific node runs)\n$ pipeline add-hook my_pipeline --function validate_data --type NODE_PRE_EXECUTE --to data_processor\n\n# Add a hook for all nodes with a specific tag\n$ pipeline add-hook my_pipeline --function log_metrics --type NODE_POST_EXECUTE --to @metrics"
  },
  {
    "objectID": "api/cli_pipeline.html#run",
    "href": "api/cli_pipeline.html#run",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Run a pipeline immediately.\nThis command executes a pipeline with the specified configuration and inputs. The pipeline will run synchronously, and the command will wait for completion.\n\n\nflowerpower pipeline run [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to run\nRequired\n\n\nexecutor\nstr\nType of executor to use\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines and configurations\nRequired\n\n\ninputs\nstr\nInput parameters for the pipeline\nRequired\n\n\nfinal_vars\nstr\nFinal variables to request from the pipeline\nRequired\n\n\nconfig\nstr\nConfiguration for the Hamilton executor\nRequired\n\n\ncache\nstr\nCache configuration for improved performance\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nwith_adapter\nstr\nConfiguration for adapters like trackers or monitors\nRequired\n\n\nmax_retries\nstr\nMaximum number of retry attempts on failure\nRequired\n\n\nretry_delay\nstr\nBase delay between retries in seconds\nRequired\n\n\njitter_factor\nstr\nRandom factor applied to delay for jitter (0-1)\nRequired\n\n\n\n\n\n\n$ pipeline run my_pipeline\n\n# Run with custom inputs\n$ pipeline run my_pipeline --inputs '{\"data_path\": \"data/myfile.csv\", \"limit\": 100}'\n\n# Specify which final variables to calculate\n$ pipeline run my_pipeline --final-vars '[\"output_table\", \"summary_metrics\"]'\n\n# Configure caching\n$ pipeline run my_pipeline --cache '{\"type\": \"memory\", \"ttl\": 3600}'\n\n# Use a different executor\n$ pipeline run my_pipeline --executor distributed\n\n# Enable adapters for monitoring/tracking\n$ pipeline run my_pipeline --with-adapter '{\"tracker\": true, \"opentelemetry\": true}'\n\n# Set a specific logging level\n$ pipeline run my_pipeline --log-level debug\n\n# Configure automatic retries on failure\n$ pipeline run my_pipeline --max-retries 3 --retry-delay 2.0 --jitter-factor 0.2"
  },
  {
    "objectID": "api/cli_pipeline.html#new",
    "href": "api/cli_pipeline.html#new",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Create a new pipeline structure.\nThis command creates a new pipeline with the necessary directory structure, configuration file, and skeleton module file. It prepares all the required components for you to start implementing your pipeline logic.\n\n\nflowerpower pipeline new [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName for the new pipeline\nRequired\n\n\nbase_dir\nstr\nBase directory to create the pipeline in\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\noverwrite\nstr\nWhether to overwrite existing pipeline with the same name\nRequired\n\n\n\n\n\n\n$ pipeline new my_new_pipeline\n\n# Create a pipeline, overwriting if it exists\n$ pipeline new my_new_pipeline --overwrite\n\n# Create a pipeline in a specific directory\n$ pipeline new my_new_pipeline --base-dir /path/to/project"
  },
  {
    "objectID": "api/cli_pipeline.html#delete",
    "href": "api/cli_pipeline.html#delete",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Delete a pipeline’s configuration and/or module files.\nThis command removes a pipeline’s configuration file and/or module file from the project. If neither –cfg nor –module is specified, both will be deleted.\n\n\nflowerpower pipeline delete [options]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to delete\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\ncfg\nstr\nDelete only the configuration file\nRequired\n\n\nmodule\nstr\nDelete only the pipeline module\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\n\n\n\n\n$ pipeline delete my_pipeline\n\n# Delete only the configuration file\n$ pipeline delete my_pipeline --cfg\n\n# Delete only the module file\n$ pipeline delete my_pipeline --module"
  },
  {
    "objectID": "api/cli_pipeline.html#show_dag",
    "href": "api/cli_pipeline.html#show_dag",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Show the DAG (Directed Acyclic Graph) of a pipeline.\nThis command generates and displays a visual representation of the pipeline’s execution graph, showing how nodes are connected and dependencies between them.\n\n\nflowerpower pipeline show_dag [options]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the visualization\nRequired\n\n\n\n\n\n\n$ pipeline show-dag my_pipeline\n\n# Generate SVG format visualization\n$ pipeline show-dag my_pipeline --format svg\n\n# Get raw graphviz object\n$ pipeline show-dag my_pipeline --format raw"
  },
  {
    "objectID": "api/cli_pipeline.html#save_dag",
    "href": "api/cli_pipeline.html#save_dag",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Save the DAG (Directed Acyclic Graph) of a pipeline to a file.\nThis command generates a visual representation of the pipeline’s execution graph and saves it to a file in the specified format.\n\n\nflowerpower pipeline save_dag [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to visualize\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the visualization\nRequired\n\n\noutput_path\nstr\nCustom file path to save the output (defaults to pipeline name)\nRequired\n\n\n\n\n\n\n$ pipeline save-dag my_pipeline\n\n# Save in SVG format\n$ pipeline save-dag my_pipeline --format svg\n\n# Save to a custom location\n$ pipeline save-dag my_pipeline --output-path ./visualizations/my_graph.png"
  },
  {
    "objectID": "api/cli_pipeline.html#show_pipelines",
    "href": "api/cli_pipeline.html#show_pipelines",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "List all available pipelines in the project.\nThis command displays a list of all pipelines defined in the project, providing an overview of what pipelines are available to run or schedule.\n\n\nflowerpower pipeline show_pipelines [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbase_dir\nstr\nBase directory containing pipelines\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nformat\nstr\nOutput format for the list (table, json, yaml)\nRequired\n\n\n\n\n\n\n$ pipeline show-pipelines\n\n# Output in JSON format\n$ pipeline show-pipelines --format json\n\n# List pipelines from a specific directory\n$ pipeline show-pipelines --base-dir /path/to/project"
  },
  {
    "objectID": "api/cli_pipeline.html#show_summary",
    "href": "api/cli_pipeline.html#show_summary",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Show summary information for one or all pipelines.\nThis command displays detailed information about pipelines including their configuration, code structure, and project context. You can view information for a specific pipeline or get an overview of all pipelines.\n\n\nflowerpower pipeline show_summary [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of specific pipeline to summarize (all if not specified)\nRequired\n\n\ncfg\nstr\nInclude configuration details\nRequired\n\n\ncode\nstr\nInclude code/module details\nRequired\n\n\nproject\nstr\nInclude project context information\nRequired\n\n\nbase_dir\nstr\nBase directory containing pipelines\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\nto_html\nstr\nGenerate HTML output instead of text\nRequired\n\n\nto_svg\nstr\nGenerate SVG output (where applicable)\nRequired\n\n\noutput_file\nstr\nFile path to save the output instead of printing to console\nRequired\n\n\n\n\n\n\n$ pipeline show-summary\n\n# Show summary for a specific pipeline\n$ pipeline show-summary --name my_pipeline\n\n# Show only configuration information\n$ pipeline show-summary --name my_pipeline --cfg --no-code --no-project\n\n# Generate HTML report\n$ pipeline show-summary --to-html --output-file pipeline_report.html"
  },
  {
    "objectID": "api/cli_pipeline.html#add_hook",
    "href": "api/cli_pipeline.html#add_hook",
    "title": "flowerpower pipeline Commands",
    "section": "",
    "text": "Add a hook to a pipeline configuration.\nThis command adds a hook function to a pipeline’s configuration. Hooks are functions that are called at specific points during pipeline execution to perform additional tasks like logging, monitoring, or data validation.\n\n\nflowerpower pipeline add_hook [options]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the pipeline to add the hook to\nRequired\n\n\nfunction_name\nstr\nName of the hook function (must be defined in the pipeline module)\nRequired\n\n\ntype\nstr\nType of hook (determines when the hook is called during execution)\nRequired\n\n\nto\nstr\nTarget node or tag (required for node-specific hooks)\nRequired\n\n\nbase_dir\nstr\nBase directory containing the pipeline\nRequired\n\n\nstorage_options\nstr\nOptions for storage backends\nRequired\n\n\nlog_level\nstr\nSet the logging level\nRequired\n\n\n\n\n\n\n$ pipeline add-hook my_pipeline --function log_results\n\n# Add a pre-run hook\n$ pipeline add-hook my_pipeline --function validate_inputs --type PRE_RUN\n\n# Add a node-specific hook (executed before a specific node runs)\n$ pipeline add-hook my_pipeline --function validate_data --type NODE_PRE_EXECUTE --to data_processor\n\n# Add a hook for all nodes with a specific tag\n$ pipeline add-hook my_pipeline --function log_metrics --type NODE_POST_EXECUTE --to @metrics"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Welcome to the FlowerPower installation guide. This page will walk you through the steps to get FlowerPower up and running on your system."
  },
  {
    "objectID": "installation.html#prerequisites",
    "href": "installation.html#prerequisites",
    "title": "Installation",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, ensure you have the following installed:\n\nPython 3.8 or higher: FlowerPower requires a modern version of Python. You can check your Python version by running:\npython --version\nA package manager: We recommend using a modern package manager like uv or pip for a smooth installation experience.\n\n\n\n\n\n\n\nNote\n\n\n\nProject and Environment Management\nFor robust project management, we highly recommend using tools like uv or pixi. These tools help you manage dependencies and ensure your projects are reproducible."
  },
  {
    "objectID": "installation.html#standard-installation",
    "href": "installation.html#standard-installation",
    "title": "Installation",
    "section": "Standard Installation",
    "text": "Standard Installation\nThe recommended way to install FlowerPower is with uv pip:\nuv pip install flowerpower\nAlternatively, you can use pip:\npip install flowerpower\nThis will install the core FlowerPower library with all the essential features to get you started."
  },
  {
    "objectID": "installation.html#optional-dependencies",
    "href": "installation.html#optional-dependencies",
    "title": "Installation",
    "section": "Optional Dependencies",
    "text": "Optional Dependencies\nFlowerPower offers optional dependencies that you can install to enable additional functionality.\n\nRQ Job Queue Support: To use FlowerPower with the Redis Queue (RQ) job queue, install the [rq] extra:\nuv pip install 'flowerpower[rq]'\nI/O Plugins: For additional I/O capabilities, install the [io] extra:\nuv pip install 'flowerpower[io]'\nHamilton UI: To use the Hamilton UI for interactive dataflow visualization, install the [ui] extra:\nuv pip install 'flowerpower[ui]'\nAll Extras: To install all optional dependencies at once, use the [all] extra:\nuv pip install 'flowerpower[all]'"
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installation",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you encounter issues during installation, here are a few tips:\n\nUse a Virtual Environment: It is highly recommended to install FlowerPower in a virtual environment to avoid conflicts with other packages. You can create one with uv:\nuv venv\nsource .venv/bin/activate\nCheck Your PATH: Ensure that your Python and script installation directories are in your system’s PATH. If you can’t run flowerpower from your terminal, this might be the issue.\nPermissions: If you get a permission error, you might be trying to install the package globally without the necessary privileges. Using a virtual environment is the best way to avoid this.\n\nIf you continue to have problems, please open an issue on our GitHub repository."
  }
]