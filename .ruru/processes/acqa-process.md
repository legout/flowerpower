# Process: Adaptive Confidence-based Quality Assurance (ACQA) v1.0

**Date:** 2025-04-15

**Authors:** Roo Commander, User (Jeremy), Gemini

---

**1. Core Concept & Goals**

**Concept:** ACQA establishes a standard operational procedure where AI-generated artifacts (code, configuration, documentation) are assessed for potential risk (via an internal "Confidence Score"). This score, combined with a user-defined "Caution Level," dictates the level of automated Quality Assurance (QA) applied before the artifact is finalized or presented. This process applies recursively to delegated tasks within the Roo Code multi-agent system.

**Goals:**
*   **Improve Reliability:** Proactively catch errors, logical flaws, or specification deviations.
*   **Increase User Trust:** Provide transparency and configurable assurance levels.
*   **Manage Complexity:** Apply appropriate scrutiny based on task difficulty and assessed risk.
*   **Optimize Workflow:** Balance QA overhead with speed according to user preference.
*   **Standardize Process:** Create a consistent, teachable workflow for all participating agents.

**Analogy:** A dynamic, context-aware peer review system where the depth of review adapts based on the perceived risk of the work and the project's overall quality requirements.

**2. Core Components**

*   **Agent Roles (within ACQA):**
    *   **Roo Commander (Coordinator):**
        *   Receives user requests, performs initial assessment.
        *   Determines/retrieves active `User Caution Level`.
        *   Delegates tasks to appropriate Developer Agents, providing necessary context (specs, files, user intent).
        *   Receives results (artifact + Confidence Score + rationale) from Developer Agents.
        *   Executes QA Decision Logic (Confidence vs. Caution).
        *   Delegates QA tasks (Reviewer/Tester) **with full context** (artifact, original prompt, relevant specifications, confidence score, required QA level).
        *   Receives QA feedback.
        *   **Critically analyzes** QA feedback for validity and significance against specifications.
        *   Initiates revisions via Boomerang Tasks or direct re-delegation if necessary, providing specific feedback.
        *   Integrates final results.
        *   Reports outcome and QA steps taken to the user.
    *   **Developer Agent(s) (e.g., `mode-maintainer`, `react-specialist`):**
        *   Responsible for generating/modifying artifacts per task instructions.
        *   **Mandatory:** Performs self-assessment to generate a `Confidence Score` (Low/Medium/High) and rationale for their output *before* reporting completion.
        *   Packages result: artifact + Confidence Score + rationale.
        *   Acts as a Coordinator for any sub-tasks they delegate, applying the ACQA process recursively.
    *   **Reviewer Agent(s) (e.g., `code-reviewer`, `second-opinion`, `technical-writer`):**
        *   Receives artifact, original prompt, specs, confidence score, and required QA level from the Coordinator.
        *   Analyzes the artifact *against the provided specifications and context*.
        *   Provides structured feedback on deviations, errors, inconsistencies, quality issues.
    *   **Tester Agent(s) (Optional/Advanced; e.g., `e2e-tester`, `integration-tester`):**
        *   Activated in low-confidence/high-caution scenarios.
        *   Focuses on identifying edge cases, suggesting test inputs/scenarios, potentially generating test stubs or using tools (`execute_command`) to run existing tests.

*   **Confidence Score:**
    *   **Nature:** Internal heuristic (Low, Medium, High) indicating estimated correctness/completeness risk. Generated by the Developer Agent.
    *   **Calculation Factors:** Task complexity/ambiguity/novelty, artifact length/complexity, error handling presence, risky patterns, dependencies touched, (optional) internal generation metrics.

*   **User Caution Level:**
    *   **Nature:** User-defined setting (`Minimal`, `Balanced`, `Thorough`) controlling QA overhead vs. speed.
    *   **Setting:**
        *   Default: `Balanced`.
        *   Configuration: Stored in a project configuration file (e.g., `.roo/project_config.toml`), potentially alongside user profile info (name, experience, preferred stack).
        *   Initial Setup: Commander can infer or ask during project onboarding.

*   **QA Actions (Triggered by Coordinator based on Confidence/Caution):**
    *   `None` (High Confidence / Minimal Caution)
    *   `Basic Linting/Formatting` (Potentially via `execute_command`)
    *   `Standard AI Peer Review` (Reviewer Agent, Balanced/Thorough Caution for Medium/Low Confidence)
    *   `Intensive AI Peer Review` (Reviewer Agent, Thorough Caution for Low Confidence)
    *   `Test Suggestion / Edge Case Analysis` (Tester Agent, Thorough Caution for Low Confidence)
    *   `Flag for Human Review` (Explicit message to user)

**3. The ACQA Workflow (Step-by-Step)**

1.  **Request:** User -> Commander.
2.  **Initial Assessment (Commander):** Analyze request, determine `User Caution Level` (from config).
3.  **Task Assignment (Commander -> Developer):** Delegate task with prompt, context (files via `read_file`, specs), and `User Caution Level`.
4.  **--- Developer Agent Execution Cycle ---**
    *   a. **Understand Task:** Process prompt, context.
    *   b. **Delegate Sub-Task (If Needed):** Act as Coordinator for sub-task (Steps 2-6 recursively). Receive result (artifact + confidence) from sub-agent, analyze feedback (Step 5.e), initiate revisions if needed (Step 5.f), then continue main task.
    *   c. **Generate/Modify Artifact:** Perform core task.
    *   d. **Self-Assess Confidence:** Calculate `Confidence Score` (Low/Medium/High) + rationale.
    *   e. **Package Result:** Bundle artifact, score, rationale.
    *   f. **Report to Commander:** Send package.
5.  **--- QA Decision & Execution Cycle (Commander) ---**
    *   a. **Receive Result:** Get package from Developer Agent.
    *   b. **Decision Logic:** Evaluate `Confidence Score` vs. `User Caution Level` to determine required `QA Action(s)`.
    *   c. **Delegate QA Task (If Triggered):** Assign task to Reviewer/Tester Agent(s) via `new_task`. **Crucially, provide:** artifact, original prompt, **relevant specifications** (read via `read_file`), confidence score, required QA level/checklist.
    *   d. **Receive QA Feedback:** Get structured feedback from QA Agent(s).
    *   e. **Critically Analyze Feedback & Detect Patterns:**
        *   Commander evaluates feedback validity against specs. Is it actionable? Significant? Avoid trivial loops. (May use `second-opinion` if unsure about the feedback itself).
        *   **Pattern Detection:** Commander tracks QA feedback patterns across recent tasks. If the *same type* of specification deviation or error (e.g., incorrect filename convention, missing required TOML field, wrong path format) is flagged repeatedly (e.g., > 2-3 times), it indicates a potential systemic issue.
        *   **Escalation for Meta-Review:** If a recurring pattern is detected, Commander should **PAUSE** the revision cycle for that specific error type and **FLAG** the pattern to the user. Instead of just initiating another fix, Commander should propose a meta-review: "Recurring issue X detected, suggesting a potential problem with Spec Y or SOP Step Z. Recommend reviewing/updating the specification/SOP before proceeding with further fixes of this type." (See Adaptive Failure Resolution Process: `.ruru/planning/adaptive_failure_resolution_process.md`)
    *   f. **Initiate Revision (If Needed & No Pattern Escalation):** If changes are required *and* no recurring pattern triggered an escalation, create **Boomerang Task** or re-delegate to original Developer Agent with *specific* revision instructions based on analyzed feedback. Go back to Step 4.a for the Developer Agent.
6.  **--- Finalization (Commander) ---**
    *   g. **Integrate & Finalize:** Once QA passed/skipped and revisions complete, finalize artifact.
    *   h. **Report to User:** Use `attempt_completion`. Present result, optionally summarizing confidence and QA steps (e.g., "Generated artifact with Medium confidence, reviewed against Spec X by AI.").

**4. Implementation Considerations**

*   **Confidence Scoring:** Develop as a core capability/prompt pattern for Developer Agents. Start simple, iterate.
*   **Agent Prompting:** Update base prompts for Commander, Developer, Reviewer, Tester roles to explicitly include ACQA responsibilities (self-assessment, context provision, structured feedback).
*   **Configuration:** Define `.roo/project_config.toml` schema for `User Caution Level` and other preferences. Implement reading this config.
*   **Context Passing:** Emphasize Commander's role in using `read_file` to fetch specs/context and passing them explicitly during QA delegation.
*   **Boomerang Tasks:** Leverage for efficient revision cycles.
*   **Transparency:** Design UI/reporting to clearly show ACQA status.

**5. Benefits**

*   Proactive, adaptive quality gate.
*   Balances speed and assurance via user control.
*   Improves reliability of complex, delegated tasks.
*   Formalizes context-aware review and critical feedback analysis.
*   Creates traceable QA record.

**6. Next Steps (Conceptual)**

1.  Develop initial Confidence Scoring heuristics.
2.  Define standard Reviewer Agent checks for `Balanced` caution.
3.  Implement `User Caution Level` config reading.
4.  Update core Agent prompts for ACQA roles.
5.  Integrate basic ACQA loop (Confidence/Caution check -> QA Delegation with Context -> Feedback Analysis -> Revision Loop) into Commander logic.
6.  Test and iterate.

This refined ACQA proposal provides a robust framework for enhancing quality and reliability within the Roo Code multi-agent system.