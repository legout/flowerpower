{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Pipeline with FlowerPower\n",
    "\n",
    "**Execution:** `uvx --with \"flowerpower[rq],requests>=2.28.0,beautifulsoup4>=4.11.0,pandas>=2.0.0,matplotlib,seaborn\" jupyter lab`\n",
    "\n",
    "This notebook demonstrates web scraping using FlowerPower's JobQueueManager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volker/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-26 16:43:18,899\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê FlowerPower Web Scraping Pipeline\n",
      "üìÅ Project: web-scraping-pipeline\n",
      "üéØ Pipeline: news_scraper\n",
      "‚è∞ Scrape time: 2025-09-26 16:43:18\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Add FlowerPower source to path\n",
    "sys.path.insert(0, str(Path().absolute().parents[2] / \"src\"))\n",
    "\n",
    "from flowerpower.flowerpower import FlowerPowerProject\n",
    "\n",
    "# Initialize project\n",
    "project = FlowerPowerProject.load(\".\")\n",
    "\n",
    "print(\"üåê FlowerPower Web Scraping Pipeline\")\n",
    "print(f\"üìÅ Project: {project.pipeline_manager.project_cfg.name}\")\n",
    "print(f\"üéØ Pipeline: news_scraper\")\n",
    "print(f\"‚è∞ Scrape time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 16:43:20.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1müöÄ Running pipeline 'news_scraper' (attempt 1/4)\u001b[0m\n",
      "\u001b[32m2025-09-26 16:43:25.523\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m232\u001b[0m - \u001b[32m\u001b[1m‚úÖ Pipeline 'news_scraper' completed successfully in 4 seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ News scraping completed!\n",
      "üìÑ Articles saved to: /home/volker/coding/flowerpower/.worktree/code-simplification-analysis/examples/web-scraping-pipeline/output/articles_20250926_164325.json\n",
      "üìä Total articles: 0\n",
      "üåê Sources: 0\n",
      "üìà Average length: 0 chars\n"
     ]
    }
   ],
   "source": [
    "# Quick scraping execution\n",
    "result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\"scrape_timestamp\": datetime.now().isoformat()},\n",
    "    final_vars=[\"processed_articles\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ News scraping completed!\")\n",
    "if \"processed_articles\" in result:\n",
    "    info = result[\"processed_articles\"]\n",
    "    print(f\"üìÑ Articles saved to: {info['output_file']}\")\n",
    "    print(f\"üìä Total articles: {info['total_articles']}\")\n",
    "    print(f\"üåê Sources: {info['unique_sources']}\")\n",
    "    print(f\"üìà Average length: {info['average_content_length']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraped Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Data file not found: data/news_articles.csv\n",
      "üí° Run the scraping pipeline first to generate data\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze scraped news data\n",
    "data_file = \"data/news_articles.csv\"\n",
    "\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"üìä News Dataset Overview\")\n",
    "    print(f\"üìà Total articles: {len(df):,}\")\n",
    "    print(f\"üì∞ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    if 'published_date' in df.columns:\n",
    "        df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "        print(f\"üìÖ Date range: {df['published_date'].min()} to {df['published_date'].max()}\")\n",
    "    \n",
    "    # Display sample articles\n",
    "    print(\"\\nüîç Sample Articles:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nüìä Content Statistics:\")\n",
    "    if 'content' in df.columns:\n",
    "        df['content_length'] = df['content'].str.len()\n",
    "        print(f\"   ‚Ä¢ Average content length: {df['content_length'].mean():.0f} characters\")\n",
    "        print(f\"   ‚Ä¢ Longest article: {df['content_length'].max():,} characters\")\n",
    "        print(f\"   ‚Ä¢ Shortest article: {df['content_length'].min():,} characters\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        source_counts = df['source'].value_counts()\n",
    "        print(f\"\\nüåê Sources ({len(source_counts)} unique):\")\n",
    "        for source, count in source_counts.head(5).items():\n",
    "            print(f\"   ‚Ä¢ {source}: {count} articles\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Content length distribution\n",
    "    if 'content_length' in df.columns:\n",
    "        df['content_length'].hist(bins=30, ax=axes[0, 0], alpha=0.7)\n",
    "        axes[0, 0].set_title('Article Length Distribution')\n",
    "        axes[0, 0].set_xlabel('Content Length (characters)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Articles by source\n",
    "    if 'source' in df.columns:\n",
    "        top_sources = df['source'].value_counts().head(8)\n",
    "        top_sources.plot(kind='bar', ax=axes[0, 1], color='lightblue')\n",
    "        axes[0, 1].set_title('Articles by Source')\n",
    "        axes[0, 1].set_xlabel('Source')\n",
    "        axes[0, 1].set_ylabel('Article Count')\n",
    "    \n",
    "    # Articles by date\n",
    "    if 'published_date' in df.columns:\n",
    "        daily_counts = df.groupby(df['published_date'].dt.date).size()\n",
    "        axes[1, 0].plot(daily_counts.index, daily_counts.values, marker='o')\n",
    "        axes[1, 0].set_title('Articles Over Time')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Article Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Content length vs source\n",
    "    if 'content_length' in df.columns and 'source' in df.columns:\n",
    "        avg_length_by_source = df.groupby('source')['content_length'].mean().sort_values(ascending=False).head(8)\n",
    "        avg_length_by_source.plot(kind='barh', ax=axes[1, 1], color='lightgreen')\n",
    "        axes[1, 1].set_title('Average Content Length by Source')\n",
    "        axes[1, 1].set_xlabel('Average Content Length')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data file not found: {data_file}\")\n",
    "    print(\"üí° Run the scraping pipeline first to generate data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different scraping configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Quick Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 10,\n",
    "            \"request_delay\": 0.5,\n",
    "            \"timeout\": 10\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"request_delay\": 2.0,\n",
    "            \"timeout\": 30,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tech Focus\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_filter\": \"en\",\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "print(\"üß™ Running Scraping Experiments\")\n",
    "print(\"==============================\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\nüîÑ {exp['name']} experiment...\")\n",
    "    \n",
    "    # Add scrape timestamp to config\n",
    "    config = exp['config'].copy()\n",
    "    config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    try:\n",
    "        result = project.pipeline_manager.run(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"]\n",
    "        )\n",
    "        \n",
    "        if \"processed_articles\" in result:\n",
    "            info = result[\"processed_articles\"]\n",
    "            experiment_results.append({\n",
    "                \"name\": exp['name'],\n",
    "                \"total_articles\": info['total_articles'],\n",
    "                \"unique_sources\": info['unique_sources'],\n",
    "                \"avg_length\": info['average_content_length']\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Articles: {info['total_articles']}, Sources: {info['unique_sources']}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Experiment failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Compare experiment results\n",
    "if experiment_results:\n",
    "    print(\"\\nüìä Experiment Comparison\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Articles comparison\n",
    "    axes[0].bar(results_df['name'], results_df['total_articles'], color='skyblue')\n",
    "    axes[0].set_title('Articles Scraped')\n",
    "    axes[0].set_ylabel('Article Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sources comparison\n",
    "    axes[1].bar(results_df['name'], results_df['unique_sources'], color='lightgreen')\n",
    "    axes[1].set_title('Unique Sources')\n",
    "    axes[1].set_ylabel('Source Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average length comparison\n",
    "    axes[2].bar(results_df['name'], results_df['avg_length'], color='lightcoral')\n",
    "    axes[2].set_title('Average Content Length')\n",
    "    axes[2].set_ylabel('Characters')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best experiment\n",
    "    best_exp = results_df.loc[results_df['total_articles'].idxmax()]\n",
    "    print(f\"\\nüèÜ Most articles: {best_exp['name']} ({best_exp['total_articles']} articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive content analysis\n",
    "analysis_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"max_articles\": 100,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"language_detection\": True,\n",
    "        \"content_classification\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"content_analysis\",\n",
    "        \"keyword_summary\",\n",
    "        \"sentiment_summary\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üîç Content Analysis Results\")\n",
    "print(\"===========================\")\n",
    "\n",
    "# Processed articles info\n",
    "if \"processed_articles\" in analysis_result:\n",
    "    articles = analysis_result[\"processed_articles\"]\n",
    "    print(f\"\\nüìÑ Processed Articles:\")\n",
    "    print(f\"   ‚Ä¢ Total articles: {articles['total_articles']}\")\n",
    "    print(f\"   ‚Ä¢ Output file: {articles['output_file']}\")\n",
    "    print(f\"   ‚Ä¢ Processing time: {articles.get('processing_time', 'N/A')}s\")\n",
    "\n",
    "# Content analysis\n",
    "if \"content_analysis\" in analysis_result:\n",
    "    analysis = analysis_result[\"content_analysis\"]\n",
    "    print(f\"\\nüìä Content Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Total words: {analysis['total_words']:,}\")\n",
    "    print(f\"   ‚Ä¢ Unique words: {analysis['unique_words']:,}\")\n",
    "    print(f\"   ‚Ä¢ Average words per article: {analysis['avg_words_per_article']:.0f}\")\n",
    "    print(f\"   ‚Ä¢ Languages detected: {analysis['languages_detected']}\")\n",
    "\n",
    "# Keyword analysis\n",
    "if \"keyword_summary\" in analysis_result:\n",
    "    keywords = analysis_result[\"keyword_summary\"]\n",
    "    print(f\"\\nüîë Keyword Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Total keywords: {keywords['total_keywords']}\")\n",
    "    print(f\"   ‚Ä¢ Unique keywords: {keywords['unique_keywords']}\")\n",
    "    \n",
    "    # Show top keywords\n",
    "    if 'top_keywords' in keywords:\n",
    "        print(f\"   ‚Ä¢ Top 10 keywords:\")\n",
    "        for i, (keyword, count) in enumerate(keywords['top_keywords'][:10], 1):\n",
    "            print(f\"     {i:2d}. {keyword}: {count} occurrences\")\n",
    "\n",
    "# Sentiment analysis\n",
    "if \"sentiment_summary\" in analysis_result:\n",
    "    sentiment = analysis_result[\"sentiment_summary\"]\n",
    "    print(f\"\\nüòä Sentiment Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Average sentiment: {sentiment['average_sentiment']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Positive articles: {sentiment['positive_count']} ({sentiment['positive_percentage']:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Negative articles: {sentiment['negative_count']} ({sentiment['negative_percentage']:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Neutral articles: {sentiment['neutral_count']} ({sentiment['neutral_percentage']:.1f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "if all(key in analysis_result for key in [\"keyword_summary\", \"sentiment_summary\"]):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Top keywords\n",
    "    if 'top_keywords' in analysis_result[\"keyword_summary\"]:\n",
    "        top_keywords = analysis_result[\"keyword_summary\"][\"top_keywords\"][:15]\n",
    "        keyword_names = [k[0] for k in top_keywords]\n",
    "        keyword_counts = [k[1] for k in top_keywords]\n",
    "        \n",
    "        axes[0, 0].barh(range(len(keyword_names)), keyword_counts, color='lightblue')\n",
    "        axes[0, 0].set_yticks(range(len(keyword_names)))\n",
    "        axes[0, 0].set_yticklabels(keyword_names)\n",
    "        axes[0, 0].set_title('Top 15 Keywords')\n",
    "        axes[0, 0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_data = analysis_result[\"sentiment_summary\"]\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral']\n",
    "    sentiment_values = [sentiment_data['positive_percentage'], \n",
    "                       sentiment_data['negative_percentage'], \n",
    "                       sentiment_data['neutral_percentage']]\n",
    "    \n",
    "    colors = ['lightgreen', 'lightcoral', 'lightyellow']\n",
    "    axes[0, 1].pie(sentiment_values, labels=sentiment_labels, autopct='%1.1f%%', colors=colors)\n",
    "    axes[0, 1].set_title('Sentiment Distribution')\n",
    "    \n",
    "    # Sentiment over time (simulated)\n",
    "    if \"processed_articles\" in analysis_result:\n",
    "        # Simulate sentiment over time\n",
    "        dates = pd.date_range(start=datetime.now() - pd.Timedelta(days=7), periods=8, freq='D')\n",
    "        positive_sentiment = [0.6, 0.55, 0.65, 0.7, 0.62, 0.58, 0.64, 0.67]\n",
    "        negative_sentiment = [0.25, 0.3, 0.2, 0.15, 0.23, 0.27, 0.21, 0.18]\n",
    "        \n",
    "        axes[1, 0].plot(dates, positive_sentiment, 'g-', label='Positive', marker='o')\n",
    "        axes[1, 0].plot(dates, negative_sentiment, 'r-', label='Negative', marker='s')\n",
    "        axes[1, 0].set_title('Sentiment Trend (Last 7 Days)')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Sentiment Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Content categories (simulated)\n",
    "    categories = ['Technology', 'Business', 'Politics', 'Science', 'Health', 'Sports']\n",
    "    category_counts = [35, 28, 22, 18, 15, 12]\n",
    "    \n",
    "    axes[1, 1].pie(category_counts, labels=categories, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Content Categories')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Background Job Queue Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Background Scraping Jobs\")\n",
    "\n",
    "# Single background job\n",
    "print(\"\\nüì• Enqueueing single scraping job...\")\n",
    "try:\n",
    "    job = project.pipeline_manager.enqueue(\n",
    "        \"news_scraper\",\n",
    "        inputs={\n",
    "            \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "            \"max_concurrent_requests\": 8,\n",
    "            \"request_delay\": 1.0,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        },\n",
    "        final_vars=[\"processed_articles\"],\n",
    "        queue_name=\"scraping\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Job enqueued: {job.id}\")\n",
    "    print(f\"   üìã Queue: {job.origin}\")\n",
    "    print(f\"   ‚è∞ Enqueued at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Enqueue failed: {e}\")\n",
    "    print(\"   üí° Requires Redis for background processing\")\n",
    "\n",
    "# Batch scraping jobs\n",
    "print(\"\\nüì¶ Enqueueing batch scraping jobs...\")\n",
    "\n",
    "batch_configs = [\n",
    "    {\n",
    "        \"name\": \"tech_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 50,\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"business_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"max_articles\": 30,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"science_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"science\", \"research\", \"innovation\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_detection\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_jobs = []\n",
    "for batch in batch_configs:\n",
    "    print(f\"\\n   üîÑ {batch['name']} scraping...\")\n",
    "    \n",
    "    try:\n",
    "        config = batch['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.enqueue(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scrape_{batch['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        \n",
    "        batch_jobs.append((batch['name'], job))\n",
    "        print(f\"     ‚úÖ Enqueued: {job.id}\")\n",
    "        print(f\"     üìä Target: {config.get('max_articles', 'unlimited')} articles\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Failed: {e}\")\n",
    "\n",
    "if batch_jobs:\n",
    "    print(f\"\\nüéâ Successfully enqueued {len(batch_jobs)} batch jobs!\")\n",
    "    print(\"\\nüöÄ To process these jobs, start workers:\")\n",
    "    print(\"   flowerpower job-queue start-worker --queue-names scraping\")\n",
    "    \n",
    "    # Create batch jobs summary\n",
    "    batch_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Job Name\": name,\n",
    "            \"Job ID\": job.id,\n",
    "            \"Queue\": job.origin,\n",
    "            \"Status\": \"Queued\"\n",
    "        }\n",
    "        for name, job in batch_jobs\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìã Batch Jobs Summary:\")\n",
    "    display(batch_df)\n",
    "else:\n",
    "    print(\"\\nüí° No batch jobs enqueued - Redis required for job queuing\")\n",
    "\n",
    "print(f\"\\nüìä Job Queue Monitoring:\")\n",
    "print(f\"   ‚Ä¢ Queue name: scraping\")\n",
    "print(f\"   ‚Ä¢ Recommended workers: 2-4 concurrent workers\")\n",
    "print(f\"   ‚Ä¢ Estimated processing time: 5-15 minutes per job\")\n",
    "print(f\"   ‚Ä¢ Memory usage: ~100-500MB per worker\")\n",
    "print(f\"   ‚Ä¢ Rate limiting: Built-in delays to respect website policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scheduled Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scheduled scraping jobs\n",
    "print(\"üìÖ Scheduled Data Collection\")\n",
    "\n",
    "# Define scraping schedules\n",
    "schedules = [\n",
    "    {\n",
    "        \"name\": \"Hourly Breaking News\",\n",
    "        \"cron\": \"0 * * * *\",  # Every hour\n",
    "        \"description\": \"Quick scan for breaking news\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 20,\n",
    "            \"categories\": [\"breaking\", \"urgent\"],\n",
    "            \"priority\": \"high\",\n",
    "            \"quick_mode\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Daily Tech News\",\n",
    "        \"cron\": \"0 8 * * *\",  # Daily at 8 AM\n",
    "        \"description\": \"Comprehensive technology news collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 100,\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Weekly Deep Dive\",\n",
    "        \"cron\": \"0 9 * * 1\",  # Weekly on Monday at 9 AM\n",
    "        \"description\": \"Comprehensive multi-category collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 500,\n",
    "            \"categories\": [\"technology\", \"business\", \"science\", \"health\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"language_detection\": True,\n",
    "            \"content_classification\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Market Opening Scan\",\n",
    "        \"cron\": \"30 9 * * 1-5\",  # Weekdays at 9:30 AM\n",
    "        \"description\": \"Business and market news before trading\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"priority\": \"high\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "scheduled_jobs = []\n",
    "\n",
    "for schedule in schedules:\n",
    "    print(f\"\\nüìã {schedule['name']}\")\n",
    "    print(f\"   ‚è∞ Schedule: {schedule['description']}\")\n",
    "    print(f\"   üîß Cron: {schedule['cron']}\")\n",
    "    print(f\"   üìä Target articles: {schedule['config'].get('max_articles', 'unlimited')}\")\n",
    "    \n",
    "    try:\n",
    "        # Add scrape timestamp to config\n",
    "        config = schedule['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.schedule(\n",
    "            \"news_scraper\",\n",
    "            cron=schedule['cron'],\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scheduled_{schedule['name'].lower().replace(' ', '_')}\"\n",
    "        )\n",
    "        \n",
    "        scheduled_jobs.append((schedule['name'], job, schedule['description']))\n",
    "        print(f\"   ‚úÖ Scheduled successfully - Job ID: {job.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Scheduling failed: {e}\")\n",
    "        print(\"   üí° Requires Redis for job scheduling\")\n",
    "\n",
    "if scheduled_jobs:\n",
    "    print(f\"\\nüéâ Successfully scheduled {len(scheduled_jobs)} scraping jobs!\")\n",
    "    print(\"\\nüöÄ To process scheduled jobs, start a worker with scheduler:\")\n",
    "    print(\"   flowerpower job-queue start-worker --with-scheduler\")\n",
    "    \n",
    "    # Create schedule visualization\n",
    "    schedule_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Schedule Name\": name,\n",
    "            \"Description\": desc,\n",
    "            \"Cron Expression\": s[\"cron\"],\n",
    "            \"Max Articles\": s[\"config\"].get(\"max_articles\", \"unlimited\")\n",
    "        }\n",
    "        for (name, job, desc), s in zip(scheduled_jobs, schedules)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìÖ Scheduled Jobs Summary:\")\n",
    "    display(schedule_df)\n",
    "    \n",
    "    # Visualize schedule frequency\n",
    "    schedule_types = ['Hourly', 'Daily', 'Weekly', 'Weekdays']\n",
    "    frequencies = [24, 1, 1/7, 5]  # executions per day\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(schedule_types, frequencies, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    plt.title('Scheduled Scraping Frequency')\n",
    "    plt.xlabel('Schedule Type')\n",
    "    plt.ylabel('Executions per Day')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(frequencies):\n",
    "        plt.text(i, v, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nüí° No schedules created - Redis required for scheduling functionality\")\n",
    "\n",
    "# Data collection estimates\n",
    "print(f\"\\nüìà Data Collection Estimates:\")\n",
    "print(f\"   ‚Ä¢ Hourly: ~20 articles = 480 articles/day\")\n",
    "print(f\"   ‚Ä¢ Daily: ~100 articles = 100 articles/day\")\n",
    "print(f\"   ‚Ä¢ Weekly: ~500 articles = 71 articles/day\")\n",
    "print(f\"   ‚Ä¢ Weekdays: ~50 articles = 250 articles/day\")\n",
    "print(f\"   ‚Ä¢ Total estimated: ~900 articles/day\")\n",
    "print(f\"   ‚Ä¢ Monthly volume: ~27,000 articles\")\n",
    "print(f\"   ‚Ä¢ Storage needed: ~50-100GB/month (with content)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data export\n",
    "print(\"üì§ Data Export and Integration\")\n",
    "\n",
    "# Run scraping with comprehensive export options\n",
    "export_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"export_formats\": [\"csv\", \"json\", \"parquet\"],\n",
    "        \"include_metadata\": True,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"compress_output\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"export_summary\",\n",
    "        \"data_quality_report\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if \"processed_articles\" in export_result:\n",
    "    articles = export_result[\"processed_articles\"]\n",
    "    print(f\"\\nüìä Export Results:\")\n",
    "    print(f\"   ‚Ä¢ Articles exported: {articles['total_articles']}\")\n",
    "    print(f\"   ‚Ä¢ Output file: {articles['output_file']}\")\n",
    "    print(f\"   ‚Ä¢ File size: {articles.get('file_size_mb', 'N/A')} MB\")\n",
    "    print(f\"   ‚Ä¢ Compression ratio: {articles.get('compression_ratio', 'N/A')}\")\n",
    "\n",
    "if \"export_summary\" in export_result:\n",
    "    summary = export_result[\"export_summary\"]\n",
    "    print(f\"\\nüìã Export Summary:\")\n",
    "    print(f\"   ‚Ä¢ Formats generated: {summary['formats_created']}\")\n",
    "    print(f\"   ‚Ä¢ Total files: {summary['total_files']}\")\n",
    "    print(f\"   ‚Ä¢ Total size: {summary['total_size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Show file details\n",
    "    if 'file_details' in summary:\n",
    "        print(f\"\\nüìÅ Generated Files:\")\n",
    "        for file_info in summary['file_details']:\n",
    "            print(f\"   ‚Ä¢ {file_info['format']}: {file_info['filename']} ({file_info['size_mb']:.1f} MB)\")\n",
    "\n",
    "if \"data_quality_report\" in export_result:\n",
    "    quality = export_result[\"data_quality_report\"]\n",
    "    print(f\"\\n‚úÖ Data Quality Report:\")\n",
    "    print(f\"   ‚Ä¢ Completeness score: {quality['completeness_score']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Accuracy score: {quality['accuracy_score']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Duplicate articles: {quality['duplicate_count']}\")\n",
    "    print(f\"   ‚Ä¢ Missing content: {quality['missing_content_count']}\")\n",
    "    print(f\"   ‚Ä¢ Quality grade: {quality['overall_grade']}\")\n",
    "\n",
    "# Create sample analysis of exported data\n",
    "print(f\"\\nüîç Sample Data Analysis\")\n",
    "\n",
    "# Simulate analysis of exported data\n",
    "sample_data = {\n",
    "    'articles': [\n",
    "        {\n",
    "            'title': 'AI Revolution in Healthcare',\n",
    "            'source': 'TechNews',\n",
    "            'sentiment': 0.7,\n",
    "            'keywords': ['AI', 'healthcare', 'innovation'],\n",
    "            'word_count': 850\n",
    "        },\n",
    "        {\n",
    "            'title': 'Market Volatility Continues',\n",
    "            'source': 'FinanceDaily',\n",
    "            'sentiment': -0.3,\n",
    "            'keywords': ['market', 'volatility', 'economy'],\n",
    "            'word_count': 650\n",
    "        },\n",
    "        {\n",
    "            'title': 'Breakthrough in Quantum Computing',\n",
    "            'source': 'ScienceToday',\n",
    "            'sentiment': 0.8,\n",
    "            'keywords': ['quantum', 'computing', 'breakthrough'],\n",
    "            'word_count': 1200\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "sample_df = pd.DataFrame(sample_data['articles'])\n",
    "\n",
    "print(\"\\nüìä Sample Exported Data:\")\n",
    "display(sample_df)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiments = sample_df['sentiment']\n",
    "axes[0, 0].hist(sentiments, bins=10, alpha=0.7, color='lightblue')\n",
    "axes[0, 0].set_title('Sentiment Distribution')\n",
    "axes[0, 0].set_xlabel('Sentiment Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Word count distribution\n",
    "word_counts = sample_df['word_count']\n",
    "axes[0, 1].hist(word_counts, bins=8, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Articles by source\n",
    "source_counts = sample_df['source'].value_counts()\n",
    "axes[1, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Articles by Source')\n",
    "\n",
    "# Keyword frequency (flattened)\n",
    "all_keywords = [kw for keywords in sample_df['keywords'] for kw in keywords]\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = dict(keyword_counts.most_common(6))\n",
    "\n",
    "axes[1, 1].bar(top_keywords.keys(), top_keywords.values(), color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('Top Keywords')\n",
    "axes[1, 1].set_xlabel('Keywords')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save export summary\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "export_summary_data = {\n",
    "    \"export_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_articles\": export_result.get('processed_articles', {}).get('total_articles', 0),\n",
    "    \"formats_exported\": ['csv', 'json', 'parquet'],\n",
    "    \"data_quality_score\": export_result.get('data_quality_report', {}).get('completeness_score', 0)\n",
    "}\n",
    "\n",
    "summary_file = f\"outputs/scraping_export_summary_{timestamp}.json\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(export_summary_data, f, indent=2)\n",
    "    print(f\"\\nüíæ Export summary saved: {summary_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not save export summary: {e}\")\n",
    "\n",
    "print(f\"\\nüîó Integration Options:\")\n",
    "print(f\"   ‚Ä¢ Database: Load into PostgreSQL, MySQL, or MongoDB\")\n",
    "print(f\"   ‚Ä¢ Analytics: Import into Tableau, Power BI, or Jupyter\")\n",
    "print(f\"   ‚Ä¢ Search: Index in Elasticsearch or Solr\")\n",
    "print(f\"   ‚Ä¢ API: Serve via REST API or GraphQL\")\n",
    "print(f\"   ‚Ä¢ ML Pipeline: Feed into machine learning models\")\n",
    "print(f\"   ‚Ä¢ Alerting: Set up keyword-based notifications\")\n",
    "\n",
    "print(f\"\\nüéâ Web scraping pipeline completed successfully!\")\n",
    "print(f\"üì∞ Data ready for analysis and downstream processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowerpower (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
