{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Pipeline with FlowerPower\n",
    "\n",
    "**Execution:** `uvx --with \"flowerpower[rq],requests>=2.28.0,beautifulsoup4>=4.11.0,pandas>=2.0.0,matplotlib,seaborn\" jupyter lab`\n",
    "\n",
    "This notebook demonstrates web scraping using FlowerPower's JobQueueManager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volker/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-26 16:43:18,899\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 FlowerPower Web Scraping Pipeline\n",
      "📁 Project: web-scraping-pipeline\n",
      "🎯 Pipeline: news_scraper\n",
      "⏰ Scrape time: 2025-09-26 16:43:18\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Add FlowerPower source to path\n",
    "sys.path.insert(0, str(Path().absolute().parents[2] / \"src\"))\n",
    "\n",
    "from flowerpower.flowerpower import FlowerPowerProject\n",
    "\n",
    "# Initialize project\n",
    "project = FlowerPowerProject.load(\".\")\n",
    "\n",
    "print(\"🌐 FlowerPower Web Scraping Pipeline\")\n",
    "print(f\"📁 Project: {project.pipeline_manager.project_cfg.name}\")\n",
    "print(f\"🎯 Pipeline: news_scraper\")\n",
    "print(f\"⏰ Scrape time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 16:43:20.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1m🚀 Running pipeline 'news_scraper' (attempt 1/4)\u001b[0m\n",
      "\u001b[32m2025-09-26 16:43:25.523\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m232\u001b[0m - \u001b[32m\u001b[1m✅ Pipeline 'news_scraper' completed successfully in 4 seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ News scraping completed!\n",
      "📄 Articles saved to: /home/volker/coding/flowerpower/.worktree/code-simplification-analysis/examples/web-scraping-pipeline/output/articles_20250926_164325.json\n",
      "📊 Total articles: 0\n",
      "🌐 Sources: 0\n",
      "📈 Average length: 0 chars\n"
     ]
    }
   ],
   "source": [
    "# Quick scraping execution\n",
    "result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\"scrape_timestamp\": datetime.now().isoformat()},\n",
    "    final_vars=[\"processed_articles\"]\n",
    ")\n",
    "\n",
    "print(\"✅ News scraping completed!\")\n",
    "if \"processed_articles\" in result:\n",
    "    info = result[\"processed_articles\"]\n",
    "    print(f\"📄 Articles saved to: {info['output_file']}\")\n",
    "    print(f\"📊 Total articles: {info['total_articles']}\")\n",
    "    print(f\"🌐 Sources: {info['unique_sources']}\")\n",
    "    print(f\"📈 Average length: {info['average_content_length']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraped Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Data file not found: data/news_articles.csv\n",
      "💡 Run the scraping pipeline first to generate data\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze scraped news data\n",
    "data_file = \"data/news_articles.csv\"\n",
    "\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"📊 News Dataset Overview\")\n",
    "    print(f\"📈 Total articles: {len(df):,}\")\n",
    "    print(f\"📰 Columns: {list(df.columns)}\")\n",
    "    \n",
    "    if 'published_date' in df.columns:\n",
    "        df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "        print(f\"📅 Date range: {df['published_date'].min()} to {df['published_date'].max()}\")\n",
    "    \n",
    "    # Display sample articles\n",
    "    print(\"\\n🔍 Sample Articles:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n📊 Content Statistics:\")\n",
    "    if 'content' in df.columns:\n",
    "        df['content_length'] = df['content'].str.len()\n",
    "        print(f\"   • Average content length: {df['content_length'].mean():.0f} characters\")\n",
    "        print(f\"   • Longest article: {df['content_length'].max():,} characters\")\n",
    "        print(f\"   • Shortest article: {df['content_length'].min():,} characters\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        source_counts = df['source'].value_counts()\n",
    "        print(f\"\\n🌐 Sources ({len(source_counts)} unique):\")\n",
    "        for source, count in source_counts.head(5).items():\n",
    "            print(f\"   • {source}: {count} articles\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Content length distribution\n",
    "    if 'content_length' in df.columns:\n",
    "        df['content_length'].hist(bins=30, ax=axes[0, 0], alpha=0.7)\n",
    "        axes[0, 0].set_title('Article Length Distribution')\n",
    "        axes[0, 0].set_xlabel('Content Length (characters)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Articles by source\n",
    "    if 'source' in df.columns:\n",
    "        top_sources = df['source'].value_counts().head(8)\n",
    "        top_sources.plot(kind='bar', ax=axes[0, 1], color='lightblue')\n",
    "        axes[0, 1].set_title('Articles by Source')\n",
    "        axes[0, 1].set_xlabel('Source')\n",
    "        axes[0, 1].set_ylabel('Article Count')\n",
    "    \n",
    "    # Articles by date\n",
    "    if 'published_date' in df.columns:\n",
    "        daily_counts = df.groupby(df['published_date'].dt.date).size()\n",
    "        axes[1, 0].plot(daily_counts.index, daily_counts.values, marker='o')\n",
    "        axes[1, 0].set_title('Articles Over Time')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Article Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Content length vs source\n",
    "    if 'content_length' in df.columns and 'source' in df.columns:\n",
    "        avg_length_by_source = df.groupby('source')['content_length'].mean().sort_values(ascending=False).head(8)\n",
    "        avg_length_by_source.plot(kind='barh', ax=axes[1, 1], color='lightgreen')\n",
    "        axes[1, 1].set_title('Average Content Length by Source')\n",
    "        axes[1, 1].set_xlabel('Average Content Length')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"⚠️ Data file not found: {data_file}\")\n",
    "    print(\"💡 Run the scraping pipeline first to generate data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different scraping configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Quick Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 10,\n",
    "            \"request_delay\": 0.5,\n",
    "            \"timeout\": 10\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"request_delay\": 2.0,\n",
    "            \"timeout\": 30,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tech Focus\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_filter\": \"en\",\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "print(\"🧪 Running Scraping Experiments\")\n",
    "print(\"==============================\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n🔄 {exp['name']} experiment...\")\n",
    "    \n",
    "    # Add scrape timestamp to config\n",
    "    config = exp['config'].copy()\n",
    "    config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    try:\n",
    "        result = project.pipeline_manager.run(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"]\n",
    "        )\n",
    "        \n",
    "        if \"processed_articles\" in result:\n",
    "            info = result[\"processed_articles\"]\n",
    "            experiment_results.append({\n",
    "                \"name\": exp['name'],\n",
    "                \"total_articles\": info['total_articles'],\n",
    "                \"unique_sources\": info['unique_sources'],\n",
    "                \"avg_length\": info['average_content_length']\n",
    "            })\n",
    "            \n",
    "            print(f\"   ✅ Articles: {info['total_articles']}, Sources: {info['unique_sources']}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Experiment failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "# Compare experiment results\n",
    "if experiment_results:\n",
    "    print(\"\\n📊 Experiment Comparison\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Articles comparison\n",
    "    axes[0].bar(results_df['name'], results_df['total_articles'], color='skyblue')\n",
    "    axes[0].set_title('Articles Scraped')\n",
    "    axes[0].set_ylabel('Article Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sources comparison\n",
    "    axes[1].bar(results_df['name'], results_df['unique_sources'], color='lightgreen')\n",
    "    axes[1].set_title('Unique Sources')\n",
    "    axes[1].set_ylabel('Source Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average length comparison\n",
    "    axes[2].bar(results_df['name'], results_df['avg_length'], color='lightcoral')\n",
    "    axes[2].set_title('Average Content Length')\n",
    "    axes[2].set_ylabel('Characters')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best experiment\n",
    "    best_exp = results_df.loc[results_df['total_articles'].idxmax()]\n",
    "    print(f\"\\n🏆 Most articles: {best_exp['name']} ({best_exp['total_articles']} articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive content analysis\n",
    "analysis_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"max_articles\": 100,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"language_detection\": True,\n",
    "        \"content_classification\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"content_analysis\",\n",
    "        \"keyword_summary\",\n",
    "        \"sentiment_summary\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"🔍 Content Analysis Results\")\n",
    "print(\"===========================\")\n",
    "\n",
    "# Processed articles info\n",
    "if \"processed_articles\" in analysis_result:\n",
    "    articles = analysis_result[\"processed_articles\"]\n",
    "    print(f\"\\n📄 Processed Articles:\")\n",
    "    print(f\"   • Total articles: {articles['total_articles']}\")\n",
    "    print(f\"   • Output file: {articles['output_file']}\")\n",
    "    print(f\"   • Processing time: {articles.get('processing_time', 'N/A')}s\")\n",
    "\n",
    "# Content analysis\n",
    "if \"content_analysis\" in analysis_result:\n",
    "    analysis = analysis_result[\"content_analysis\"]\n",
    "    print(f\"\\n📊 Content Analysis:\")\n",
    "    print(f\"   • Total words: {analysis['total_words']:,}\")\n",
    "    print(f\"   • Unique words: {analysis['unique_words']:,}\")\n",
    "    print(f\"   • Average words per article: {analysis['avg_words_per_article']:.0f}\")\n",
    "    print(f\"   • Languages detected: {analysis['languages_detected']}\")\n",
    "\n",
    "# Keyword analysis\n",
    "if \"keyword_summary\" in analysis_result:\n",
    "    keywords = analysis_result[\"keyword_summary\"]\n",
    "    print(f\"\\n🔑 Keyword Analysis:\")\n",
    "    print(f\"   • Total keywords: {keywords['total_keywords']}\")\n",
    "    print(f\"   • Unique keywords: {keywords['unique_keywords']}\")\n",
    "    \n",
    "    # Show top keywords\n",
    "    if 'top_keywords' in keywords:\n",
    "        print(f\"   • Top 10 keywords:\")\n",
    "        for i, (keyword, count) in enumerate(keywords['top_keywords'][:10], 1):\n",
    "            print(f\"     {i:2d}. {keyword}: {count} occurrences\")\n",
    "\n",
    "# Sentiment analysis\n",
    "if \"sentiment_summary\" in analysis_result:\n",
    "    sentiment = analysis_result[\"sentiment_summary\"]\n",
    "    print(f\"\\n😊 Sentiment Analysis:\")\n",
    "    print(f\"   • Average sentiment: {sentiment['average_sentiment']:.3f}\")\n",
    "    print(f\"   • Positive articles: {sentiment['positive_count']} ({sentiment['positive_percentage']:.1f}%)\")\n",
    "    print(f\"   • Negative articles: {sentiment['negative_count']} ({sentiment['negative_percentage']:.1f}%)\")\n",
    "    print(f\"   • Neutral articles: {sentiment['neutral_count']} ({sentiment['neutral_percentage']:.1f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "if all(key in analysis_result for key in [\"keyword_summary\", \"sentiment_summary\"]):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Top keywords\n",
    "    if 'top_keywords' in analysis_result[\"keyword_summary\"]:\n",
    "        top_keywords = analysis_result[\"keyword_summary\"][\"top_keywords\"][:15]\n",
    "        keyword_names = [k[0] for k in top_keywords]\n",
    "        keyword_counts = [k[1] for k in top_keywords]\n",
    "        \n",
    "        axes[0, 0].barh(range(len(keyword_names)), keyword_counts, color='lightblue')\n",
    "        axes[0, 0].set_yticks(range(len(keyword_names)))\n",
    "        axes[0, 0].set_yticklabels(keyword_names)\n",
    "        axes[0, 0].set_title('Top 15 Keywords')\n",
    "        axes[0, 0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_data = analysis_result[\"sentiment_summary\"]\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral']\n",
    "    sentiment_values = [sentiment_data['positive_percentage'], \n",
    "                       sentiment_data['negative_percentage'], \n",
    "                       sentiment_data['neutral_percentage']]\n",
    "    \n",
    "    colors = ['lightgreen', 'lightcoral', 'lightyellow']\n",
    "    axes[0, 1].pie(sentiment_values, labels=sentiment_labels, autopct='%1.1f%%', colors=colors)\n",
    "    axes[0, 1].set_title('Sentiment Distribution')\n",
    "    \n",
    "    # Sentiment over time (simulated)\n",
    "    if \"processed_articles\" in analysis_result:\n",
    "        # Simulate sentiment over time\n",
    "        dates = pd.date_range(start=datetime.now() - pd.Timedelta(days=7), periods=8, freq='D')\n",
    "        positive_sentiment = [0.6, 0.55, 0.65, 0.7, 0.62, 0.58, 0.64, 0.67]\n",
    "        negative_sentiment = [0.25, 0.3, 0.2, 0.15, 0.23, 0.27, 0.21, 0.18]\n",
    "        \n",
    "        axes[1, 0].plot(dates, positive_sentiment, 'g-', label='Positive', marker='o')\n",
    "        axes[1, 0].plot(dates, negative_sentiment, 'r-', label='Negative', marker='s')\n",
    "        axes[1, 0].set_title('Sentiment Trend (Last 7 Days)')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Sentiment Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Content categories (simulated)\n",
    "    categories = ['Technology', 'Business', 'Politics', 'Science', 'Health', 'Sports']\n",
    "    category_counts = [35, 28, 22, 18, 15, 12]\n",
    "    \n",
    "    axes[1, 1].pie(category_counts, labels=categories, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Content Categories')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Insufficient data for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Background Job Queue Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Background Scraping Jobs\")\n",
    "\n",
    "# Single background job\n",
    "print(\"\\n📥 Enqueueing single scraping job...\")\n",
    "try:\n",
    "    job = project.pipeline_manager.enqueue(\n",
    "        \"news_scraper\",\n",
    "        inputs={\n",
    "            \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "            \"max_concurrent_requests\": 8,\n",
    "            \"request_delay\": 1.0,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        },\n",
    "        final_vars=[\"processed_articles\"],\n",
    "        queue_name=\"scraping\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✅ Job enqueued: {job.id}\")\n",
    "    print(f\"   📋 Queue: {job.origin}\")\n",
    "    print(f\"   ⏰ Enqueued at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Enqueue failed: {e}\")\n",
    "    print(\"   💡 Requires Redis for background processing\")\n",
    "\n",
    "# Batch scraping jobs\n",
    "print(\"\\n📦 Enqueueing batch scraping jobs...\")\n",
    "\n",
    "batch_configs = [\n",
    "    {\n",
    "        \"name\": \"tech_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 50,\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"business_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"max_articles\": 30,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"science_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"science\", \"research\", \"innovation\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_detection\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_jobs = []\n",
    "for batch in batch_configs:\n",
    "    print(f\"\\n   🔄 {batch['name']} scraping...\")\n",
    "    \n",
    "    try:\n",
    "        config = batch['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.enqueue(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scrape_{batch['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        \n",
    "        batch_jobs.append((batch['name'], job))\n",
    "        print(f\"     ✅ Enqueued: {job.id}\")\n",
    "        print(f\"     📊 Target: {config.get('max_articles', 'unlimited')} articles\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ❌ Failed: {e}\")\n",
    "\n",
    "if batch_jobs:\n",
    "    print(f\"\\n🎉 Successfully enqueued {len(batch_jobs)} batch jobs!\")\n",
    "    print(\"\\n🚀 To process these jobs, start workers:\")\n",
    "    print(\"   flowerpower job-queue start-worker --queue-names scraping\")\n",
    "    \n",
    "    # Create batch jobs summary\n",
    "    batch_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Job Name\": name,\n",
    "            \"Job ID\": job.id,\n",
    "            \"Queue\": job.origin,\n",
    "            \"Status\": \"Queued\"\n",
    "        }\n",
    "        for name, job in batch_jobs\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📋 Batch Jobs Summary:\")\n",
    "    display(batch_df)\n",
    "else:\n",
    "    print(\"\\n💡 No batch jobs enqueued - Redis required for job queuing\")\n",
    "\n",
    "print(f\"\\n📊 Job Queue Monitoring:\")\n",
    "print(f\"   • Queue name: scraping\")\n",
    "print(f\"   • Recommended workers: 2-4 concurrent workers\")\n",
    "print(f\"   • Estimated processing time: 5-15 minutes per job\")\n",
    "print(f\"   • Memory usage: ~100-500MB per worker\")\n",
    "print(f\"   • Rate limiting: Built-in delays to respect website policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scheduled Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scheduled scraping jobs\n",
    "print(\"📅 Scheduled Data Collection\")\n",
    "\n",
    "# Define scraping schedules\n",
    "schedules = [\n",
    "    {\n",
    "        \"name\": \"Hourly Breaking News\",\n",
    "        \"cron\": \"0 * * * *\",  # Every hour\n",
    "        \"description\": \"Quick scan for breaking news\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 20,\n",
    "            \"categories\": [\"breaking\", \"urgent\"],\n",
    "            \"priority\": \"high\",\n",
    "            \"quick_mode\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Daily Tech News\",\n",
    "        \"cron\": \"0 8 * * *\",  # Daily at 8 AM\n",
    "        \"description\": \"Comprehensive technology news collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 100,\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Weekly Deep Dive\",\n",
    "        \"cron\": \"0 9 * * 1\",  # Weekly on Monday at 9 AM\n",
    "        \"description\": \"Comprehensive multi-category collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 500,\n",
    "            \"categories\": [\"technology\", \"business\", \"science\", \"health\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"language_detection\": True,\n",
    "            \"content_classification\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Market Opening Scan\",\n",
    "        \"cron\": \"30 9 * * 1-5\",  # Weekdays at 9:30 AM\n",
    "        \"description\": \"Business and market news before trading\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"priority\": \"high\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "scheduled_jobs = []\n",
    "\n",
    "for schedule in schedules:\n",
    "    print(f\"\\n📋 {schedule['name']}\")\n",
    "    print(f\"   ⏰ Schedule: {schedule['description']}\")\n",
    "    print(f\"   🔧 Cron: {schedule['cron']}\")\n",
    "    print(f\"   📊 Target articles: {schedule['config'].get('max_articles', 'unlimited')}\")\n",
    "    \n",
    "    try:\n",
    "        # Add scrape timestamp to config\n",
    "        config = schedule['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.schedule(\n",
    "            \"news_scraper\",\n",
    "            cron=schedule['cron'],\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scheduled_{schedule['name'].lower().replace(' ', '_')}\"\n",
    "        )\n",
    "        \n",
    "        scheduled_jobs.append((schedule['name'], job, schedule['description']))\n",
    "        print(f\"   ✅ Scheduled successfully - Job ID: {job.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Scheduling failed: {e}\")\n",
    "        print(\"   💡 Requires Redis for job scheduling\")\n",
    "\n",
    "if scheduled_jobs:\n",
    "    print(f\"\\n🎉 Successfully scheduled {len(scheduled_jobs)} scraping jobs!\")\n",
    "    print(\"\\n🚀 To process scheduled jobs, start a worker with scheduler:\")\n",
    "    print(\"   flowerpower job-queue start-worker --with-scheduler\")\n",
    "    \n",
    "    # Create schedule visualization\n",
    "    schedule_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Schedule Name\": name,\n",
    "            \"Description\": desc,\n",
    "            \"Cron Expression\": s[\"cron\"],\n",
    "            \"Max Articles\": s[\"config\"].get(\"max_articles\", \"unlimited\")\n",
    "        }\n",
    "        for (name, job, desc), s in zip(scheduled_jobs, schedules)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📅 Scheduled Jobs Summary:\")\n",
    "    display(schedule_df)\n",
    "    \n",
    "    # Visualize schedule frequency\n",
    "    schedule_types = ['Hourly', 'Daily', 'Weekly', 'Weekdays']\n",
    "    frequencies = [24, 1, 1/7, 5]  # executions per day\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(schedule_types, frequencies, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    plt.title('Scheduled Scraping Frequency')\n",
    "    plt.xlabel('Schedule Type')\n",
    "    plt.ylabel('Executions per Day')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(frequencies):\n",
    "        plt.text(i, v, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n💡 No schedules created - Redis required for scheduling functionality\")\n",
    "\n",
    "# Data collection estimates\n",
    "print(f\"\\n📈 Data Collection Estimates:\")\n",
    "print(f\"   • Hourly: ~20 articles = 480 articles/day\")\n",
    "print(f\"   • Daily: ~100 articles = 100 articles/day\")\n",
    "print(f\"   • Weekly: ~500 articles = 71 articles/day\")\n",
    "print(f\"   • Weekdays: ~50 articles = 250 articles/day\")\n",
    "print(f\"   • Total estimated: ~900 articles/day\")\n",
    "print(f\"   • Monthly volume: ~27,000 articles\")\n",
    "print(f\"   • Storage needed: ~50-100GB/month (with content)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data export\n",
    "print(\"📤 Data Export and Integration\")\n",
    "\n",
    "# Run scraping with comprehensive export options\n",
    "export_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"export_formats\": [\"csv\", \"json\", \"parquet\"],\n",
    "        \"include_metadata\": True,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"compress_output\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"export_summary\",\n",
    "        \"data_quality_report\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if \"processed_articles\" in export_result:\n",
    "    articles = export_result[\"processed_articles\"]\n",
    "    print(f\"\\n📊 Export Results:\")\n",
    "    print(f\"   • Articles exported: {articles['total_articles']}\")\n",
    "    print(f\"   • Output file: {articles['output_file']}\")\n",
    "    print(f\"   • File size: {articles.get('file_size_mb', 'N/A')} MB\")\n",
    "    print(f\"   • Compression ratio: {articles.get('compression_ratio', 'N/A')}\")\n",
    "\n",
    "if \"export_summary\" in export_result:\n",
    "    summary = export_result[\"export_summary\"]\n",
    "    print(f\"\\n📋 Export Summary:\")\n",
    "    print(f\"   • Formats generated: {summary['formats_created']}\")\n",
    "    print(f\"   • Total files: {summary['total_files']}\")\n",
    "    print(f\"   • Total size: {summary['total_size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Show file details\n",
    "    if 'file_details' in summary:\n",
    "        print(f\"\\n📁 Generated Files:\")\n",
    "        for file_info in summary['file_details']:\n",
    "            print(f\"   • {file_info['format']}: {file_info['filename']} ({file_info['size_mb']:.1f} MB)\")\n",
    "\n",
    "if \"data_quality_report\" in export_result:\n",
    "    quality = export_result[\"data_quality_report\"]\n",
    "    print(f\"\\n✅ Data Quality Report:\")\n",
    "    print(f\"   • Completeness score: {quality['completeness_score']:.1f}%\")\n",
    "    print(f\"   • Accuracy score: {quality['accuracy_score']:.1f}%\")\n",
    "    print(f\"   • Duplicate articles: {quality['duplicate_count']}\")\n",
    "    print(f\"   • Missing content: {quality['missing_content_count']}\")\n",
    "    print(f\"   • Quality grade: {quality['overall_grade']}\")\n",
    "\n",
    "# Create sample analysis of exported data\n",
    "print(f\"\\n🔍 Sample Data Analysis\")\n",
    "\n",
    "# Simulate analysis of exported data\n",
    "sample_data = {\n",
    "    'articles': [\n",
    "        {\n",
    "            'title': 'AI Revolution in Healthcare',\n",
    "            'source': 'TechNews',\n",
    "            'sentiment': 0.7,\n",
    "            'keywords': ['AI', 'healthcare', 'innovation'],\n",
    "            'word_count': 850\n",
    "        },\n",
    "        {\n",
    "            'title': 'Market Volatility Continues',\n",
    "            'source': 'FinanceDaily',\n",
    "            'sentiment': -0.3,\n",
    "            'keywords': ['market', 'volatility', 'economy'],\n",
    "            'word_count': 650\n",
    "        },\n",
    "        {\n",
    "            'title': 'Breakthrough in Quantum Computing',\n",
    "            'source': 'ScienceToday',\n",
    "            'sentiment': 0.8,\n",
    "            'keywords': ['quantum', 'computing', 'breakthrough'],\n",
    "            'word_count': 1200\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "sample_df = pd.DataFrame(sample_data['articles'])\n",
    "\n",
    "print(\"\\n📊 Sample Exported Data:\")\n",
    "display(sample_df)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiments = sample_df['sentiment']\n",
    "axes[0, 0].hist(sentiments, bins=10, alpha=0.7, color='lightblue')\n",
    "axes[0, 0].set_title('Sentiment Distribution')\n",
    "axes[0, 0].set_xlabel('Sentiment Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Word count distribution\n",
    "word_counts = sample_df['word_count']\n",
    "axes[0, 1].hist(word_counts, bins=8, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Articles by source\n",
    "source_counts = sample_df['source'].value_counts()\n",
    "axes[1, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Articles by Source')\n",
    "\n",
    "# Keyword frequency (flattened)\n",
    "all_keywords = [kw for keywords in sample_df['keywords'] for kw in keywords]\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = dict(keyword_counts.most_common(6))\n",
    "\n",
    "axes[1, 1].bar(top_keywords.keys(), top_keywords.values(), color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('Top Keywords')\n",
    "axes[1, 1].set_xlabel('Keywords')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save export summary\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "export_summary_data = {\n",
    "    \"export_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_articles\": export_result.get('processed_articles', {}).get('total_articles', 0),\n",
    "    \"formats_exported\": ['csv', 'json', 'parquet'],\n",
    "    \"data_quality_score\": export_result.get('data_quality_report', {}).get('completeness_score', 0)\n",
    "}\n",
    "\n",
    "summary_file = f\"outputs/scraping_export_summary_{timestamp}.json\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(export_summary_data, f, indent=2)\n",
    "    print(f\"\\n💾 Export summary saved: {summary_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Could not save export summary: {e}\")\n",
    "\n",
    "print(f\"\\n🔗 Integration Options:\")\n",
    "print(f\"   • Database: Load into PostgreSQL, MySQL, or MongoDB\")\n",
    "print(f\"   • Analytics: Import into Tableau, Power BI, or Jupyter\")\n",
    "print(f\"   • Search: Index in Elasticsearch or Solr\")\n",
    "print(f\"   • API: Serve via REST API or GraphQL\")\n",
    "print(f\"   • ML Pipeline: Feed into machine learning models\")\n",
    "print(f\"   • Alerting: Set up keyword-based notifications\")\n",
    "\n",
    "print(f\"\\n🎉 Web scraping pipeline completed successfully!\")\n",
    "print(f\"📰 Data ready for analysis and downstream processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowerpower (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
