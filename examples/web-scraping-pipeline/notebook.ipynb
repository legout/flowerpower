{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Pipeline with FlowerPower\n",
    "\n",
    "**Execution:** `uvx --with \"flowerpower[rq],requests>=2.28.0,beautifulsoup4>=4.11.0,pandas>=2.0.0,matplotlib,seaborn\" jupyter lab`\n",
    "\n",
    "This notebook demonstrates web scraping using FlowerPower's JobQueueManager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volker/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-26 16:40:28,548\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê FlowerPower Web Scraping Pipeline\n",
      "üìÅ Project: web-scraping-pipeline\n",
      "üéØ Pipeline: news_scraper\n",
      "‚è∞ Scrape time: 2025-09-26 16:40:28\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Add FlowerPower source to path\n",
    "sys.path.insert(0, str(Path().absolute().parents[2] / \"src\"))\n",
    "\n",
    "from flowerpower.flowerpower import FlowerPowerProject\n",
    "\n",
    "# Initialize project\n",
    "project = FlowerPowerProject.load(\".\")\n",
    "\n",
    "print(\"üåê FlowerPower Web Scraping Pipeline\")\n",
    "print(f\"üìÅ Project: {project.pipeline_manager.project_cfg.name}\")\n",
    "print(f\"üéØ Pipeline: news_scraper\")\n",
    "print(f\"‚è∞ Scrape time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 16:40:31.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1müöÄ Running pipeline 'news_scraper' (attempt 1/4)\u001b[0m\n",
      "-------------------------------------------------------------------\n",
      "Oh no an error! Need help with Hamilton?\n",
      "Join our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\u001b[32m2025-09-26 16:40:31.438\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m248\u001b[0m - \u001b[33m\u001b[1m‚ö†Ô∏è  Pipeline 'news_scraper' failed (attempt 1/4): 2 errors encountered: \n",
      "  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n",
      "  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead.\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:31.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m251\u001b[0m - \u001b[1müîÑ Retrying in 1.01 seconds...\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:32.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1müöÄ Running pipeline 'news_scraper' (attempt 2/4)\u001b[0m\n",
      "-------------------------------------------------------------------\n",
      "Oh no an error! Need help with Hamilton?\n",
      "Join our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\u001b[32m2025-09-26 16:40:32.454\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m248\u001b[0m - \u001b[33m\u001b[1m‚ö†Ô∏è  Pipeline 'news_scraper' failed (attempt 2/4): 2 errors encountered: \n",
      "  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n",
      "  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead.\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:32.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m251\u001b[0m - \u001b[1müîÑ Retrying in 2.11 seconds...\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:34.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1müöÄ Running pipeline 'news_scraper' (attempt 3/4)\u001b[0m\n",
      "-------------------------------------------------------------------\n",
      "Oh no an error! Need help with Hamilton?\n",
      "Join our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\u001b[32m2025-09-26 16:40:34.566\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m248\u001b[0m - \u001b[33m\u001b[1m‚ö†Ô∏è  Pipeline 'news_scraper' failed (attempt 3/4): 2 errors encountered: \n",
      "  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n",
      "  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead.\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:34.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m251\u001b[0m - \u001b[1müîÑ Retrying in 4.26 seconds...\u001b[0m\n",
      "\u001b[32m2025-09-26 16:40:38.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1müöÄ Running pipeline 'news_scraper' (attempt 4/4)\u001b[0m\n",
      "-------------------------------------------------------------------\n",
      "Oh no an error! Need help with Hamilton?\n",
      "Join our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\u001b[32m2025-09-26 16:40:38.837\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mflowerpower.pipeline.pipeline\u001b[0m:\u001b[36m_execute_with_retry\u001b[0m:\u001b[36m257\u001b[0m - \u001b[31m\u001b[1m‚ùå Pipeline 'news_scraper' failed after 4 attempts in 7 seconds: 2 errors encountered: \n",
      "  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n",
      "  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2 errors encountered: \n  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick scraping execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mproject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipeline_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnews_scraper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscrape_timestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43misoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessed_articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ News scraping completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprocessed_articles\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/manager.py:431\u001b[39m, in \u001b[36mPipelineManager.run\u001b[39m\u001b[34m(self, name, run_config, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._executor._project_context = \u001b[38;5;28mself\u001b[39m._project_context\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Delegate to executor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/executor.py:87\u001b[39m, in \u001b[36mPipelineExecutor.run\u001b[39m\u001b[34m(self, name, run_config, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m pipeline = \u001b[38;5;28mself\u001b[39m._registry.get_pipeline(\n\u001b[32m     82\u001b[39m     name=name,\n\u001b[32m     83\u001b[39m     project_context=\u001b[38;5;28mself\u001b[39m._project_context,\n\u001b[32m     84\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/pipeline.py:142\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, run_config, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m retry_exceptions = retry_config[\u001b[33m\"\u001b[39m\u001b[33mretry_exceptions\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Execute with retry logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjitter_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjitter_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/pipeline.py:227\u001b[39m, in \u001b[36mPipeline._execute_with_retry\u001b[39m\u001b[34m(self, run_config, max_retries, retry_delay, jitter_factor, retry_exceptions, start_time)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    223\u001b[39m     logger.info(\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müöÄ Running pipeline \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     end_time = dt.datetime.now()\n\u001b[32m    230\u001b[39m     duration = humanize.naturaldelta(end_time - start_time)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/pipeline.py:312\u001b[39m, in \u001b[36mPipeline._execute_pipeline\u001b[39m\u001b[34m(self, run_config)\u001b[39m\n\u001b[32m    303\u001b[39m     dr = (\n\u001b[32m    304\u001b[39m         driver.Builder()\n\u001b[32m    305\u001b[39m         .with_config(run_config.config)\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m         .build()\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     result = \u001b[43mdr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinal_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# Clean up executor if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/hamilton/driver.py:646\u001b[39m, in \u001b[36mDriver.execute\u001b[39m\u001b[34m(self, final_vars, overrides, display_graph, inputs)\u001b[39m\n\u001b[32m    644\u001b[39m     error_execution = e\n\u001b[32m    645\u001b[39m     error_telemetry = telemetry.sanitize_error(*sys.exc_info())\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter.does_hook(\u001b[33m\"\u001b[39m\u001b[33mpost_graph_execute\u001b[39m\u001b[33m\"\u001b[39m, is_async=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/hamilton/driver.py:632\u001b[39m, in \u001b[36mDriver.execute\u001b[39m\u001b[34m(self, final_vars, overrides, display_graph, inputs)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28mself\u001b[39m.adapter.call_all_lifecycle_hooks_sync(\n\u001b[32m    624\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpre_graph_execute\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    625\u001b[39m         run_id=run_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         overrides=overrides,\n\u001b[32m    630\u001b[39m     )\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__raw_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_final_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter.does_method(\u001b[33m\"\u001b[39m\u001b[33mdo_build_result\u001b[39m\u001b[33m\"\u001b[39m, is_async=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    636\u001b[39m         \u001b[38;5;66;03m# Build the result if we have a result builder\u001b[39;00m\n\u001b[32m    637\u001b[39m         outputs = \u001b[38;5;28mself\u001b[39m.adapter.call_lifecycle_method_sync(\n\u001b[32m    638\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdo_build_result\u001b[39m\u001b[33m\"\u001b[39m, outputs=outputs\n\u001b[32m    639\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/hamilton/driver.py:817\u001b[39m, in \u001b[36mDriver.__raw_execute\u001b[39m\u001b[34m(self, final_vars, overrides, display_graph, inputs, _fn_graph, _run_id)\u001b[39m\n\u001b[32m    815\u001b[39m run_id = _run_id\n\u001b[32m    816\u001b[39m nodes, user_nodes = function_graph.get_upstream_nodes(final_vars, inputs, overrides)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m \u001b[43mDriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO -- validate within the function graph itself\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m display_graph:  \u001b[38;5;66;03m# deprecated flow.\u001b[39;00m\n\u001b[32m    821\u001b[39m     logger.warning(\n\u001b[32m    822\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdisplay_graph=True is deprecated. It will be removed in the 2.0.0 release. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    823\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use visualize_execution().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    824\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/.venv/lib/python3.12/site-packages/hamilton/driver.py:591\u001b[39m, in \u001b[36mDriver.validate_inputs\u001b[39m\u001b[34m(fn_graph, adapter, user_nodes, inputs, nodes_set)\u001b[39m\n\u001b[32m    589\u001b[39m errors.sort()\n\u001b[32m    590\u001b[39m error_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m errors encountered: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m.join(errors)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_str)\n",
      "\u001b[31mValueError\u001b[39m: 2 errors encountered: \n  Error: Type requirement mismatch. Expected date_range:typing.Dict[str, str] got Munch({'start_date': '2024-01-01', 'end_date': '2024-12-31'}):<class 'munch.Munch'> instead.\n  Error: Type requirement mismatch. Expected keywords:typing.Dict[str, typing.Any] got Munch({'include': ['technology', 'science', 'business'], 'exclude': ['spam', 'advertisement']}):<class 'munch.Munch'> instead."
     ]
    }
   ],
   "source": [
    "# Quick scraping execution\n",
    "result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\"scrape_timestamp\": datetime.now().isoformat()},\n",
    "    final_vars=[\"processed_articles\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ News scraping completed!\")\n",
    "if \"processed_articles\" in result:\n",
    "    info = result[\"processed_articles\"]\n",
    "    print(f\"üìÑ Articles saved to: {info['output_file']}\")\n",
    "    print(f\"üìä Total articles: {info['total_articles']}\")\n",
    "    print(f\"üåê Sources: {info['unique_sources']}\")\n",
    "    print(f\"üìà Average length: {info['average_content_length']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraped Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze scraped news data\n",
    "data_file = \"data/news_articles.csv\"\n",
    "\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"üìä News Dataset Overview\")\n",
    "    print(f\"üìà Total articles: {len(df):,}\")\n",
    "    print(f\"üì∞ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    if 'published_date' in df.columns:\n",
    "        df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "        print(f\"üìÖ Date range: {df['published_date'].min()} to {df['published_date'].max()}\")\n",
    "    \n",
    "    # Display sample articles\n",
    "    print(\"\\nüîç Sample Articles:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nüìä Content Statistics:\")\n",
    "    if 'content' in df.columns:\n",
    "        df['content_length'] = df['content'].str.len()\n",
    "        print(f\"   ‚Ä¢ Average content length: {df['content_length'].mean():.0f} characters\")\n",
    "        print(f\"   ‚Ä¢ Longest article: {df['content_length'].max():,} characters\")\n",
    "        print(f\"   ‚Ä¢ Shortest article: {df['content_length'].min():,} characters\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        source_counts = df['source'].value_counts()\n",
    "        print(f\"\\nüåê Sources ({len(source_counts)} unique):\")\n",
    "        for source, count in source_counts.head(5).items():\n",
    "            print(f\"   ‚Ä¢ {source}: {count} articles\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Content length distribution\n",
    "    if 'content_length' in df.columns:\n",
    "        df['content_length'].hist(bins=30, ax=axes[0, 0], alpha=0.7)\n",
    "        axes[0, 0].set_title('Article Length Distribution')\n",
    "        axes[0, 0].set_xlabel('Content Length (characters)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Articles by source\n",
    "    if 'source' in df.columns:\n",
    "        top_sources = df['source'].value_counts().head(8)\n",
    "        top_sources.plot(kind='bar', ax=axes[0, 1], color='lightblue')\n",
    "        axes[0, 1].set_title('Articles by Source')\n",
    "        axes[0, 1].set_xlabel('Source')\n",
    "        axes[0, 1].set_ylabel('Article Count')\n",
    "    \n",
    "    # Articles by date\n",
    "    if 'published_date' in df.columns:\n",
    "        daily_counts = df.groupby(df['published_date'].dt.date).size()\n",
    "        axes[1, 0].plot(daily_counts.index, daily_counts.values, marker='o')\n",
    "        axes[1, 0].set_title('Articles Over Time')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Article Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Content length vs source\n",
    "    if 'content_length' in df.columns and 'source' in df.columns:\n",
    "        avg_length_by_source = df.groupby('source')['content_length'].mean().sort_values(ascending=False).head(8)\n",
    "        avg_length_by_source.plot(kind='barh', ax=axes[1, 1], color='lightgreen')\n",
    "        axes[1, 1].set_title('Average Content Length by Source')\n",
    "        axes[1, 1].set_xlabel('Average Content Length')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data file not found: {data_file}\")\n",
    "    print(\"üí° Run the scraping pipeline first to generate data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different scraping configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Quick Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 10,\n",
    "            \"request_delay\": 0.5,\n",
    "            \"timeout\": 10\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep Scrape\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"request_delay\": 2.0,\n",
    "            \"timeout\": 30,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tech Focus\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_filter\": \"en\",\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "print(\"üß™ Running Scraping Experiments\")\n",
    "print(\"==============================\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\nüîÑ {exp['name']} experiment...\")\n",
    "    \n",
    "    # Add scrape timestamp to config\n",
    "    config = exp['config'].copy()\n",
    "    config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    try:\n",
    "        result = project.pipeline_manager.run(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"]\n",
    "        )\n",
    "        \n",
    "        if \"processed_articles\" in result:\n",
    "            info = result[\"processed_articles\"]\n",
    "            experiment_results.append({\n",
    "                \"name\": exp['name'],\n",
    "                \"total_articles\": info['total_articles'],\n",
    "                \"unique_sources\": info['unique_sources'],\n",
    "                \"avg_length\": info['average_content_length']\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Articles: {info['total_articles']}, Sources: {info['unique_sources']}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Experiment failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Compare experiment results\n",
    "if experiment_results:\n",
    "    print(\"\\nüìä Experiment Comparison\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Articles comparison\n",
    "    axes[0].bar(results_df['name'], results_df['total_articles'], color='skyblue')\n",
    "    axes[0].set_title('Articles Scraped')\n",
    "    axes[0].set_ylabel('Article Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sources comparison\n",
    "    axes[1].bar(results_df['name'], results_df['unique_sources'], color='lightgreen')\n",
    "    axes[1].set_title('Unique Sources')\n",
    "    axes[1].set_ylabel('Source Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average length comparison\n",
    "    axes[2].bar(results_df['name'], results_df['avg_length'], color='lightcoral')\n",
    "    axes[2].set_title('Average Content Length')\n",
    "    axes[2].set_ylabel('Characters')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best experiment\n",
    "    best_exp = results_df.loc[results_df['total_articles'].idxmax()]\n",
    "    print(f\"\\nüèÜ Most articles: {best_exp['name']} ({best_exp['total_articles']} articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive content analysis\n",
    "analysis_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"max_articles\": 100,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"language_detection\": True,\n",
    "        \"content_classification\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"content_analysis\",\n",
    "        \"keyword_summary\",\n",
    "        \"sentiment_summary\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üîç Content Analysis Results\")\n",
    "print(\"===========================\")\n",
    "\n",
    "# Processed articles info\n",
    "if \"processed_articles\" in analysis_result:\n",
    "    articles = analysis_result[\"processed_articles\"]\n",
    "    print(f\"\\nüìÑ Processed Articles:\")\n",
    "    print(f\"   ‚Ä¢ Total articles: {articles['total_articles']}\")\n",
    "    print(f\"   ‚Ä¢ Output file: {articles['output_file']}\")\n",
    "    print(f\"   ‚Ä¢ Processing time: {articles.get('processing_time', 'N/A')}s\")\n",
    "\n",
    "# Content analysis\n",
    "if \"content_analysis\" in analysis_result:\n",
    "    analysis = analysis_result[\"content_analysis\"]\n",
    "    print(f\"\\nüìä Content Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Total words: {analysis['total_words']:,}\")\n",
    "    print(f\"   ‚Ä¢ Unique words: {analysis['unique_words']:,}\")\n",
    "    print(f\"   ‚Ä¢ Average words per article: {analysis['avg_words_per_article']:.0f}\")\n",
    "    print(f\"   ‚Ä¢ Languages detected: {analysis['languages_detected']}\")\n",
    "\n",
    "# Keyword analysis\n",
    "if \"keyword_summary\" in analysis_result:\n",
    "    keywords = analysis_result[\"keyword_summary\"]\n",
    "    print(f\"\\nüîë Keyword Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Total keywords: {keywords['total_keywords']}\")\n",
    "    print(f\"   ‚Ä¢ Unique keywords: {keywords['unique_keywords']}\")\n",
    "    \n",
    "    # Show top keywords\n",
    "    if 'top_keywords' in keywords:\n",
    "        print(f\"   ‚Ä¢ Top 10 keywords:\")\n",
    "        for i, (keyword, count) in enumerate(keywords['top_keywords'][:10], 1):\n",
    "            print(f\"     {i:2d}. {keyword}: {count} occurrences\")\n",
    "\n",
    "# Sentiment analysis\n",
    "if \"sentiment_summary\" in analysis_result:\n",
    "    sentiment = analysis_result[\"sentiment_summary\"]\n",
    "    print(f\"\\nüòä Sentiment Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Average sentiment: {sentiment['average_sentiment']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Positive articles: {sentiment['positive_count']} ({sentiment['positive_percentage']:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Negative articles: {sentiment['negative_count']} ({sentiment['negative_percentage']:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Neutral articles: {sentiment['neutral_count']} ({sentiment['neutral_percentage']:.1f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "if all(key in analysis_result for key in [\"keyword_summary\", \"sentiment_summary\"]):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Top keywords\n",
    "    if 'top_keywords' in analysis_result[\"keyword_summary\"]:\n",
    "        top_keywords = analysis_result[\"keyword_summary\"][\"top_keywords\"][:15]\n",
    "        keyword_names = [k[0] for k in top_keywords]\n",
    "        keyword_counts = [k[1] for k in top_keywords]\n",
    "        \n",
    "        axes[0, 0].barh(range(len(keyword_names)), keyword_counts, color='lightblue')\n",
    "        axes[0, 0].set_yticks(range(len(keyword_names)))\n",
    "        axes[0, 0].set_yticklabels(keyword_names)\n",
    "        axes[0, 0].set_title('Top 15 Keywords')\n",
    "        axes[0, 0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_data = analysis_result[\"sentiment_summary\"]\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral']\n",
    "    sentiment_values = [sentiment_data['positive_percentage'], \n",
    "                       sentiment_data['negative_percentage'], \n",
    "                       sentiment_data['neutral_percentage']]\n",
    "    \n",
    "    colors = ['lightgreen', 'lightcoral', 'lightyellow']\n",
    "    axes[0, 1].pie(sentiment_values, labels=sentiment_labels, autopct='%1.1f%%', colors=colors)\n",
    "    axes[0, 1].set_title('Sentiment Distribution')\n",
    "    \n",
    "    # Sentiment over time (simulated)\n",
    "    if \"processed_articles\" in analysis_result:\n",
    "        # Simulate sentiment over time\n",
    "        dates = pd.date_range(start=datetime.now() - pd.Timedelta(days=7), periods=8, freq='D')\n",
    "        positive_sentiment = [0.6, 0.55, 0.65, 0.7, 0.62, 0.58, 0.64, 0.67]\n",
    "        negative_sentiment = [0.25, 0.3, 0.2, 0.15, 0.23, 0.27, 0.21, 0.18]\n",
    "        \n",
    "        axes[1, 0].plot(dates, positive_sentiment, 'g-', label='Positive', marker='o')\n",
    "        axes[1, 0].plot(dates, negative_sentiment, 'r-', label='Negative', marker='s')\n",
    "        axes[1, 0].set_title('Sentiment Trend (Last 7 Days)')\n",
    "        axes[1, 0].set_xlabel('Date')\n",
    "        axes[1, 0].set_ylabel('Sentiment Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Content categories (simulated)\n",
    "    categories = ['Technology', 'Business', 'Politics', 'Science', 'Health', 'Sports']\n",
    "    category_counts = [35, 28, 22, 18, 15, 12]\n",
    "    \n",
    "    axes[1, 1].pie(category_counts, labels=categories, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Content Categories')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Background Job Queue Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Background Scraping Jobs\")\n",
    "\n",
    "# Single background job\n",
    "print(\"\\nüì• Enqueueing single scraping job...\")\n",
    "try:\n",
    "    job = project.pipeline_manager.enqueue(\n",
    "        \"news_scraper\",\n",
    "        inputs={\n",
    "            \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "            \"max_concurrent_requests\": 8,\n",
    "            \"request_delay\": 1.0,\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        },\n",
    "        final_vars=[\"processed_articles\"],\n",
    "        queue_name=\"scraping\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Job enqueued: {job.id}\")\n",
    "    print(f\"   üìã Queue: {job.origin}\")\n",
    "    print(f\"   ‚è∞ Enqueued at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Enqueue failed: {e}\")\n",
    "    print(\"   üí° Requires Redis for background processing\")\n",
    "\n",
    "# Batch scraping jobs\n",
    "print(\"\\nüì¶ Enqueueing batch scraping jobs...\")\n",
    "\n",
    "batch_configs = [\n",
    "    {\n",
    "        \"name\": \"tech_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"max_articles\": 50,\n",
    "            \"extract_keywords\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"business_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"max_articles\": 30,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"science_news\",\n",
    "        \"config\": {\n",
    "            \"categories\": [\"science\", \"research\", \"innovation\"],\n",
    "            \"max_articles\": 25,\n",
    "            \"language_detection\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_jobs = []\n",
    "for batch in batch_configs:\n",
    "    print(f\"\\n   üîÑ {batch['name']} scraping...\")\n",
    "    \n",
    "    try:\n",
    "        config = batch['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.enqueue(\n",
    "            \"news_scraper\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scrape_{batch['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        \n",
    "        batch_jobs.append((batch['name'], job))\n",
    "        print(f\"     ‚úÖ Enqueued: {job.id}\")\n",
    "        print(f\"     üìä Target: {config.get('max_articles', 'unlimited')} articles\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Failed: {e}\")\n",
    "\n",
    "if batch_jobs:\n",
    "    print(f\"\\nüéâ Successfully enqueued {len(batch_jobs)} batch jobs!\")\n",
    "    print(\"\\nüöÄ To process these jobs, start workers:\")\n",
    "    print(\"   flowerpower job-queue start-worker --queue-names scraping\")\n",
    "    \n",
    "    # Create batch jobs summary\n",
    "    batch_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Job Name\": name,\n",
    "            \"Job ID\": job.id,\n",
    "            \"Queue\": job.origin,\n",
    "            \"Status\": \"Queued\"\n",
    "        }\n",
    "        for name, job in batch_jobs\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìã Batch Jobs Summary:\")\n",
    "    display(batch_df)\n",
    "else:\n",
    "    print(\"\\nüí° No batch jobs enqueued - Redis required for job queuing\")\n",
    "\n",
    "print(f\"\\nüìä Job Queue Monitoring:\")\n",
    "print(f\"   ‚Ä¢ Queue name: scraping\")\n",
    "print(f\"   ‚Ä¢ Recommended workers: 2-4 concurrent workers\")\n",
    "print(f\"   ‚Ä¢ Estimated processing time: 5-15 minutes per job\")\n",
    "print(f\"   ‚Ä¢ Memory usage: ~100-500MB per worker\")\n",
    "print(f\"   ‚Ä¢ Rate limiting: Built-in delays to respect website policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scheduled Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scheduled scraping jobs\n",
    "print(\"üìÖ Scheduled Data Collection\")\n",
    "\n",
    "# Define scraping schedules\n",
    "schedules = [\n",
    "    {\n",
    "        \"name\": \"Hourly Breaking News\",\n",
    "        \"cron\": \"0 * * * *\",  # Every hour\n",
    "        \"description\": \"Quick scan for breaking news\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 20,\n",
    "            \"categories\": [\"breaking\", \"urgent\"],\n",
    "            \"priority\": \"high\",\n",
    "            \"quick_mode\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Daily Tech News\",\n",
    "        \"cron\": \"0 8 * * *\",  # Daily at 8 AM\n",
    "        \"description\": \"Comprehensive technology news collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 100,\n",
    "            \"categories\": [\"technology\", \"ai\", \"software\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Weekly Deep Dive\",\n",
    "        \"cron\": \"0 9 * * 1\",  # Weekly on Monday at 9 AM\n",
    "        \"description\": \"Comprehensive multi-category collection\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 500,\n",
    "            \"categories\": [\"technology\", \"business\", \"science\", \"health\"],\n",
    "            \"extract_keywords\": True,\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"language_detection\": True,\n",
    "            \"content_classification\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Market Opening Scan\",\n",
    "        \"cron\": \"30 9 * * 1-5\",  # Weekdays at 9:30 AM\n",
    "        \"description\": \"Business and market news before trading\",\n",
    "        \"config\": {\n",
    "            \"max_articles\": 50,\n",
    "            \"categories\": [\"business\", \"finance\", \"market\"],\n",
    "            \"sentiment_analysis\": True,\n",
    "            \"priority\": \"high\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "scheduled_jobs = []\n",
    "\n",
    "for schedule in schedules:\n",
    "    print(f\"\\nüìã {schedule['name']}\")\n",
    "    print(f\"   ‚è∞ Schedule: {schedule['description']}\")\n",
    "    print(f\"   üîß Cron: {schedule['cron']}\")\n",
    "    print(f\"   üìä Target articles: {schedule['config'].get('max_articles', 'unlimited')}\")\n",
    "    \n",
    "    try:\n",
    "        # Add scrape timestamp to config\n",
    "        config = schedule['config'].copy()\n",
    "        config['scrape_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        job = project.pipeline_manager.schedule(\n",
    "            \"news_scraper\",\n",
    "            cron=schedule['cron'],\n",
    "            inputs=config,\n",
    "            final_vars=[\"processed_articles\"],\n",
    "            queue_name=\"scraping\",\n",
    "            job_id=f\"scheduled_{schedule['name'].lower().replace(' ', '_')}\"\n",
    "        )\n",
    "        \n",
    "        scheduled_jobs.append((schedule['name'], job, schedule['description']))\n",
    "        print(f\"   ‚úÖ Scheduled successfully - Job ID: {job.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Scheduling failed: {e}\")\n",
    "        print(\"   üí° Requires Redis for job scheduling\")\n",
    "\n",
    "if scheduled_jobs:\n",
    "    print(f\"\\nüéâ Successfully scheduled {len(scheduled_jobs)} scraping jobs!\")\n",
    "    print(\"\\nüöÄ To process scheduled jobs, start a worker with scheduler:\")\n",
    "    print(\"   flowerpower job-queue start-worker --with-scheduler\")\n",
    "    \n",
    "    # Create schedule visualization\n",
    "    schedule_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Schedule Name\": name,\n",
    "            \"Description\": desc,\n",
    "            \"Cron Expression\": s[\"cron\"],\n",
    "            \"Max Articles\": s[\"config\"].get(\"max_articles\", \"unlimited\")\n",
    "        }\n",
    "        for (name, job, desc), s in zip(scheduled_jobs, schedules)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìÖ Scheduled Jobs Summary:\")\n",
    "    display(schedule_df)\n",
    "    \n",
    "    # Visualize schedule frequency\n",
    "    schedule_types = ['Hourly', 'Daily', 'Weekly', 'Weekdays']\n",
    "    frequencies = [24, 1, 1/7, 5]  # executions per day\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(schedule_types, frequencies, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    plt.title('Scheduled Scraping Frequency')\n",
    "    plt.xlabel('Schedule Type')\n",
    "    plt.ylabel('Executions per Day')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(frequencies):\n",
    "        plt.text(i, v, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nüí° No schedules created - Redis required for scheduling functionality\")\n",
    "\n",
    "# Data collection estimates\n",
    "print(f\"\\nüìà Data Collection Estimates:\")\n",
    "print(f\"   ‚Ä¢ Hourly: ~20 articles = 480 articles/day\")\n",
    "print(f\"   ‚Ä¢ Daily: ~100 articles = 100 articles/day\")\n",
    "print(f\"   ‚Ä¢ Weekly: ~500 articles = 71 articles/day\")\n",
    "print(f\"   ‚Ä¢ Weekdays: ~50 articles = 250 articles/day\")\n",
    "print(f\"   ‚Ä¢ Total estimated: ~900 articles/day\")\n",
    "print(f\"   ‚Ä¢ Monthly volume: ~27,000 articles\")\n",
    "print(f\"   ‚Ä¢ Storage needed: ~50-100GB/month (with content)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data export\n",
    "print(\"üì§ Data Export and Integration\")\n",
    "\n",
    "# Run scraping with comprehensive export options\n",
    "export_result = project.pipeline_manager.run(\n",
    "    \"news_scraper\",\n",
    "    inputs={\n",
    "        \"scrape_timestamp\": datetime.now().isoformat(),\n",
    "        \"export_formats\": [\"csv\", \"json\", \"parquet\"],\n",
    "        \"include_metadata\": True,\n",
    "        \"extract_keywords\": True,\n",
    "        \"sentiment_analysis\": True,\n",
    "        \"compress_output\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"processed_articles\",\n",
    "        \"export_summary\",\n",
    "        \"data_quality_report\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if \"processed_articles\" in export_result:\n",
    "    articles = export_result[\"processed_articles\"]\n",
    "    print(f\"\\nüìä Export Results:\")\n",
    "    print(f\"   ‚Ä¢ Articles exported: {articles['total_articles']}\")\n",
    "    print(f\"   ‚Ä¢ Output file: {articles['output_file']}\")\n",
    "    print(f\"   ‚Ä¢ File size: {articles.get('file_size_mb', 'N/A')} MB\")\n",
    "    print(f\"   ‚Ä¢ Compression ratio: {articles.get('compression_ratio', 'N/A')}\")\n",
    "\n",
    "if \"export_summary\" in export_result:\n",
    "    summary = export_result[\"export_summary\"]\n",
    "    print(f\"\\nüìã Export Summary:\")\n",
    "    print(f\"   ‚Ä¢ Formats generated: {summary['formats_created']}\")\n",
    "    print(f\"   ‚Ä¢ Total files: {summary['total_files']}\")\n",
    "    print(f\"   ‚Ä¢ Total size: {summary['total_size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Show file details\n",
    "    if 'file_details' in summary:\n",
    "        print(f\"\\nüìÅ Generated Files:\")\n",
    "        for file_info in summary['file_details']:\n",
    "            print(f\"   ‚Ä¢ {file_info['format']}: {file_info['filename']} ({file_info['size_mb']:.1f} MB)\")\n",
    "\n",
    "if \"data_quality_report\" in export_result:\n",
    "    quality = export_result[\"data_quality_report\"]\n",
    "    print(f\"\\n‚úÖ Data Quality Report:\")\n",
    "    print(f\"   ‚Ä¢ Completeness score: {quality['completeness_score']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Accuracy score: {quality['accuracy_score']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Duplicate articles: {quality['duplicate_count']}\")\n",
    "    print(f\"   ‚Ä¢ Missing content: {quality['missing_content_count']}\")\n",
    "    print(f\"   ‚Ä¢ Quality grade: {quality['overall_grade']}\")\n",
    "\n",
    "# Create sample analysis of exported data\n",
    "print(f\"\\nüîç Sample Data Analysis\")\n",
    "\n",
    "# Simulate analysis of exported data\n",
    "sample_data = {\n",
    "    'articles': [\n",
    "        {\n",
    "            'title': 'AI Revolution in Healthcare',\n",
    "            'source': 'TechNews',\n",
    "            'sentiment': 0.7,\n",
    "            'keywords': ['AI', 'healthcare', 'innovation'],\n",
    "            'word_count': 850\n",
    "        },\n",
    "        {\n",
    "            'title': 'Market Volatility Continues',\n",
    "            'source': 'FinanceDaily',\n",
    "            'sentiment': -0.3,\n",
    "            'keywords': ['market', 'volatility', 'economy'],\n",
    "            'word_count': 650\n",
    "        },\n",
    "        {\n",
    "            'title': 'Breakthrough in Quantum Computing',\n",
    "            'source': 'ScienceToday',\n",
    "            'sentiment': 0.8,\n",
    "            'keywords': ['quantum', 'computing', 'breakthrough'],\n",
    "            'word_count': 1200\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "sample_df = pd.DataFrame(sample_data['articles'])\n",
    "\n",
    "print(\"\\nüìä Sample Exported Data:\")\n",
    "display(sample_df)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiments = sample_df['sentiment']\n",
    "axes[0, 0].hist(sentiments, bins=10, alpha=0.7, color='lightblue')\n",
    "axes[0, 0].set_title('Sentiment Distribution')\n",
    "axes[0, 0].set_xlabel('Sentiment Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Word count distribution\n",
    "word_counts = sample_df['word_count']\n",
    "axes[0, 1].hist(word_counts, bins=8, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Articles by source\n",
    "source_counts = sample_df['source'].value_counts()\n",
    "axes[1, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Articles by Source')\n",
    "\n",
    "# Keyword frequency (flattened)\n",
    "all_keywords = [kw for keywords in sample_df['keywords'] for kw in keywords]\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = dict(keyword_counts.most_common(6))\n",
    "\n",
    "axes[1, 1].bar(top_keywords.keys(), top_keywords.values(), color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('Top Keywords')\n",
    "axes[1, 1].set_xlabel('Keywords')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save export summary\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "export_summary_data = {\n",
    "    \"export_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_articles\": export_result.get('processed_articles', {}).get('total_articles', 0),\n",
    "    \"formats_exported\": ['csv', 'json', 'parquet'],\n",
    "    \"data_quality_score\": export_result.get('data_quality_report', {}).get('completeness_score', 0)\n",
    "}\n",
    "\n",
    "summary_file = f\"outputs/scraping_export_summary_{timestamp}.json\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(export_summary_data, f, indent=2)\n",
    "    print(f\"\\nüíæ Export summary saved: {summary_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not save export summary: {e}\")\n",
    "\n",
    "print(f\"\\nüîó Integration Options:\")\n",
    "print(f\"   ‚Ä¢ Database: Load into PostgreSQL, MySQL, or MongoDB\")\n",
    "print(f\"   ‚Ä¢ Analytics: Import into Tableau, Power BI, or Jupyter\")\n",
    "print(f\"   ‚Ä¢ Search: Index in Elasticsearch or Solr\")\n",
    "print(f\"   ‚Ä¢ API: Serve via REST API or GraphQL\")\n",
    "print(f\"   ‚Ä¢ ML Pipeline: Feed into machine learning models\")\n",
    "print(f\"   ‚Ä¢ Alerting: Set up keyword-based notifications\")\n",
    "\n",
    "print(f\"\\nüéâ Web scraping pipeline completed successfully!\")\n",
    "print(f\"üì∞ Data ready for analysis and downstream processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowerpower (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
