{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training Pipeline Example - Customer Churn Prediction\n",
    "\n",
    "**Execution:** `uvx --with \"flowerpower[rq],pandas>=2.0.0,scikit-learn>=1.3.0,matplotlib,seaborn\" jupyter lab`\n",
    "\n",
    "This notebook demonstrates FlowerPower for ML workflows with customer churn prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add FlowerPower source to path\n",
    "sys.path.insert(0, str(Path().absolute().parents[2] / \"src\"))\n",
    "\n",
    "from flowerpower.flowerpower import FlowerPowerProject\n",
    "\n",
    "# Initialize project\n",
    "project = FlowerPowerProject.load(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 FlowerPower ML Training Pipeline\n",
      "===================================\n",
      "📁 Project: ml-training-pipeline\n",
      "🎯 Pipeline: customer_churn\n",
      "⚡ Quick execution mode\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚡ Quick execution mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Quick pipeline execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43mproject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipeline_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_churn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43misoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_evaluation_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ ML pipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_evaluation_report\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/manager.py:431\u001b[39m, in \u001b[36mPipelineManager.run\u001b[39m\u001b[34m(self, name, run_config, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._executor._project_context = \u001b[38;5;28mself\u001b[39m._project_context\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Delegate to executor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/executor.py:81\u001b[39m, in \u001b[36mPipelineExecutor.run\u001b[39m\u001b[34m(self, name, run_config, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     setup_logging(level=run_config.log_level)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Get the pipeline object from registry\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_project_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline.run(run_config=run_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/registry.py:219\u001b[39m, in \u001b[36mPipelineRegistry.get_pipeline\u001b[39m\u001b[34m(self, name, project_context, reload)\u001b[39m\n\u001b[32m    216\u001b[39m config = \u001b[38;5;28mself\u001b[39m.load_config(name, reload=reload)\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Load pipeline module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Import Pipeline class here to avoid circular import\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/registry.py:302\u001b[39m, in \u001b[36mPipelineRegistry.load_module\u001b[39m\u001b[34m(self, name, reload)\u001b[39m\n\u001b[32m    299\u001b[39m module_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpipelines.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Load the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m module = \u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;66;03m# Cache the module (will be stored in consolidated cache when pipeline is created)\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# For now, we'll update the existing cache entry if it exists\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pipeline_data_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/src/flowerpower/pipeline/base.py:31\u001b[39m, in \u001b[36mload_module\u001b[39m\u001b[34m(name, reload)\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib.reload(sys.modules[name])\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules[name]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-aarch64-gnu/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/flowerpower/.worktree/code-simplification-analysis/examples/ml-training-pipeline/pipelines/customer_churn.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhamilton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_modifiers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, parameterize\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mloguru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingClassifier, RandomForestClassifier\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (classification_report, confusion_matrix,\n\u001b[32m     18\u001b[39m                              roc_auc_score)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"🤖 FlowerPower ML Training Pipeline\")\n",
    "print(\"===================================\")\n",
    "print(f\"📁 Project: {project.pipeline_manager.project_cfg.name}\")\n",
    "print(f\"🎯 Pipeline: customer_churn\")\n",
    "print(f\"⚡ Quick execution mode\")\n",
    "\n",
    "# Quick pipeline execution\n",
    "result = project.pipeline_manager.run(\n",
    "    \"customer_churn\",\n",
    "    inputs={\"training_date\": datetime.now().isoformat()},\n",
    "    final_vars=[\"model_evaluation_report\"]\n",
    ")\n",
    "\n",
    "print(\"✅ ML pipeline completed successfully!\")\n",
    "if \"model_evaluation_report\" in result:\n",
    "    evaluation = result[\"model_evaluation_report\"]\n",
    "    print(f\"📊 Model accuracy: {evaluation['model_metrics']['accuracy']:.3f}\")\n",
    "    print(f\"🎯 F1 score: {evaluation['model_metrics']['f1_score']:.3f}\")\n",
    "    print(f\"📄 Full report: {evaluation['report_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the customer data that our ML pipeline will process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the customer data\n",
    "data_file = \"data/customer_data.csv\"\n",
    "\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"📊 Dataset shape: {df.shape}\")\n",
    "    print(f\"📈 Features: {df.columns.tolist()}\")\n",
    "    print(f\"🎯 Target variable: 'churn' (if available)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Dataset Overview:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Dataset Info:\")\n",
    "    display(df.info())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Statistical Summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    # Check for target variable\n",
    "    if 'churn' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Churn Distribution:\")\n",
    "        churn_counts = df['churn'].value_counts()\n",
    "        print(churn_counts)\n",
    "        \n",
    "        # Visualize churn distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        churn_counts.plot(kind='bar')\n",
    "        plt.title('Churn Distribution')\n",
    "        plt.xlabel('Churn')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        churn_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Churn Percentage')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"⚠️ Data file not found: {data_file}\")\n",
    "    print(\"💡 The pipeline will generate synthetic data during execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detailed pipeline execution\n",
    "detailed_result = project.pipeline_manager.run(\n",
    "    \"customer_churn\",\n",
    "    inputs={\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"test_size\": 0.2,\n",
    "        \"random_state\": 42,\n",
    "        \"model_params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_samples_split\": 2,\n",
    "            \"min_samples_leaf\": 1\n",
    "        }\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"trained_model\",\n",
    "        \"model_predictions\",\n",
    "        \"model_evaluation_report\",\n",
    "        \"customer_features\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"🔍 Detailed Pipeline Analysis\")\n",
    "print(\"=============================\")\n",
    "\n",
    "# Analyze trained model\n",
    "if \"trained_model\" in detailed_result:\n",
    "    model_info = detailed_result[\"trained_model\"]\n",
    "    print(f\"\\n🤖 Model Training:\")\n",
    "    print(f\"   • Algorithm: {model_info['model_type']}\")\n",
    "    print(f\"   • Training time: {model_info['training_time']:.2f}s\")\n",
    "    print(f\"   • Model file: {model_info['model_file']}\")\n",
    "\n",
    "# Analyze predictions\n",
    "if \"model_predictions\" in detailed_result:\n",
    "    predictions = detailed_result[\"model_predictions\"]\n",
    "    print(f\"\\n🎯 Predictions:\")\n",
    "    print(f\"   • Prediction count: {predictions['prediction_count']}\")\n",
    "    print(f\"   • Churn predicted: {predictions['churn_predictions']}\")\n",
    "    print(f\"   • No churn predicted: {predictions['no_churn_predictions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model's performance with key metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract evaluation metrics for visualization\n",
    "if \"model_evaluation_report\" in detailed_result:\n",
    "    evaluation = detailed_result[\"model_evaluation_report\"]\n",
    "    metrics = evaluation[\"model_metrics\"]\n",
    "    \n",
    "    print(\"📈 Model Performance Metrics\")\n",
    "    print(\"============================\")\n",
    "    \n",
    "    # Display key metrics\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "    \n",
    "    # Create performance visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Metrics bar chart\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    metric_values = [metrics['accuracy'], metrics['precision'], \n",
    "                    metrics['recall'], metrics['f1_score']]\n",
    "    \n",
    "    axes[0, 0].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "    axes[0, 0].set_title('Model Performance Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    for i, v in enumerate(metric_values):\n",
    "        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    # Confusion matrix (simulated for demonstration)\n",
    "    conf_matrix = [[85, 15], [20, 80]]  # Example confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Confusion Matrix')\n",
    "    axes[0, 1].set_xlabel('Predicted')\n",
    "    axes[0, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # Feature importance (simulated)\n",
    "    feature_names = ['tenure', 'monthly_charges', 'total_charges', 'contract_type', 'payment_method']\n",
    "    importance_scores = [0.25, 0.20, 0.18, 0.15, 0.12]\n",
    "    \n",
    "    axes[1, 0].barh(feature_names, importance_scores, color='lightblue')\n",
    "    axes[1, 0].set_title('Feature Importance')\n",
    "    axes[1, 0].set_xlabel('Importance Score')\n",
    "    \n",
    "    # Training history (simulated)\n",
    "    epochs = list(range(1, 11))\n",
    "    train_acc = [0.65, 0.70, 0.75, 0.78, 0.80, 0.82, 0.83, 0.84, 0.84, 0.85]\n",
    "    val_acc = [0.63, 0.68, 0.72, 0.74, 0.76, 0.77, 0.78, 0.78, 0.79, 0.79]\n",
    "    \n",
    "    axes[1, 1].plot(epochs, train_acc, 'b-', label='Training Accuracy', marker='o')\n",
    "    axes[1, 1].plot(epochs, val_acc, 'r-', label='Validation Accuracy', marker='s')\n",
    "    axes[1, 1].set_title('Training Progress')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display evaluation summary\n",
    "    print(f\"\\n📄 Evaluation Report: {evaluation['report_file']}\")\n",
    "    print(f\"⏱️ Evaluation time: {evaluation['evaluation_metadata']['completed_at']}\")\n",
    "    print(f\"🎯 Model ready for deployment: {metrics['accuracy'] > 0.75}\")\n",
    "else:\n",
    "    print(\"⚠️ No evaluation results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment with Different Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline with different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Default\",\n",
    "        \"config\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Large Test Set\",\n",
    "        \"config\": {\n",
    "            \"test_size\": 0.3,\n",
    "            \"random_state\": 42,\n",
    "            \"model_params\": {\n",
    "                \"n_estimators\": 150,\n",
    "                \"max_depth\": 8\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Feature Engineering Focus\", \n",
    "        \"config\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 123,\n",
    "            \"feature_selection\": True,\n",
    "            \"scale_features\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "print(\"🧪 Running ML Experiments\")\n",
    "print(\"==========================\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n🔄 Running {exp['name']} experiment...\")\n",
    "    \n",
    "    # Add training date to config\n",
    "    config = exp['config'].copy()\n",
    "    config['training_date'] = datetime.now().isoformat()\n",
    "    \n",
    "    try:\n",
    "        result = project.pipeline_manager.run(\n",
    "            \"customer_churn\",\n",
    "            inputs=config,\n",
    "            final_vars=[\"model_evaluation_report\"]\n",
    "        )\n",
    "        \n",
    "        if \"model_evaluation_report\" in result:\n",
    "            metrics = result[\"model_evaluation_report\"][\"model_metrics\"]\n",
    "            experiment_results.append({\n",
    "                \"name\": exp['name'],\n",
    "                \"accuracy\": metrics['accuracy'],\n",
    "                \"f1_score\": metrics['f1_score'],\n",
    "                \"precision\": metrics['precision'],\n",
    "                \"recall\": metrics['recall']\n",
    "            })\n",
    "            \n",
    "            print(f\"   ✅ Accuracy: {metrics['accuracy']:.3f}, F1: {metrics['f1_score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Experiment failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "# Compare experiment results\n",
    "if experiment_results:\n",
    "    print(\"\\n📊 Experiment Comparison\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(results_df['name'], results_df['accuracy'], color='skyblue')\n",
    "    axes[0].set_title('Accuracy Comparison')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    for i, v in enumerate(results_df['accuracy']):\n",
    "        axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    # All metrics comparison\n",
    "    metrics_cols = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    x = range(len(results_df))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics_cols):\n",
    "        axes[1].bar([pos + width * i for pos in x], results_df[metric], \n",
    "                   width, label=metric.title())\n",
    "    \n",
    "    axes[1].set_title('All Metrics Comparison')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_xticks([pos + width * 1.5 for pos in x])\n",
    "    axes[1].set_xticklabels(results_df['name'])\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best experiment\n",
    "    best_exp = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "    print(f\"\\n🏆 Best performing experiment: {best_exp['name']}\")\n",
    "    print(f\"   📈 F1 Score: {best_exp['f1_score']:.3f}\")\n",
    "    print(f\"   🎯 Accuracy: {best_exp['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Background Job Queue Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule model training jobs for background execution using FlowerPower's JobQueueManager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate job queue functionality for ML training\n",
    "print(\"🚀 Job Queue ML Training Example\")\n",
    "print(\"=================================\")\n",
    "\n",
    "# Queue a training job\n",
    "try:\n",
    "    job = project.pipeline_manager.enqueue(\n",
    "        \"customer_churn\",\n",
    "        inputs={\n",
    "            \"training_date\": datetime.now().isoformat(),\n",
    "            \"test_size\": 0.25,\n",
    "            \"model_params\": {\n",
    "                \"n_estimators\": 200,\n",
    "                \"max_depth\": 10\n",
    "            }\n",
    "        },\n",
    "        final_vars=[\"model_evaluation_report\"],\n",
    "        queue_name=\"ml_training\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Training job enqueued!\")\n",
    "    print(f\"🔧 Job ID: {job.id}\")\n",
    "    print(f\"📋 Queue: {job.origin}\")\n",
    "    print(\"\\n🚀 To process this job, start a worker:\")\n",
    "    print(\"   flowerpower job-queue start-worker --queue-names ml_training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Job queue error: {e}\")\n",
    "    print(\"💡 This requires Redis to be running for background jobs\")\n",
    "\n",
    "# Schedule recurring model retraining\n",
    "try:\n",
    "    scheduled_job = project.pipeline_manager.schedule(\n",
    "        \"customer_churn\",\n",
    "        cron=\"0 2 * * 1\",  # Every Monday at 2 AM\n",
    "        inputs={\n",
    "            \"training_date\": datetime.now().isoformat(),\n",
    "            \"test_size\": 0.2,\n",
    "            \"retrain_model\": True\n",
    "        },\n",
    "        final_vars=[\"model_evaluation_report\"],\n",
    "        queue_name=\"ml_training\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📅 Scheduled weekly model retraining!\")\n",
    "    print(f\"🔧 Job ID: {scheduled_job.id}\")\n",
    "    print(f\"⏰ Schedule: Every Monday at 2:00 AM\")\n",
    "    print(\"\\n🚀 To process scheduled jobs, start a worker with scheduler:\")\n",
    "    print(\"   flowerpower job-queue start-worker --with-scheduler\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Scheduling error: {e}\")\n",
    "    print(\"💡 This requires Redis to be running for scheduled jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Export and Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export trained models and analyze results for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline with export focus\n",
    "export_result = project.pipeline_manager.run(\n",
    "    \"customer_churn\",\n",
    "    inputs={\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"export_model\": True,\n",
    "        \"save_predictions\": True,\n",
    "        \"generate_report\": True\n",
    "    },\n",
    "    final_vars=[\n",
    "        \"trained_model\",\n",
    "        \"model_predictions\",\n",
    "        \"model_evaluation_report\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"📦 Model Export and Analysis\")\n",
    "print(\"=============================\")\n",
    "\n",
    "# Model export information\n",
    "if \"trained_model\" in export_result:\n",
    "    model_info = export_result[\"trained_model\"]\n",
    "    print(f\"\\n🤖 Trained Model:\")\n",
    "    print(f\"   • Model type: {model_info['model_type']}\")\n",
    "    print(f\"   • Model file: {model_info['model_file']}\")\n",
    "    print(f\"   • Training time: {model_info['training_time']:.2f}s\")\n",
    "    print(f\"   • Model size: {model_info.get('model_size', 'N/A')}\")\n",
    "\n",
    "# Predictions export\n",
    "if \"model_predictions\" in export_result:\n",
    "    predictions_info = export_result[\"model_predictions\"]\n",
    "    print(f\"\\n🎯 Predictions:\")\n",
    "    print(f\"   • Predictions file: {predictions_info['predictions_file']}\")\n",
    "    print(f\"   • Prediction count: {predictions_info['prediction_count']}\")\n",
    "    print(f\"   • Churn rate: {predictions_info['churn_predictions'] / predictions_info['prediction_count']:.1%}\")\n",
    "\n",
    "# Evaluation report\n",
    "if \"model_evaluation_report\" in export_result:\n",
    "    evaluation = export_result[\"model_evaluation_report\"]\n",
    "    print(f\"\\n📊 Evaluation Report:\")\n",
    "    print(f\"   • Report file: {evaluation['report_file']}\")\n",
    "    print(f\"   • Model accuracy: {evaluation['model_metrics']['accuracy']:.3f}\")\n",
    "    print(f\"   • Model F1 score: {evaluation['model_metrics']['f1_score']:.3f}\")\n",
    "    print(f\"   • Ready for production: {evaluation['model_metrics']['accuracy'] > 0.8}\")\n",
    "\n",
    "# Create deployment summary\n",
    "print(f\"\\n🚀 Deployment Summary\")\n",
    "print(f\"=====================\")\n",
    "print(f\"📅 Training date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"📈 Model performance: {'Good' if export_result.get('model_evaluation_report', {}).get('model_metrics', {}).get('accuracy', 0) > 0.75 else 'Needs improvement'}\")\n",
    "print(f\"✅ Ready for deployment: {export_result.get('model_evaluation_report', {}).get('model_metrics', {}).get('accuracy', 0) > 0.8}\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_data = {\n",
    "    \"training_timestamp\": datetime.now().isoformat(),\n",
    "    \"model_accuracy\": export_result.get('model_evaluation_report', {}).get('model_metrics', {}).get('accuracy', 0),\n",
    "    \"model_f1_score\": export_result.get('model_evaluation_report', {}).get('model_metrics', {}).get('f1_score', 0),\n",
    "    \"total_predictions\": export_result.get('model_predictions', {}).get('prediction_count', 0),\n",
    "    \"churn_predictions\": export_result.get('model_predictions', {}).get('churn_predictions', 0)\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_data])\n",
    "summary_file = f\"outputs/ml_training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"\\n💾 Training summary saved: {summary_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Could not save summary: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 ML Training Pipeline completed successfully!\")\n",
    "print(f\"💡 Use the exported model for real-time churn prediction in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different aspects of the ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with pipeline-only mode (no job queue)\n",
    "print(\"🔧 Pipeline-Only Mode Experiment\")\n",
    "print(\"==================================\")\n",
    "\n",
    "# Use pipeline manager directly\n",
    "simple_result = project.pipeline_manager.run(\n",
    "    \"customer_churn\",\n",
    "    inputs={\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"simple_mode\": True,\n",
    "        \"test_size\": 0.3\n",
    "    },\n",
    "    final_vars=[\"customer_features\", \"model_evaluation_report\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Pipeline-only execution completed\")\n",
    "print(\"💡 This mode doesn't require Redis - perfect for development!\")\n",
    "\n",
    "if \"customer_features\" in simple_result:\n",
    "    features = simple_result[\"customer_features\"]\n",
    "    print(f\"📊 Features processed: {features['feature_count']}\")\n",
    "\n",
    "if \"model_evaluation_report\" in simple_result:\n",
    "    metrics = simple_result[\"model_evaluation_report\"][\"model_metrics\"]\n",
    "    print(f\"🎯 Quick model accuracy: {metrics['accuracy']:.3f}\")\n",
    "\n",
    "# Custom feature engineering experiment\n",
    "print(\"\\n🛠️ Custom Feature Engineering\")\n",
    "print(\"==============================\")\n",
    "\n",
    "custom_feature_result = project.pipeline_manager.run(\n",
    "    \"customer_churn\",\n",
    "    inputs={\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"feature_engineering\": {\n",
    "            \"polynomial_features\": True,\n",
    "            \"interaction_features\": True,\n",
    "            \"feature_selection_k\": 15\n",
    "        },\n",
    "        \"cross_validation\": True\n",
    "    },\n",
    "    final_vars=[\"customer_features\", \"model_evaluation_report\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Custom feature engineering completed\")\n",
    "\n",
    "if \"customer_features\" in custom_feature_result:\n",
    "    features = custom_feature_result[\"customer_features\"]\n",
    "    print(f\"🔧 Enhanced features: {features['feature_count']}\")\n",
    "    print(f\"⚡ Feature selection applied: {features.get('selected_features', 'N/A')}\")\n",
    "\n",
    "if \"model_evaluation_report\" in custom_feature_result:\n",
    "    metrics = custom_feature_result[\"model_evaluation_report\"][\"model_metrics\"]\n",
    "    print(f\"📈 Enhanced model accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"🎯 Enhanced F1 score: {metrics['f1_score']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowerpower (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
