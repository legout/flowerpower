# Pipeline-Only Example - Text Processing

**Execution:** `uvx --with "flowerpower,pandas>=2.0.0,matplotlib,seaborn,nltk" jupyter lab`

This notebook demonstrates FlowerPower's pipeline functionality without job queue dependencies.

## Quick Start

```python
import sys
import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter
import re

# Add FlowerPower source to path
sys.path.insert(0, str(Path().absolute().parents[2] / "src"))

from flowerpower.pipeline.manager import PipelineManager

# Initialize pipeline manager without job queue
pipeline_manager = PipelineManager(
    project_cfg_path="conf/project.yml",
    base_dir=".",
    fs=None,
    cfg_dir="conf",
    pipelines_dir="pipelines"
)

print("üìù FlowerPower Pipeline-Only Text Processor")
print("===========================================")
print("üí° No Redis or job queue required!")
print(f"üéØ Pipeline: text_processor")
print(f"‚è∞ Process time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
```

```python
# Quick text processing execution
result = pipeline_manager.run(
    "text_processor",
    inputs={"process_timestamp": datetime.now().isoformat()},
    final_vars=["text_analysis_results"]
)

print("‚úÖ Text processing completed successfully!")
if "text_analysis_results" in result:
    analysis = result["text_analysis_results"]
    print(f"üìÑ Analysis completed at: {analysis['processing_metadata']['completed_at']}")
    print(f"üìä Chunks processed: {analysis['processing_metadata']['total_chunks_processed']}")
    
    if 'word_analysis' in analysis:
        word_stats = analysis['word_analysis']
        print(f"üìù Words: {word_stats['total_words']} total, {word_stats['unique_words']} unique")
    
    if 'sentiment_analysis' in analysis:
        sentiment = analysis['sentiment_analysis']
        print(f"üòä Overall sentiment: {sentiment['overall_sentiment']}")
    
    if 'output_file' in analysis:
        print(f"üíæ Results saved to: {analysis['output_file']}")
```

## 1. Input Text Data Exploration

```python
# Load and explore the input text data
text_file = "data/sample_texts.txt"

if Path(text_file).exists():
    with open(text_file, 'r', encoding='utf-8') as f:
        text_content = f.read()
    
    print(f"üìä Text Data Overview")
    print(f"=====================")
    print(f"üìà Total characters: {len(text_content):,}")
    print(f"üìù Total words: {len(text_content.split()):,}")
    print(f"üìÑ Total lines: {len(text_content.splitlines()):,}")
    
    # Show sample text
    print("\nüîç Sample Text (first 500 characters):")
    print("=" * 50)
    print(text_content[:500] + "...")
    print("=" * 50)
    
    # Basic text statistics
    words = text_content.split()
    word_lengths = [len(word) for word in words]
    
    print(f"\nüìä Basic Statistics:")
    print(f"   ‚Ä¢ Average word length: {sum(word_lengths) / len(word_lengths):.1f} characters")
    print(f"   ‚Ä¢ Longest word: {max(word_lengths)} characters")
    
    # Word frequency analysis
    word_freq = Counter(word.lower().strip('.,!?;:"()[]') for word in words)
    print(f"\nüî§ Most Common Words:")
    for word, count in word_freq.most_common(5):
        print(f"   ‚Ä¢ {word}: {count} times")
    
    # Create simple visualization
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # Word length distribution
    axes[0].hist(word_lengths, bins=15, alpha=0.7, color='lightblue')
    axes[0].set_title('Word Length Distribution')
    axes[0].set_xlabel('Word Length')
    axes[0].set_ylabel('Frequency')
    
    # Top words frequency
    top_words = dict(word_freq.most_common(8))
    axes[1].barh(list(top_words.keys()), list(top_words.values()), color='orange')
    axes[1].set_title('Top 8 Most Frequent Words')
    axes[1].set_xlabel('Frequency')
    
    plt.tight_layout()
    plt.show()
    
else:
    print(f"‚ö†Ô∏è Text file not found: {text_file}")
    print("üí° The pipeline will generate sample text data during execution")
```

## 2. Detailed Pipeline Analysis

```python
# Run detailed text processing with all intermediate steps
detailed_result = pipeline_manager.run(
    "text_processor",
    inputs={
        "process_timestamp": datetime.now().isoformat(),
        "chunk_size": 1000,
        "operations": ["word_count", "sentence_count", "extract_keywords", "analyze_sentiment"]
    },
    final_vars=[
        "text_chunks",
        "word_analysis",
        "sentence_analysis",
        "keyword_analysis",
        "sentiment_analysis",
        "text_analysis_results"
    ]
)

print("üîç Detailed Pipeline Analysis")
print("==============================")

# Analyze text chunking
if "text_chunks" in detailed_result:
    chunks = detailed_result["text_chunks"]
    print(f"\nüìÑ Text Chunking:")
    print(f"   ‚Ä¢ Total chunks: {chunks['total_chunks']}")
    print(f"   ‚Ä¢ Chunk size: {chunks['chunk_size']} characters")
    print(f"   ‚Ä¢ Processing time: {chunks['processing_time']:.2f}s")

# Analyze word processing
if "word_analysis" in detailed_result:
    words = detailed_result["word_analysis"]
    print(f"\nüìù Word Analysis:")
    print(f"   ‚Ä¢ Total words: {words['total_words']:,}")
    print(f"   ‚Ä¢ Unique words: {words['unique_words']:,}")
    print(f"   ‚Ä¢ Vocabulary richness: {words['vocabulary_richness']:.3f}")

# Analyze keyword extraction
if "keyword_analysis" in detailed_result:
    keywords = detailed_result["keyword_analysis"]
    print(f"\nüîë Keyword Analysis:")
    print(f"   ‚Ä¢ Total keywords extracted: {keywords['total_keywords']}")
    
    if 'top_keywords' in keywords:
        print(f"   ‚Ä¢ Top keywords:")
        for keyword, score in keywords['top_keywords'][:5]:
            print(f"     - {keyword}: {score:.3f}")

# Analyze sentiment processing
if "sentiment_analysis" in detailed_result:
    sentiment = detailed_result["sentiment_analysis"]
    print(f"\nüòä Sentiment Analysis:")
    print(f"   ‚Ä¢ Overall sentiment: {sentiment['overall_sentiment']}")
    print(f"   ‚Ä¢ Sentiment score: {sentiment['sentiment_score']:.3f}")
    print(f"   ‚Ä¢ Confidence: {sentiment['confidence']:.3f}")

# Overall analysis results
if "text_analysis_results" in detailed_result:
    results = detailed_result["text_analysis_results"]
    print(f"\nüìä Overall Results:")
    print(f"   ‚Ä¢ Processing completed: {results['processing_metadata']['completed_at']}")
    print(f"   ‚Ä¢ Total processing time: {results['processing_metadata']['total_processing_time']:.2f}s")
```

## 3. Pipeline Performance Visualization

```python
# Create visualizations of the processing results
print("üìà Text Processing Performance Visualization")
print("============================================")

if all(key in detailed_result for key in ['word_analysis', 'sentence_analysis', 'keyword_analysis', 'sentiment_analysis']):
    words = detailed_result['word_analysis']
    sentences = detailed_result['sentence_analysis']
    keywords = detailed_result['keyword_analysis']
    sentiment = detailed_result['sentiment_analysis']
    
    # Create dashboard
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Basic statistics
    stats = ['Total Words', 'Unique Words', 'Total Sentences', 'Total Keywords']
    values = [words['total_words'], words['unique_words'], sentences['total_sentences'], keywords['total_keywords']]
    
    axes[0, 0].bar(stats, values, color=['lightblue', 'lightgreen', 'lightcoral', 'gold'])
    axes[0, 0].set_title('Text Statistics Overview')
    axes[0, 0].set_ylabel('Count')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Sentiment distribution
    sentiment_data = {
        'Positive': sentiment['positive_chunks'],
        'Neutral': sentiment['neutral_chunks'],
        'Negative': sentiment['negative_chunks']
    }
    
    colors = ['lightgreen', 'lightgray', 'lightcoral']
    axes[0, 1].pie(sentiment_data.values(), labels=sentiment_data.keys(), autopct='%1.1f%%', colors=colors)
    axes[0, 1].set_title('Sentiment Distribution')
    
    # Keyword frequency
    if 'top_keywords' in keywords:
        top_kw = keywords['top_keywords'][:8]
        kw_words = [kw[0] for kw in top_kw]
        kw_scores = [kw[1] for kw in top_kw]
        
        axes[1, 0].barh(kw_words, kw_scores, color='gold')
        axes[1, 0].set_title('Top Keywords by Relevance')
        axes[1, 0].set_xlabel('Relevance Score')
    
    # Processing timeline
    processing_stages = ['Chunking', 'Word\nAnalysis', 'Sentence\nAnalysis', 'Keyword\nExtraction', 'Sentiment\nAnalysis']
    processing_times = [0.5, 0.3, 0.2, 0.4, 0.3]  # Example times
    
    axes[1, 1].plot(processing_stages, processing_times, marker='o', linewidth=2, markersize=8, color='blue')
    axes[1, 1].set_title('Processing Timeline')
    axes[1, 1].set_ylabel('Time (seconds)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Performance summary
    total_time = detailed_result.get('text_analysis_results', {}).get('processing_metadata', {}).get('total_processing_time', 0)
    print(f"\n‚ö° Performance Summary:")
    print(f"   ‚Ä¢ Total processing time: {total_time:.2f}s")
    print(f"   ‚Ä¢ Words processed per second: {words['total_words'] / max(total_time, 0.1):.0f}")
    print(f"   ‚Ä¢ No external dependencies: No Redis required")
```

## 4. Configuration Experiments

```python
# Test different pipeline configurations
print("üß™ Text Processing Configuration Experiments")
print("=============================================")

# Define different configurations
experiments = [
    {
        "name": "Basic Processing",
        "config": {
            "operations": ["word_count", "sentence_count"],
            "chunk_size": 500
        }
    },
    {
        "name": "Comprehensive Analysis",
        "config": {
            "operations": ["word_count", "sentence_count", "extract_keywords", "analyze_sentiment"],
            "chunk_size": 1000
        }
    },
    {
        "name": "Keyword Focus",
        "config": {
            "operations": ["extract_keywords"],
            "chunk_size": 2000
        }
    }
]

experiment_results = []

for exp in experiments:
    print(f"\nüîÑ Running {exp['name']}...")
    
    config = exp['config'].copy()
    config['process_timestamp'] = datetime.now().isoformat()
    
    try:
        start_time = datetime.now()
        
        result = pipeline_manager.run(
            "text_processor",
            inputs=config,
            final_vars=["text_analysis_results"]
        )
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        if "text_analysis_results" in result:
            analysis = result["text_analysis_results"]
            
            experiment_results.append({
                "name": exp['name'],
                "chunks_processed": analysis['processing_metadata']['total_chunks_processed'],
                "processing_time": processing_time,
                "operations": len(config['operations']),
                "chunk_size": config['chunk_size'],
                "success": True
            })
            
            print(f"   ‚úÖ Completed in {processing_time:.2f}s")
            print(f"   üìä Chunks processed: {analysis['processing_metadata']['total_chunks_processed']}")
            
    except Exception as e:
        print(f"   ‚ùå Error: {e}")

# Analyze experiment results
if experiment_results:
    print(f"\nüìä Experiment Results Summary")
    print(f"==============================")
    
    results_df = pd.DataFrame(experiment_results)
    display(results_df)
    
    # Create comparison visualization
    successful_results = [r for r in experiment_results if r['success']]
    
    if successful_results:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        names = [r['name'] for r in successful_results]
        times = [r['processing_time'] for r in successful_results]
        chunks = [r['chunks_processed'] for r in successful_results]
        
        # Processing time comparison
        axes[0].bar(names, times, color='lightblue')
        axes[0].set_title('Processing Time Comparison')
        axes[0].set_ylabel('Time (seconds)')
        axes[0].tick_params(axis='x', rotation=45)
        
        # Chunks processed comparison
        axes[1].bar(names, chunks, color='lightgreen')
        axes[1].set_title('Chunks Processed')
        axes[1].set_ylabel('Number of Chunks')
        axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
```

## 5. Export and Integration

```python
# Export processed text results
print("üì§ Text Analysis Export and Integration")
print("=======================================")

# Run analysis with export
export_result = pipeline_manager.run(
    "text_processor",
    inputs={
        "process_timestamp": datetime.now().isoformat(),
        "operations": ["word_count", "sentence_count", "extract_keywords", "analyze_sentiment"],
        "save_to_file": True,
        "output_format": "json"
    },
    final_vars=["text_analysis_results"]
)

if "text_analysis_results" in export_result:
    analysis = export_result["text_analysis_results"]
    
    print(f"\nüìä Export Results:")
    print(f"   ‚Ä¢ Analysis completed: {analysis['processing_metadata']['completed_at']}")
    print(f"   ‚Ä¢ Output file: {analysis.get('output_file', 'In-memory only')}")
    print(f"   ‚Ä¢ Processing time: {analysis['processing_metadata']['total_processing_time']:.2f}s")
    
    # Extract key metrics for export
    export_data = {
        "analysis_timestamp": analysis['processing_metadata']['completed_at'],
        "processing_time": analysis['processing_metadata']['total_processing_time'],
        "chunks_processed": analysis['processing_metadata']['total_chunks_processed']
    }
    
    # Add word analysis if available
    if 'word_analysis' in analysis:
        word_stats = analysis['word_analysis']
        export_data.update({
            "total_words": word_stats['total_words'],
            "unique_words": word_stats['unique_words'],
            "vocabulary_richness": word_stats['vocabulary_richness']
        })
        print(f"   ‚Ä¢ Total words: {word_stats['total_words']:,}")
    
    # Add sentiment analysis if available
    if 'sentiment_analysis' in analysis:
        sentiment = analysis['sentiment_analysis']
        export_data.update({
            "overall_sentiment": sentiment['overall_sentiment'],
            "sentiment_score": sentiment['sentiment_score']
        })
        print(f"   ‚Ä¢ Overall sentiment: {sentiment['overall_sentiment']}")
    
    # Save export summary
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_file = f"outputs/text_analysis_summary_{timestamp}.json"
    
    try:
        os.makedirs("outputs", exist_ok=True)
        with open(summary_file, 'w') as f:
            json.dump(export_data, f, indent=2)
        print(f"\nüíæ Analysis summary saved: {summary_file}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not save summary: {e}")
    
    # Export as CSV
    csv_file = f"outputs/text_metrics_{timestamp}.csv"
    try:
        metrics_df = pd.DataFrame([export_data])
        metrics_df.to_csv(csv_file, index=False)
        print(f"üíæ Metrics exported to CSV: {csv_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save CSV: {e}")

print(f"\nüîó Integration Options:")
print(f"   ‚Ä¢ Spreadsheet: Import CSV into Excel/Google Sheets")
print(f"   ‚Ä¢ Database: Store metrics in SQLite/PostgreSQL")
print(f"   ‚Ä¢ API: Serve analysis via REST endpoints")
print(f"   ‚Ä¢ ML Pipeline: Use as features for machine learning")

print(f"\nüéâ Pipeline-only text processing completed successfully!")
print(f"üí° Perfect for development, prototyping, and lightweight processing")